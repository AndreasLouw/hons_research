{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53374,"status":"ok","timestamp":1696356858469,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"sMTacwHyZP-0","outputId":"e2ee16a5-ef01-47f0-cd25-bb497c70ff4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (3.27.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (17.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (2.1.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (2.0.5)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (2023.7.22)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch) (1.3.0)\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill\u003c0.3.8,\u003e=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]\u003c2023.9.0,\u003e=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Collecting huggingface-hub\u003c1.0.0,\u003e=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Collecting responses\u003c0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,\u003e=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk\u003e=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds\u003e=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs\u003e=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,\u003c5,\u003e=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six\u003e=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds\u003e=0.4.0-\u003ewandb) (1.16.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (3.2.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.3)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.4.0)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Collecting gitdb\u003c5,\u003e=4.0.1 (from GitPython!=3.1.29,\u003e=1.0.0-\u003ewandb)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (3.12.4)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (4.5.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.0.5)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2023.3.post1)\n","Collecting smmap\u003c6,\u003e=3.0.1 (from gitdb\u003c5,\u003e=4.0.1-\u003eGitPython!=3.1.29,\u003e=1.0.0-\u003ewandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=f8bd024eec1b3ee92e6cb5dd431d92e430a0bff2a30c619a03969efe0ab594fc\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built pathtools\n","Installing collected packages: pathtools, xxhash, smmap, setproctitle, sentry-sdk, docker-pycreds, dill, responses, multiprocess, huggingface-hub, gitdb, GitPython, wandb, datasets, evaluate\n","Successfully installed GitPython-3.1.37 datasets-2.14.5 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.0 gitdb-4.0.10 huggingface-hub-0.17.3 multiprocess-0.70.15 pathtools-0.1.2 responses-0.18.0 sentry-sdk-1.31.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.11 xxhash-3.3.0\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-o89s4knk\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-o89s4knk\n","  Resolved https://github.com/huggingface/transformers to commit 5c66378ceae6e358d113c7237034013c0f075dd1\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (3.12.4)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (0.17.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2.31.0)\n","Collecting tokenizers\u003c0.15,\u003e=0.14 (from transformers==4.35.0.dev0)\n","  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors\u003e=0.3.1 (from transformers==4.35.0.dev0)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.16.4-\u003etransformers==4.35.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.16.4-\u003etransformers==4.35.0.dev0) (4.5.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.16.4 (from transformers==4.35.0.dev0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.35.0.dev0) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.35.0.dev0) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.35.0.dev0) (2.0.5)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.35.0.dev0) (2023.7.22)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.35.0.dev0-py3-none-any.whl size=7749045 sha256=d0a3a75f6436437a222ffdb0919d458fca629bfcae7397a66d330a768aff5a58\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cxmfw1f8/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n","Successfully built transformers\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.17.3\n","    Uninstalling huggingface-hub-0.17.3:\n","      Successfully uninstalled huggingface-hub-0.17.3\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.35.0.dev0\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.11)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.37)\n","Requirement already satisfied: requests\u003c3,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.31.0)\n","Requirement already satisfied: docker-pycreds\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs\u003e=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,\u003c5,\u003e=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six\u003e=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds\u003e=0.4.0-\u003ewandb) (1.16.0)\n","Requirement already satisfied: gitdb\u003c5,\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,\u003e=1.0.0-\u003ewandb) (4.0.10)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2.0.5)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2023.7.22)\n","Requirement already satisfied: smmap\u003c6,\u003e=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb\u003c5,\u003e=4.0.1-\u003eGitPython!=3.1.29,\u003e=1.0.0-\u003ewandb) (5.0.1)\n","Collecting accelerate\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch\u003e=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.10.0-\u003eaccelerate) (3.27.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.10.0-\u003eaccelerate) (17.0.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.10.0-\u003eaccelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (2.0.5)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (2023.7.22)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.10.0-\u003eaccelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.23.0\n"]}],"source":["!pip install torch torchvision\n","!pip install datasets evaluate wandb\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install wandb\n","!pip install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31683,"status":"ok","timestamp":1696356890145,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"xu5wBqMkkf62","outputId":"5302f025-806b-4161-8c94-88bbbf10c834"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79976,"status":"ok","timestamp":1696356970114,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"SDb4tIi39i1u","outputId":"ae25d662-0446-4961-e902-d6997aca73dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-10-03 18:14:50--  https://docs.google.com/uc?export=download\u0026confirm=t\u0026id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF\n","Resolving docs.google.com (docs.google.com)... 172.217.194.113, 172.217.194.138, 172.217.194.102, ...\n","Connecting to docs.google.com (docs.google.com)|172.217.194.113|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-04-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lksq64ibcf2uqmj6aurljr49pojvnr5s/1696356825000/11183032721846533402/*/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF?e=download\u0026uuid=cf717c91-64fc-4610-99dd-4bf58fef1b33 [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-10-03 18:14:51--  https://doc-04-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lksq64ibcf2uqmj6aurljr49pojvnr5s/1696356825000/11183032721846533402/*/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF?e=download\u0026uuid=cf717c91-64fc-4610-99dd-4bf58fef1b33\n","Resolving doc-04-bk-docs.googleusercontent.com (doc-04-bk-docs.googleusercontent.com)... 64.233.170.132, 2404:6800:4003:c1a::84\n","Connecting to doc-04-bk-docs.googleusercontent.com (doc-04-bk-docs.googleusercontent.com)|64.233.170.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2246169376 (2.1G) [application/x-zip-compressed]\n","Saving to: ‘fabi-data.zip’\n","\n","fabi-data.zip       100%[===================\u003e]   2.09G  41.0MB/s    in 59s     \n","\n","2023-10-03 18:15:51 (36.2 MB/s) - ‘fabi-data.zip’ saved [2246169376/2246169376]\n","\n","Archive:  fabi-data.zip\n","   creating: data/fabi-data/test/\n","   creating: data/fabi-data/test/0/\n","  inflating: data/fabi-data/test/0/Anaphes nitens 1 Female on Gonipterus egg capsule.jpg  \n","   creating: data/fabi-data/test/1/\n","  inflating: data/fabi-data/test/1/SBush_Blastopsylla occidentalis adult 3.jpg  \n","  inflating: data/fabi-data/test/1/SBush_Blastopsylla occidentalis adult2.jpg  \n","   creating: data/fabi-data/test/10/\n","  inflating: data/fabi-data/test/10/Heavily infested lower branches.jpeg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus 1a.jpg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus 1b.jpg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus galls Selitrichodes female 1a.jpg  \n","   creating: data/fabi-data/test/11/\n","  inflating: data/fabi-data/test/11/DSC_9298.NEF  \n","  inflating: data/fabi-data/test/11/DSC_9300.jpg  \n","  inflating: data/fabi-data/test/11/IMG_3426.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3429.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3469.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3481.JPG  \n","   creating: data/fabi-data/test/12/\n","  inflating: data/fabi-data/test/12/145F5006-CF53-4BFC-81DC-F1EC372DB56D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/1714C5B0-4599-4DC9-BB9A-FB6C2FD8A7CF_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/2DFA84C6-5FB3-431B-B6B2-56A4FC8F4C52_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/322C9D7E-E06B-44B9-B7AE-E36993B32062_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/4C6FB127-A670-40B6-BEF5-A4451AFA5521_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/593AF0C5-A2FA-4062-A271-B5A168B1D497_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/6866D250-37C9-4478-845F-B32662F4028E_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/7C4CA639-5682-49AA-8E90-47BE10C8621C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/C8B432D2-317E-4B5A-A020-F0182775ABEB_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/F05178E4-EAE9-4C5D-96AC-D72451666010_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/F233A982-D15C-4E02-B366-23D6CC03F3BA_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/FFCB3191-F78F-489C-B098-6E356E695FAE_1_105_c.jpeg  \n","   creating: data/fabi-data/test/13/\n","  inflating: data/fabi-data/test/13/Male F1 3 April 2015 resized.jpg  \n","  inflating: data/fabi-data/test/13/Male F1 April 2015.jpg  \n","  inflating: data/fabi-data/test/13/Male mystery wasp 8 Oct 2013.jpg  \n","  inflating: data/fabi-data/test/13/Male mystery wasp 9 Oct2013.jpg  \n","  inflating: data/fabi-data/test/13/Mummy April 2015 resized.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Female Oct 2013 5b.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 10.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 11.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 9.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 1.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 4.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 6.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Parasitised nymph 1.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus pupae underneath 3a.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus two parasitised nymphs 4a.jpg  \n","  inflating: data/fabi-data/test/13/Pb possibly hyper.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02b.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02c adjusted.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02e.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr03.jpg  \n","  inflating: data/fabi-data/test/13/SpObsSA05c.jpg  \n","  inflating: data/fabi-data/test/13/SpObsSA09 1.tif  \n","  inflating: data/fabi-data/test/13/SpObsSA09 3.tif  \n","  inflating: data/fabi-data/test/13/SpObsSA09 4.tif  \n","   creating: data/fabi-data/test/14/\n","  inflating: data/fabi-data/test/14/Possibly Quadristichus resized for email.jpg  \n","  inflating: data/fabi-data/test/14/Qmendeli.jpg  \n","  inflating: data/fabi-data/test/14/Qmendeli7.jpg  \n","   creating: data/fabi-data/test/15/\n","  inflating: data/fabi-data/test/15/Selitrichodes female 2.tif  \n","  inflating: data/fabi-data/test/15/Selitrichodes female 7a.jpg  \n","   creating: data/fabi-data/test/16/\n","  inflating: data/fabi-data/test/16/Image_122b.jpg  \n","  inflating: data/fabi-data/test/16/Image_126b.jpg  \n","   creating: data/fabi-data/test/17/\n","  inflating: data/fabi-data/test/17/Close up of browned larvae.jpg  \n","  inflating: data/fabi-data/test/17/Close up of strange eggs.jpg  \n","  inflating: data/fabi-data/test/17/DSCN0198.JPG  \n","  inflating: data/fabi-data/test/17/DSCN1806.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7030.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7031.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7476.JPG  \n","  inflating: data/fabi-data/test/17/Larvae1 no4.jpg  \n","  inflating: data/fabi-data/test/17/Larvae3 no2.jpg  \n","  inflating: data/fabi-data/test/17/Scarred Sirex Larvae 1.jpg  \n","  inflating: data/fabi-data/test/17/Sirex larvae scarred head.jpg  \n","  inflating: data/fabi-data/test/17/Sirex scarred tail 2.jpg  \n","  inflating: data/fabi-data/test/17/Sirex scarred tail.jpg  \n","  inflating: data/fabi-data/test/17/Sirex with nematodes 2.jpg  \n","   creating: data/fabi-data/test/18/\n","  inflating: data/fabi-data/test/18/04EB584A-3B0E-45E4-8300-8FF93C2A455D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/2913384C-B94B-45C7-A87E-225B8873EEAD_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/3E9E2B2A-C617-4E08-B30D-E1C9DBC145D0_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/4DD2AC59-333B-4C2E-9C0B-E75D3765169A_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/6D67302D-F633-454A-AD60-A2B272BBB497_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/7D5BE38F-F614-4A70-97C8-1BAB81289AA1_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/7F257B7B-0B44-4A78-A4F2-2CB1E8799183_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/Adult Middelburg Spondyliaspis wing different angle.jpg  \n","  inflating: data/fabi-data/test/18/Adult Middelburg Spondyliaspis.jpg  \n","  inflating: data/fabi-data/test/18/AFFB4796-1560-4A3B-B72F-E204255C0A85_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/BB7DD3C2-0CB0-45E9-BB24-0DB2DEFCB119_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/Damage 2.jpg  \n","  inflating: data/fabi-data/test/18/E11EB6E7-2C6E-4900-AC81-5AB8E745324D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002212.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002214.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002217.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002218.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002223.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002229.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002231.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002234.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002236.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002237.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002239.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002241.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002245.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002254.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002265.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002271.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002282.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002283.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002286.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002289.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002293.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002295.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002300.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002302.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002307.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002317.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002319.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002335.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002345.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002346.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002348.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002358.tif  \n","  inflating: data/fabi-data/test/18/none-0000002384.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002391.jpg  \n","  inflating: data/fabi-data/test/18/Spondyliaspis Adult2.jpg  \n","  inflating: data/fabi-data/test/18/Stack3A.jpg  \n","  inflating: data/fabi-data/test/18/StackC.jpg  \n","   creating: data/fabi-data/test/19/\n","  inflating: data/fabi-data/test/19/DSCN0820.JPG  \n","  inflating: data/fabi-data/test/19/DSCN5459.JPG  \n","  inflating: data/fabi-data/test/19/eggs7.jpg  \n","  inflating: data/fabi-data/test/19/IMG_0582.JPG  \n","  inflating: data/fabi-data/test/19/IMG_0638.JPG  \n","  inflating: data/fabi-data/test/19/none-0000001397.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001398.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001399.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001401.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001405.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001425.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001433.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001434.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001436.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001438.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001440.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001452.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001459.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001461.jpeg  \n","  inflating: data/fabi-data/test/19/none-0000001468.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001469.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001472.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001483.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001487.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris female 1.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Male 2.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Male 4.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Samantha Bush.jpg  \n","   creating: data/fabi-data/test/2/\n","  inflating: data/fabi-data/test/2/SBush_Cleruchoides noackae_ovipositing into Thaumastocoris peregrinus eggs2.jpg  \n","  inflating: data/fabi-data/test/2/SBush_Cleruchoides noackae_with Thaumastocoris peregrinus eggs1.jpg  \n","   creating: data/fabi-data/test/20/\n","  inflating: data/fabi-data/test/20/DSCN1701.JPG  \n","  inflating: data/fabi-data/test/20/DSCN1705.JPG  \n","  inflating: data/fabi-data/test/20/IMG_2308.JPG  \n","   creating: data/fabi-data/test/21/\n","  inflating: data/fabi-data/test/21/311C3CAF-DAD3-4027-BF1E-5C96BCD5A648_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/21/CA4A546E-F2D4-4A2E-B530-A33605E87097_1_105_c.jpeg  \n","   creating: data/fabi-data/test/22/\n","  inflating: data/fabi-data/test/22/Armillaria GM Granados 03.JPG  \n","  inflating: data/fabi-data/test/22/Armillaria GM Granados 06.JPG  \n","  inflating: data/fabi-data/test/22/Armillaria mellea mushrooms Bavarian Germany 2009.jpg  \n","   creating: data/fabi-data/test/23/\n","  inflating: data/fabi-data/test/23/none-0000000728.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000730.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000732.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000736.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000742.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000747.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000760.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000769.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000770.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000776.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000777.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000779.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000781.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000784.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000792.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000796.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000803.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000805.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000809.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000810.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000812.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000813.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000817.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000834.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000845.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000851.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000858.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000860.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000862.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000876.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000893.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000895.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000916.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000917.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000929.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000932.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000939.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000940.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000944.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000947.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000953.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000956.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000957.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000967.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000975.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000984.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000991.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001001.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001002.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001012.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001015.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001017.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001024.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001025.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001029.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001040.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001050.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001051.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001054.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001065.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001073.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001076.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001078.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001080.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001087.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001091.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001093.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001097.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001104.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001106.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001117.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001120.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001127.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001131.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001135.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001136.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001145.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001146.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001150.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001152.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001154.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001155.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001158.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001159.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001160.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001162.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001163.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001170.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001179.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001183.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001185.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001195.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001203.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001209.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001210.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001213.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001214.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001218.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001224.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001238.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001241.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001246.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001248.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001259.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001266.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001267.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001268.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001269.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001270.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001276.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001281.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001282.jpg  \n","   creating: data/fabi-data/test/24/\n","  inflating: data/fabi-data/test/24/Bacterial blight cause by Pantoea ananatis.png  \n","  inflating: data/fabi-data/test/24/Bacterial blight Pantoea anantis_Izette.JPG  \n","   creating: data/fabi-data/test/25/\n","  inflating: data/fabi-data/test/25/Bacterial wilt caused by _i_Ralstonia solanacearum_i_.png  \n","   creating: data/fabi-data/test/26/\n","  inflating: data/fabi-data/test/26/Botryosphaeria canker internal Neofusicoccum spp_Izette.jpg  \n","   creating: data/fabi-data/test/27/\n","  inflating: data/fabi-data/test/27/Fig. 4. Fruiting structures with abundant mature conidia 3.jpg  \n","   creating: data/fabi-data/test/28/\n","  inflating: data/fabi-data/test/28/Calonectria symptoms on _i_Eucalyptus_i_ leaves.JPG  \n","   creating: data/fabi-data/test/29/\n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados01.JPG  \n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados14.JPG  \n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados17.JPG  \n","   creating: data/fabi-data/test/3/\n","  inflating: data/fabi-data/test/3/6A4F68D7-85B0-4F91-BA7B-E0D906B37970_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/6C6B7899-4F1D-472E-B48B-6BD804C37DA6_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/8531B8E1-3ACC-4EEB-99E1-21A44F1B7765_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/AEE41DD1-E10E-4E3E-B21F-91639A4F475C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/DSCN0361.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0363.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0377.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0803.JPG  \n","  inflating: data/fabi-data/test/3/DSCN1608.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4174.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4191.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4202.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5065.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5492.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5499.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5504.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5515.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5593.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5600.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5605.JPG  \n","  inflating: data/fabi-data/test/3/DSCN6967.JPG  \n","  inflating: data/fabi-data/test/3/DSCN6968.JPG  \n","  inflating: data/fabi-data/test/3/DSCN7320.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8134.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8137.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8146.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8149.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8150.JPG  \n","  inflating: data/fabi-data/test/3/FF36B45F-AC9F-44BD-BA00-4C670A18D626_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/IMG_0687.JPG  \n","  inflating: data/fabi-data/test/3/IMG_0699.JPG  \n","  inflating: data/fabi-data/test/3/IMG_0737.JPG  \n","   creating: data/fabi-data/test/30/\n","  inflating: data/fabi-data/test/30/T.zuluensis KZN ZG14 GM Granados 01.JPG  \n","  inflating: data/fabi-data/test/30/T.zuluensis KZN ZG14 GM Granados 06.JPG  \n","   creating: data/fabi-data/test/31/\n","  inflating: data/fabi-data/test/31/D.sapinea GM Granados 06.JPG  \n","  inflating: data/fabi-data/test/31/Diplodia5.JPG  \n","  inflating: data/fabi-data/test/31/Diplodia6.jpg  \n","   creating: data/fabi-data/test/32/\n","  inflating: data/fabi-data/test/32/Fusarium circinatum root rot.JPG  \n","   creating: data/fabi-data/test/33/\n","  inflating: data/fabi-data/test/33/Glycaspis 1.png  \n","   creating: data/fabi-data/test/34/\n","  inflating: data/fabi-data/test/34/_i_Eucalyptus smithii_i_ collar rot.JPG  \n","   creating: data/fabi-data/test/35/\n","  inflating: data/fabi-data/test/35/E.salmonicolor Podocarpus GM Granados 03.JPG  \n","   creating: data/fabi-data/test/36/\n","  inflating: data/fabi-data/test/36/F.circinatum GM Granados 03.JPG  \n","  inflating: data/fabi-data/test/36/F.circinatum GM Granados 05.JPG  \n","   creating: data/fabi-data/test/37/\n","  inflating: data/fabi-data/test/37/Powdery mildew GM Granados01.JPG  \n","   creating: data/fabi-data/test/38/\n","  inflating: data/fabi-data/test/38/Pseudophaeolus KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/test/38/Pseudophaeolus KZN GM Granados 05.JPG  \n","   creating: data/fabi-data/test/39/\n","  inflating: data/fabi-data/test/39/none-0000001288.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001302.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001304.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001312.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001316.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001319.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001320.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001324.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001328.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001330.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001331.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001339.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001347.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001351.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001358.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001363.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001366.jpg  \n","  inflating: data/fabi-data/test/39/Quambalaria GM Granados 01.JPG  \n","   creating: data/fabi-data/test/4/\n","  inflating: data/fabi-data/test/4/Ctenarytaina male 2 SBush.tif  \n","  inflating: data/fabi-data/test/4/Nymph 3 SBush.tif  \n","   creating: data/fabi-data/test/40/\n","  inflating: data/fabi-data/test/40/Fig.1c_FABInews.jpg  \n","   creating: data/fabi-data/test/41/\n","  inflating: data/fabi-data/test/41/Rhizina T Paap_IMG_5700.JPG  \n","   creating: data/fabi-data/test/42/\n","  inflating: data/fabi-data/test/42/Fig2_Conio.JPG  \n","   creating: data/fabi-data/test/43/\n","  inflating: data/fabi-data/test/43/none-0000000007.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000009.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000010.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000011.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000012.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000017.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000018.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000020.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000031.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000035.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000038.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000041.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000046.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000050.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000051.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000052.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000054.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000060.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000073.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000078.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000079.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000087.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000092.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000096.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000105.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000115.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000122.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000130.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000131.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000133.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000138.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000142.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000144.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000149.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000150.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000152.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000156.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000159.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000164.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000172.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000177.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000183.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000184.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000187.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000189.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000191.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000194.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000216.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000228.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000231.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000232.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000233.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000236.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000237.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000240.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000244.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000254.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000261.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000262.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000264.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000265.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000272.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000278.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000279.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000291.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000292.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000300.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000308.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000311.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000315.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000321.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000323.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000325.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000334.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000342.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000346.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000352.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000355.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000364.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000369.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000378.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000382.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000384.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000386.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000399.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000415.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000422.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000438.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000448.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000451.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000456.jpg  \n","  inflating: data/fabi-data/test/43/T.destructans KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/test/43/T.destructans KZN GM Granados 02.JPG  \n","   creating: data/fabi-data/test/44/\n","  inflating: data/fabi-data/test/44/none-0000000463.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000471.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000474.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000480.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000481.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000489.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000498.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000499.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000503.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000506.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000513.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000518.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000521.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000522.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000524.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000525.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000527.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000528.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000541.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000545.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000546.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000559.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000560.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000562.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000564.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000567.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000571.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000580.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000590.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000616.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000618.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000619.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000631.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000633.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000641.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000642.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000644.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000650.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000653.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000656.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000662.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000665.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000667.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000675.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000678.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000687.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000688.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000689.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000691.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000696.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000703.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000707.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000710.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000723.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000726.jpg  \n","   creating: data/fabi-data/test/45/\n","  inflating: data/fabi-data/test/45/Wattle rust KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/test/45/Wattle rust_Harding_KZN_GMGranados02.JPG  \n","  inflating: data/fabi-data/test/45/Wattle rust_Harding_KZN_GMGranados03.JPG  \n","   creating: data/fabi-data/test/5/\n","  inflating: data/fabi-data/test/5/Pupa and pupal casing.JPG  \n","   creating: data/fabi-data/test/6/\n","  inflating: data/fabi-data/test/6/16360465-C594-4220-9124-037C12A24735_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/2E3AECF5-9282-4CE9-80DC-D3F7064BB10B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/37324B65-6842-4318-9D81-15802953C399_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/45B43995-1E05-433E-B3B9-C7DACAA4C7CB_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/57834C65-77AB-47BD-A83D-FCC9AD90308B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/717C21AA-80AF-499E-8FEC-641DFD88CC01_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/934D8377-ACA2-42A6-8637-0691D4ED4869_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/Adult  male pic1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Adult Gb emerging 2 SBush.jpg  \n","  inflating: data/fabi-data/test/6/B8A1BEA8-679E-4DA3-AEF3-003D35786B9A_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/C7C2B96C-DD06-489C-B8A4-A3532F7C482B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/CD90717B-31F2-48E8-988C-8EB95C74CE13_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/CF9D1E33-7828-4022-A572-31F52447B7D2_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D6A57709-5D66-49B7-BA88-A5EB6D3E728B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D8BB34D7-7381-4320-98AA-EF74A4B83A1C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D93C028E-6E67-46F4-A64E-466A44652C28_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/DD84EA7E-C791-44B8-BCAF-2B90C75BB13D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/E7E14C4D-E772-4F18-A48E-E2913CDC1813_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/EB170C11-FF6D-4962-82E1-1157B22100BD_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/Gb Fifth instar nymph 4 SBush.tif  \n","  inflating: data/fabi-data/test/6/Gb Fifth Instar Nymph 6 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb instar 1 and 2 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb possibly second and third instar 1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb possibly third instar 1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002042.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002053.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002054.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002057.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002062.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002064.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002068.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002080.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002083.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002084.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002085.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002088.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002090.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002102.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002108.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002112.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002113.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002117.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002119.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002128.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002129.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002131.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002134.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002135.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002141.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002143.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002144.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002147.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002150.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002153.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002160.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002163.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002165.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002174.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002179.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002184.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002187.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002188.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002198.jpg  \n","   creating: data/fabi-data/test/7/\n","  inflating: data/fabi-data/test/7/80CFE9F1-7450-4B61-BE80-D93BCD03AE91_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/9A54E69A-0431-4817-BA25-0527E0B252D1_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/BE8504A8-0047-4E20-886A-4802BC642523_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/E5C68486-5E51-49CF-8007-77C05C81C09E_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/Gonipterus_adult4.jpg  \n","  inflating: data/fabi-data/test/7/Gonipterus_adult8.jpg  \n","  inflating: data/fabi-data/test/7/Gonipterus_larvae5.jpg  \n","  inflating: data/fabi-data/test/7/IMG_4254.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4255.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4461.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4467.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6114.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6168.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6170.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6199.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6202.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6213.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6217.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6218.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6219.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6227.JPG  \n","  inflating: data/fabi-data/test/7/none-0000001491.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001492.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001496.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001501.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001505.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001506.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001515.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001516.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001523.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001534.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001535.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001538.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001540.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001551.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001556.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001559.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001560.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001563.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001565.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001568.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001589.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001590.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001595.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001597.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001598.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001602.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001605.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001608.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001611.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001613.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001615.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001623.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001638.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001643.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001648.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001652.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001659.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001660.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001661.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001665.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001667.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001675.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001678.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001681.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001688.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001691.tif  \n","  inflating: data/fabi-data/test/7/none-0000001693.tif  \n","  inflating: data/fabi-data/test/7/none-0000001694.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001696.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001701.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001704.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001706.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001707.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001708.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001711.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001712.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001730.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001751.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001754.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001756.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001762.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001763.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001764.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001766.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001772.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001776.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001777.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001781.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001783.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001794.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001796.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001797.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001801.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001803.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001817.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001826.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001828.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001832.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001835.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001841.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001843.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001854.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001863.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001867.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001875.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001876.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001882.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001884.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001886.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001888.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001905.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001919.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001935.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001943.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001945.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001949.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001950.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001962.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001965.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001973.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001978.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001979.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001982.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001990.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001994.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001997.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002003.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002005.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002006.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002011.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002013.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002017.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002021.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002024.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002028.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002030.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002036.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002037.jpg  \n","   creating: data/fabi-data/test/8/\n","  inflating: data/fabi-data/test/8/Dissected_gall3.jpg  \n","  inflating: data/fabi-data/test/8/DSCN6609.JPG  \n","  inflating: data/fabi-data/test/8/DSCN6616.JPG  \n","  inflating: data/fabi-data/test/8/FB6A01A9-D6DD-4B2D-92B9-4D40FAF11427_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/8/IMG_0335.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0343.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0350.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0358.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1971.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1986.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1988.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2000.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2011.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2013.JPG  \n","  inflating: data/fabi-data/test/8/Leptocybe invasa pupae 1.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe invasa pupae 2.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe male 3.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe4.JPG_Female_wasp.jpg  \n","  inflating: data/fabi-data/test/8/Oviposition_scars_Linvasa4.jpg  \n","   creating: data/fabi-data/test/9/\n","  inflating: data/fabi-data/test/9/Megastigmus female brown 1b.jpg  \n","  inflating: data/fabi-data/test/9/Megastigmus Female cream 1a.jpg  \n","   creating: data/fabi-data/train/\n","   creating: data/fabi-data/train/0/\n","  inflating: data/fabi-data/train/0/Anitens_female_c.jpg  \n","   creating: data/fabi-data/train/1/\n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentails nymph 2.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis adult.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis adult4.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis nymph 1.jpg  \n","   creating: data/fabi-data/train/10/\n","  inflating: data/fabi-data/train/10/Galls of Ophelimus maskelli.jpeg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 1bresized.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 2a.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 2b.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 3a.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 11.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 3.tif  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 5.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 9.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 13.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes males 2.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes males 3.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 4.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 7.jpg  \n","   creating: data/fabi-data/train/11/\n","  inflating: data/fabi-data/train/11/DSC_0136 15 Edited.jpg  \n","  inflating: data/fabi-data/train/11/DSC_0142 21 Edited.jpg  \n","  inflating: data/fabi-data/train/11/DSC_0345.jpeg  \n","  inflating: data/fabi-data/train/11/DSC_9267.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9284.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9300.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9303.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9313.NEF  \n","  inflating: data/fabi-data/train/11/IMG_3374.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3422.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3423.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3424.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3427.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3428.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3431.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3466.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3467.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3468.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3471.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3474.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3477.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3480.JPG  \n","   creating: data/fabi-data/train/12/\n","  inflating: data/fabi-data/train/12/043C2B0E-C3E7-41B9-9BDB-C01CCA3C162D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/07026B37-232F-472A-BD0F-B017A7E5D5FA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0810112E-10C8-4ED0-9631-F61C83B146AD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0C7BA9F2-1A31-402A-A934-5CCE57889440_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0F76D033-8624-48C3-AA22-C33F924A3BFB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1506CA87-DCA3-47BD-881A-F968632308A0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1B6118D6-9FCE-4E11-9321-B0B7FD32570E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1C463DFF-26B7-4B58-BA3D-30E3AAFF74F3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/2A8EA66A-47FA-4675-8A0F-A176E733F78F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/2B21E100-F2D3-4E88-A0B8-2DD7CF73D3CB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/3776CD74-D775-4549-9FC4-DE0900DE8D20_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/39A60521-0B4D-4BBE-8FC5-AEFACC231A03_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/40FB2F99-92B1-4B88-B3C0-4B69C7C85FC7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/526A8779-51E4-4F73-BA4F-D91565CC716D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/54C59E62-E004-4BC6-8970-CE5BC29CF418_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/56D75DDE-CECE-4354-AC15-3D34AD513CA4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/5E308468-AE09-40CC-8D27-F09E25F0ACE1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/6BBF1749-8DAB-44F8-A278-4358F7A13615_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/6D4A3BE3-862A-4CE5-BF87-23FB9228E00B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/724605D2-B7D1-44D2-8124-E760342BFC17_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/76839D17-EAD6-4101-BA01-F63B919210AD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/782BB49D-9DBC-483C-9287-4CC6B53D273D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7C3DF9AE-3B0E-4897-AE00-BBD3F399201A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7D1AAA3E-56B4-45C6-B12F-CA100601C8C5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7EB5BBAF-7087-4C26-9437-57E968136593_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/809A0A24-F9B9-4431-980F-42302DEA701F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/81A74718-6965-4B9B-BC5D-7755B3C0CFD5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A18C6426-F85D-4E8C-8586-756DFC19D558_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A2545607-6019-4246-B72C-4468F0731855_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A83C99CE-D70E-42FE-9904-F5D4B38D428E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/B1DDF735-DFB9-4554-9B04-EEB62685A0CC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/B510B188-C0A3-4D3F-87EE-113B3184B211_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/BB66BC9C-7344-430C-95A2-FAA61725E76C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/CA051F33-8495-478B-9BA0-ED423A3887E6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/CA94B040-9C6D-4D2E-8DC6-D8CED087FA33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D0B97ABD-EEF2-462D-87F9-B9FB84F31D3E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D1C50C5E-5AA3-42A5-956A-076AB7980730_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D5FB90EA-0D5F-4846-B752-54F838173840_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/DC9F9526-78EC-4D61-BC13-B957FBAAA3E9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/DEC15466-3268-4182-9714-C3E4E5CE946C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/E12927A8-3986-4FCF-8AEE-89151DA14143_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/E74FD0F1-6A4C-42A4-8D5D-5BF11CBCF057_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/ECF5A653-FF4E-4F7E-860C-1B886D70BA76_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/EF507F58-1071-481D-AB04-F2EF3D2A532F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/F05DDFC7-4F98-4057-B6DC-2D0A75CF6D3D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/Larvae in growing tip of pine tree.jpeg  \n","   creating: data/fabi-data/train/13/\n","  inflating: data/fabi-data/train/13/Contents of dissected mummy.tif  \n","  inflating: data/fabi-data/train/13/Contents of dissected mummy2.tif  \n","  inflating: data/fabi-data/train/13/Dark.tif  \n","  inflating: data/fabi-data/train/13/Dark2.tif  \n","  inflating: data/fabi-data/train/13/Dark3.tif  \n","  inflating: data/fabi-data/train/13/Dark4.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 1a.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 1b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 2a.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 2b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 4.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 5a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 5b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 6a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 6b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 7a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 7b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 8a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 8b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 9 middle leg.jpg  \n","  inflating: data/fabi-data/train/13/From Gb mummies.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 2 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 3 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 4 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 5 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 6 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 7 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 1 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 10 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 11  Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 12 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 2 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 3 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 4 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 5 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 6 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 7 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Mummy April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus resized autocorrected.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus resized.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus2.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus3.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus4.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 1a.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 4.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 5.tif  \n","  inflating: data/fabi-data/train/13/P bliteus female Oct 2013 8a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus female Oct 2013 8b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 1a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 2b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 4.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 5.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 6.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 7.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 8.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus males feeding on honey paper Oct 2013 12.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Males feeding on honey paper Oct2013 13.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 2.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 5.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph 2b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph and lerp 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae 1a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Pupae 2.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae underneath 3b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus two parasitized nymphs 4b.jpg  \n","  inflating: data/fabi-data/train/13/Parasitised nymph Oct2013 1a.tif  \n","  inflating: data/fabi-data/train/13/Parasitised nymph Oct2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper2.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper3.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper5.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper6.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper7.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02a.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02c.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02d.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02f.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA04.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA04b.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05b edited.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05b.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05d.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA09 2.tif  \n","  inflating: data/fabi-data/train/13/SpObsSA10 1.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA10 2.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA10 3.jpg  \n","   creating: data/fabi-data/train/14/\n","  inflating: data/fabi-data/train/14/Possibly Quadristichus 2.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus 3 resized for email.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus autocorrected.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli2.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli5.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli6.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli8.jpg  \n","   creating: data/fabi-data/train/15/\n","  inflating: data/fabi-data/train/15/Copy of SA Ophelimus galls Selitrichodes Males 7.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 3.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 4.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5a.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5b.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 6.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 7a resized.jpg  \n","   creating: data/fabi-data/train/16/\n","  inflating: data/fabi-data/train/16/Image_120.jpg  \n","  inflating: data/fabi-data/train/16/Image_120b.jpg  \n","  inflating: data/fabi-data/train/16/Image_122.jpg  \n","  inflating: data/fabi-data/train/16/Image_125.jpg  \n","  inflating: data/fabi-data/train/16/Image_125b.jpg  \n","  inflating: data/fabi-data/train/16/Image_126.jpg  \n","  inflating: data/fabi-data/train/16/Image_127.jpg  \n","  inflating: data/fabi-data/train/16/Image_127b.jpg  \n","   creating: data/fabi-data/train/17/\n","  inflating: data/fabi-data/train/17/0CFB1157-776A-40EB-9023-8BFC1744B30A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/412F67DA-6F11-470D-BF35-4404E7BE6794_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/536385B9-96F1-4710-9D87-DA9AAE64D896_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/BDA98CCA-8E7D-417B-999D-5A5F0BAF1600_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/Browned larvae 1.jpg  \n","  inflating: data/fabi-data/train/17/DSCN0183.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0197.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0200.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0203.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0320.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1113.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1114.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1119.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1126.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1442.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1443.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1773.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1783.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1786.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1787.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1790.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1795.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1796.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1799.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1800.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1819.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1825.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1827.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1831.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2292.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2293.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2294.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2304.JPG  \n","  inflating: data/fabi-data/train/17/DSCN6938.JPG  \n","  inflating: data/fabi-data/train/17/DSCN6956.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7018.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7034.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7475.JPG  \n","  inflating: data/fabi-data/train/17/For scale.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no5.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no8.jpg  \n","  inflating: data/fabi-data/train/17/Larvae2 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae2 no2.jpg  \n","  inflating: data/fabi-data/train/17/Larvae3 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae4 stacked.jpg  \n","  inflating: data/fabi-data/train/17/Scale for Sirex larvae scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex larvae scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 1.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 3.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 4.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs 1.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs 2.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs with scale.jpg  \n","   creating: data/fabi-data/train/18/\n","  inflating: data/fabi-data/train/18/305175B6-1E20-49BE-AB7D-71FEEFEC553B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/406E93C6-E8FC-4B57-BFE7-F37D04B308C3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/4F1A6E39-3E0A-45F5-991D-556269EA3360_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/5C3B7AB7-6306-4F36-8E38-464431484BE0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/8E9BB382-24AB-4D23-A302-9701614AA090_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/90D2C906-6218-4A8D-886B-BFFA0D34ECC6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Adult Middelburg Spondyliaspis wing.jpg  \n","  inflating: data/fabi-data/train/18/Adult Middelburg Spondyliaspis2.jpg  \n","  inflating: data/fabi-data/train/18/Adult3.jpg  \n","  inflating: data/fabi-data/train/18/Adult4.jpg  \n","  inflating: data/fabi-data/train/18/AE3F0360-0A86-4032-A276-A92FCD48A89E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/B60AF6E2-271A-4CEE-A109-4AB2B67173E4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/B9D76401-6115-4CD8-8DF1-92DEEF573860_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/BDD5449D-7C42-48DB-AB3D-2444F43340AE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Damage 1.jpg  \n","  inflating: data/fabi-data/train/18/DC45DECE-6511-4F0B-8ECB-ADDC0265CC47_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/E2E5D0CA-FE62-4E35-9F00-D1EEF0620D71_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/ECA42586-03C5-4E02-A0ED-A57CF4A14C27_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/EF6AA3C8-9E98-4386-923D-A93977BF475A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Eggs.jpg  \n","  inflating: data/fabi-data/train/18/F1DCE26E-119E-4667-B246-85DBF0EC0FCC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/F45DF301-8453-4CAD-A8F7-47728C6DA032_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/F4829593-B31B-4AF4-98BE-3665CF01FF4E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/FCFBDFFE-4377-4114-AB98-49C6053B261F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Hatched eggs and lerps3.jpg  \n","  inflating: data/fabi-data/train/18/Hatched eggs and lerps5.jpg  \n","  inflating: data/fabi-data/train/18/Individual2 pic1.tif  \n","  inflating: data/fabi-data/train/18/Individual2 pic2.jpg  \n","  inflating: data/fabi-data/train/18/Individual2 pic4.tif  \n","  inflating: data/fabi-data/train/18/Largest lerp1.jpg  \n","  inflating: data/fabi-data/train/18/Medium Lerp.jpg  \n","  inflating: data/fabi-data/train/18/Medium Lerp3.jpg  \n","  inflating: data/fabi-data/train/18/Newly emerged adult.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002206.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002207.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002208.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002209.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002210.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002211.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002213.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002215.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002216.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002219.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002220.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002221.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002222.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002224.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002225.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002226.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002227.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002228.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002230.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002232.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002233.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002235.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002238.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002240.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002242.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002243.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002244.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002246.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002247.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002248.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002249.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002250.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002251.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002252.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002253.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002255.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002256.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002257.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002258.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002259.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002260.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002261.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002262.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002263.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002264.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002266.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002267.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002268.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002269.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002270.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002272.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002273.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002274.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002275.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002276.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002277.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002278.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002279.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002280.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002281.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002284.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002285.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002287.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002288.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002290.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002291.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002292.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002294.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002296.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002297.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002298.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002299.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002301.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002303.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002304.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002305.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002306.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002308.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002309.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002310.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002311.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002312.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002313.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002314.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002315.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002316.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002318.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002320.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002321.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002322.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002323.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002324.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002325.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002326.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002327.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002328.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002329.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002330.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002331.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002332.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002333.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002334.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002336.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002337.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002338.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002339.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002340.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002341.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002342.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002343.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002344.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002347.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002349.tif  \n","  inflating: data/fabi-data/train/18/none-0000002350.tif  \n","  inflating: data/fabi-data/train/18/none-0000002351.tif  \n","  inflating: data/fabi-data/train/18/none-0000002352.tif  \n","  inflating: data/fabi-data/train/18/none-0000002353.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002354.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002355.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002356.tif  \n","  inflating: data/fabi-data/train/18/none-0000002357.tif  \n","  inflating: data/fabi-data/train/18/none-0000002359.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002360.png  \n","  inflating: data/fabi-data/train/18/none-0000002361.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002362.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002363.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002364.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002365.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002366.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002367.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002368.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002369.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002370.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002371.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002372.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002373.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002374.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002375.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002376.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002377.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002378.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002379.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002380.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002381.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002382.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002383.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002385.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002386.tif  \n","  inflating: data/fabi-data/train/18/none-0000002387.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002388.tif  \n","  inflating: data/fabi-data/train/18/none-0000002389.tif  \n","  inflating: data/fabi-data/train/18/none-0000002390.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002392.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002393.tif  \n","  inflating: data/fabi-data/train/18/Nymph3.jpg  \n","  inflating: data/fabi-data/train/18/Nymph4.jpg  \n","  inflating: data/fabi-data/train/18/NymphLargeStackD.tif  \n","  inflating: data/fabi-data/train/18/Spondyliapsis adult 1a.jpg  \n","  inflating: data/fabi-data/train/18/Spondyliaspis wing different angle.jpg  \n","  inflating: data/fabi-data/train/18/Spondyliaspis wing.jpg  \n","  inflating: data/fabi-data/train/18/Stack with mesurements.jpg  \n","  inflating: data/fabi-data/train/18/Stack2C.jpg  \n","  inflating: data/fabi-data/train/18/Stack3B.jpg  \n","  inflating: data/fabi-data/train/18/Stack3C.jpg  \n","  inflating: data/fabi-data/train/18/StackB.jpg  \n","   creating: data/fabi-data/train/19/\n","  inflating: data/fabi-data/train/19/DSCN0817.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0819.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0821.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0827.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0828.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1595.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1596.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1597.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5446.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5456.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5457.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5458.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5463.JPG  \n","  inflating: data/fabi-data/train/19/eggs3.jpg  \n","  inflating: data/fabi-data/train/19/eggs4.jpg  \n","  inflating: data/fabi-data/train/19/Image14(2).jpg  \n","  inflating: data/fabi-data/train/19/Image21.jpg  \n","  inflating: data/fabi-data/train/19/Image26(2).jpg  \n","  inflating: data/fabi-data/train/19/Image43.jpg  \n","  inflating: data/fabi-data/train/19/Image46.jpg  \n","  inflating: data/fabi-data/train/19/IMG_0584.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0585.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0635.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0640.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0641.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0643.JPG  \n","  inflating: data/fabi-data/train/19/none-0000001395.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001396.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001400.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001402.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001403.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001404.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001406.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001407.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001408.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001409.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001410.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001411.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001412.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001413.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001414.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001415.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001416.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001417.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001418.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001419.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001420.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001421.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001422.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001423.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001424.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001426.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001427.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001428.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001429.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001430.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001431.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001432.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001435.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001437.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001439.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001441.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001442.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001443.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001444.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001445.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001446.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001447.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001448.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001449.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001450.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001451.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001453.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001454.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001455.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001456.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001457.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001458.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001460.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001462.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001463.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001464.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001465.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001466.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001467.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001470.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001471.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001473.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001474.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001475.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001476.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001477.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001478.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001479.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001480.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001481.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001482.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001484.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001485.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001486.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001488.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 2.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 3.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 4.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 7.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 1.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 3.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 5.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph2.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph3 adjusted.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph3.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph4.jpg  \n","   creating: data/fabi-data/train/2/\n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_male1.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_male2.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_ovipositing in Thaumastocoris peregrinus eggs1.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_with Thaumastocoris peregrinus eggs 2.jpg  \n","   creating: data/fabi-data/train/20/\n","  inflating: data/fabi-data/train/20/DSCN1695.JPG  \n","  inflating: data/fabi-data/train/20/DSCN1702.JPG  \n","  inflating: data/fabi-data/train/20/DSCN1708.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2289.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2290.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2314.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2324.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2329.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2341.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2342.JPG  \n","   creating: data/fabi-data/train/21/\n","  inflating: data/fabi-data/train/21/58BF974D-D89C-4CC6-AD15-9A702742AF8E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/A78D97FB-7738-4289-A3DD-9CD6B3F31A12_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/E8D65B8D-BDF5-4DA6-B8A4-3197AE190C51_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/FAA4C561-105A-4A9C-9ED5-9B945CD7B9F3_1_105_c.jpeg  \n","   creating: data/fabi-data/train/22/\n","  inflating: data/fabi-data/train/22/Armillaria fuscipes mushrooms Martin.bmp  \n","  inflating: data/fabi-data/train/22/Armillaria GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria internal_Izette.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria mellea clumps mushrooms Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria mellea mushroom clumps Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria mellea mushrooms on stump Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria montagnei mushrooms and spores Argentina 2012.jpg  \n","   creating: data/fabi-data/train/23/\n","  inflating: data/fabi-data/train/23/Austropuccinia1.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia2.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia3- ginna.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia3.JPG  \n","  inflating: data/fabi-data/train/23/none-0000000729.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000731.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000733.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000734.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000735.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000737.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000738.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000739.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000740.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000741.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000743.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000744.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000745.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000746.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000748.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000749.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000750.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000751.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000752.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000753.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000754.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000755.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000756.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000757.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000758.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000759.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000761.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000762.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000763.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000764.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000765.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000766.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000767.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000768.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000771.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000772.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000773.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000774.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000775.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000778.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000780.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000782.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000783.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000785.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000786.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000787.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000788.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000789.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000790.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000791.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000793.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000794.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000795.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000797.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000798.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000799.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000800.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000801.tif  \n","  inflating: data/fabi-data/train/23/none-0000000802.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000804.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000806.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000807.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000808.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000811.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000814.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000815.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000816.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000818.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000819.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000820.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000821.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000822.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000823.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000824.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000825.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000826.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000827.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000828.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000829.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000830.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000831.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000832.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000833.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000835.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000836.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000837.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000838.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000839.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000840.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000841.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000842.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000843.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000844.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000846.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000847.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000848.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000849.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000850.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000852.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000853.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000854.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000855.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000856.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000857.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000859.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000861.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000863.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000864.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000865.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000866.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000867.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000868.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000869.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000870.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000871.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000872.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000873.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000874.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000875.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000877.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000878.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000879.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000880.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000881.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000882.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000883.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000884.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000885.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000886.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000887.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000888.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000889.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000890.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000891.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000892.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000894.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000896.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000897.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000898.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000899.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000900.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000901.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000902.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000903.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000904.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000905.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000906.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000907.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000908.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000909.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000910.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000911.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000912.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000913.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000914.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000915.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000918.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000919.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000920.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000921.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000922.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000923.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000924.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000925.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000926.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000927.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000928.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000930.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000931.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000933.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000934.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000935.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000936.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000937.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000938.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000941.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000942.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000943.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000945.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000946.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000948.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000949.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000950.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000951.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000952.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000954.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000955.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000958.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000959.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000960.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000961.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000962.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000963.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000964.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000965.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000966.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000968.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000969.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000970.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000971.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000972.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000973.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000974.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000976.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000977.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000978.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000979.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000980.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000981.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000982.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000983.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000985.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000986.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000987.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000988.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000989.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000990.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000992.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000993.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000994.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000995.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000996.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000997.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000998.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000999.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001000.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001003.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001004.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001005.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001006.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001007.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001008.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001009.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001010.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001011.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001013.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001014.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001016.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001018.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001019.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001020.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001021.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001022.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001023.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001026.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001027.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001028.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001030.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001031.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001032.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001033.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001034.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001035.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001036.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001037.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001038.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001039.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001041.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001042.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001043.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001044.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001045.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001046.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001047.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001048.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001049.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001052.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001053.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001055.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001056.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001057.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001058.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001059.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001060.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001061.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001062.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001063.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001064.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001066.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001067.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001068.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001069.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001070.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001071.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001072.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001074.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001075.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001077.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001079.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001081.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001082.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001083.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001084.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001085.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001086.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001088.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001089.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001090.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001092.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001094.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001095.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001096.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001098.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001099.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001100.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001101.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001102.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001103.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001105.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001107.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001108.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001109.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001110.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001111.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001112.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001113.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001114.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001115.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001116.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001118.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001119.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001121.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001122.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001123.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001124.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001125.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001126.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001128.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001129.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001130.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001132.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001133.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001134.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001137.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001138.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001139.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001140.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001141.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001142.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001143.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001144.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001147.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001148.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001149.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001151.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001153.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001156.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001157.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001161.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001164.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001165.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001166.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001167.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001168.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001169.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001171.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001172.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001173.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001174.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001175.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001176.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001177.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001178.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001180.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001181.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001182.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001184.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001186.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001187.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001188.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001189.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001190.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001191.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001192.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001193.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001194.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001196.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001197.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001198.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001199.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001200.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001201.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001202.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001204.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001205.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001206.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001207.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001208.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001211.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001212.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001215.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001216.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001217.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001219.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001220.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001221.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001222.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001223.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001225.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001226.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001227.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001228.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001229.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001230.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001231.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001232.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001233.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001234.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001235.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001236.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001237.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001239.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001240.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001242.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001243.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001244.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001245.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001247.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001249.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001250.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001251.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001252.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001253.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001254.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001255.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001256.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001257.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001258.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001260.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001261.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001262.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001263.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001264.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001265.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001271.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001272.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001273.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001274.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001275.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001277.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001278.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001279.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001280.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001283.jpg  \n","   creating: data/fabi-data/train/24/\n","  inflating: data/fabi-data/train/24/Bacterial wilt cause by _i_Erwinia psidii_i_.png  \n","  inflating: data/fabi-data/train/24/Blisters on bark caused by _i_Erwinia psidii_i_.png  \n","  inflating: data/fabi-data/train/24/Dieback caused by _i_Erwinia psidii_i_ .png  \n","  inflating: data/fabi-data/train/24/Internal discolouration caused by _i_Erwinia psidii_i_.png  \n","   creating: data/fabi-data/train/25/\n","  inflating: data/fabi-data/train/25/Bacterial exudate of _i_Ralstonia solanacearum_i_.png  \n","  inflating: data/fabi-data/train/25/Bacterial Wilt external Ralstonia pseudosolanacearum_Izette.JPG  \n","  inflating: data/fabi-data/train/25/Bacterial wilt internal Ralstonia pseudosolanacearum_Izette.JPG  \n","  inflating: data/fabi-data/train/25/Discoloured wood caused by _i_Ralstonia solanacearum_i_.png  \n","   creating: data/fabi-data/train/26/\n","  inflating: data/fabi-data/train/26/Botryosphaeria canker external Neofusicoccum spp_Izette.jpg  \n","   creating: data/fabi-data/train/27/\n","  inflating: data/fabi-data/train/27/Fig. 3. A. Botryosphaeria canker on Eucalyptus sp. (F Jami).JPG  \n","  inflating: data/fabi-data/train/27/Fig. 4 Fruiting structures with abundant mature conidia.jpg  \n","  inflating: data/fabi-data/train/27/Fig. 4 Fruiting structures with abundant mature conidia2.jpg  \n","   creating: data/fabi-data/train/28/\n","  inflating: data/fabi-data/train/28/Calonectria leaf blight in _i_Eucalyptus plantation_i_.jpg  \n","   creating: data/fabi-data/train/29/\n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados02.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados03.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados04.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados05.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados07.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados08.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados09.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados10.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados12.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados13.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados15.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados16.JPG  \n","   creating: data/fabi-data/train/3/\n","  inflating: data/fabi-data/train/3/0C5FDA49-F7CC-461A-ACAB-8CEF936B2FE7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/1821C4A9-0060-49BE-875A-D2744FCDF300_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/26436902-1326-48AC-94C1-D954563010E8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/276FEA3C-D13E-485A-A77A-20E7C27AEEB7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/2FE42382-B37D-4589-B270-C18228444240_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/301FB1EE-42D3-4F1F-9557-AFEAC0D5ACED_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/3A380EEF-EDAA-4536-BAD3-53647701CA33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/40DF7AC2-02EB-431F-8187-C14CE5FDF3E5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/44CB9D90-2F90-45AC-BE5B-538DB9C1AF33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/5BC678E2-0A5E-400D-81D6-EFD53035639A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/684A08F6-7BC3-4457-A9DF-8CACFE2F1AB0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/7666AC27-C65B-4142-95E0-8FFF0A777124_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/8208F627-9878-4D90-B66C-5F86798F4212_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/9F3A3A9A-FB7A-4B9A-B40C-5525472CAF3A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/A0439CCD-AB23-4F24-A2A4-F87F227B439A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/BEBEEB9B-4406-40C0-BC1C-EC8464BA873F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/D7FD66DA-BC1C-451F-8AAA-D8A5FAF8F23A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/DE9DC524-5B08-4B39-BE83-C1B056B79B2C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/DSC000012.JPG  \n","  inflating: data/fabi-data/train/3/DSC00004.JPG  \n","  inflating: data/fabi-data/train/3/DSC00005.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0028.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0055.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0056.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0095.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0097.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0101.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0117.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0364.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0395.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0399.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0802.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0984.JPG  \n","  inflating: data/fabi-data/train/3/DSCN1332.JPG  \n","  inflating: data/fabi-data/train/3/DSCN1580.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4175.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4178.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4181.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4182.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4185.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4204.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5066.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5070.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5071.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5073.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5490.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5494.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5500.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5501.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5503.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5507.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5508.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5509.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5511.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5579.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5581.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5584.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5585.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5586.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5587.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5590.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5592.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5594.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5595.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5596.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5598.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5602.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5603.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5604.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6962.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6963.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6964.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6975.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7292.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7294.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7295.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7296.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7300.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7322.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8135.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8136.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8139.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8140.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8147.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8148.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8152.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8153.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8155.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8157.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8159.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8160.JPG  \n","  inflating: data/fabi-data/train/3/E02A74C2-39AE-4ADA-B062-448A5F21DD79_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/E6979849-8FE9-4D9B-86D4-BB6CE1C225C2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/FC0B348D-0788-4094-B2AA-68CD3DA014C2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/IMG_0674.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0675.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0677.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0678.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0679.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0680.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0681.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0685.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0686.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0696.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0697.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0698.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0700.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0701.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0702.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0711.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0712.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0713.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0714.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0715.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0716.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0717.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0718.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0721.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0723.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0725.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0726.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0727.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0728.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0735.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0740.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0742.JPG  \n","  inflating: data/fabi-data/train/3/larvae and tunnels.JPG  \n","   creating: data/fabi-data/train/30/\n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 07.JPG  \n","   creating: data/fabi-data/train/31/\n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 07.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 08.JPG  \n","  inflating: data/fabi-data/train/31/Diplodia1 Ginna.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia1.JPG  \n","  inflating: data/fabi-data/train/31/Diplodia2.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia3.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia4.JPG  \n","   creating: data/fabi-data/train/32/\n","  inflating: data/fabi-data/train/32/Fusarium circinatum collar rot.jpg  \n","   creating: data/fabi-data/train/33/\n","  inflating: data/fabi-data/train/33/Glycaspis 2.png  \n","  inflating: data/fabi-data/train/33/Glycaspis 3.png  \n","  inflating: data/fabi-data/train/33/P bliteus 1.png  \n","  inflating: data/fabi-data/train/33/P bliteus 2.png  \n","   creating: data/fabi-data/train/34/\n","  inflating: data/fabi-data/train/34/_i_Acacia mearnsii_i_ gummosis.JPG  \n","   creating: data/fabi-data/train/35/\n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 05.JPG  \n","   creating: data/fabi-data/train/36/\n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/36/Fuarium circinatum caramel coloured lessions and cankers.JPG  \n","  inflating: data/fabi-data/train/36/Fusarium circinatum resin exudation down main stem.JPG  \n","   creating: data/fabi-data/train/37/\n","  inflating: data/fabi-data/train/37/Powdery mildew GM Granados02.JPG  \n","  inflating: data/fabi-data/train/37/Powdery mildew GM Granados03.JPG  \n","   creating: data/fabi-data/train/38/\n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 06.JPG  \n","   creating: data/fabi-data/train/39/\n","  inflating: data/fabi-data/train/39/none-0000001284.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001285.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001286.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001287.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001289.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001290.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001291.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001292.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001293.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001294.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001295.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001296.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001297.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001298.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001299.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001300.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001301.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001303.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001305.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001306.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001307.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001308.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001309.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001310.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001311.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001313.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001314.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001315.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001317.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001318.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001321.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001322.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001323.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001325.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001326.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001327.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001329.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001332.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001333.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001334.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001335.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001336.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001337.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001338.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001340.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001341.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001342.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001343.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001344.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001345.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001346.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001348.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001349.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001350.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001352.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001353.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001354.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001355.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001356.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001357.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001359.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001360.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001361.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001362.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001364.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001365.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001367.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001368.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001369.jpg  \n","  inflating: data/fabi-data/train/39/Quambalaria GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/39/Quambalaria GM Granados 03.JPG  \n","   creating: data/fabi-data/train/4/\n","  inflating: data/fabi-data/train/4/Ctenarytaina male 1 SBush.tif  \n","  inflating: data/fabi-data/train/4/Ctenarytaina male 3 SBush.tif  \n","  inflating: data/fabi-data/train/4/Ctenarytaina male resized SBush.tif  \n","  inflating: data/fabi-data/train/4/Nymph 1 SBush.tif  \n","  inflating: data/fabi-data/train/4/Nymph 2 SBush.tif  \n","   creating: data/fabi-data/train/40/\n","  inflating: data/fabi-data/train/40/Fig.1b_FABInews.jpg  \n","  inflating: data/fabi-data/train/40/Fig.1e_Greyling_2016.png  \n","  inflating: data/fabi-data/train/40/Fig1.e_Greyling_2016.png  \n","  inflating: data/fabi-data/train/40/Fig1d_Dell_2008.png  \n","   creating: data/fabi-data/train/41/\n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5706.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5707.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5715.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5718.JPG  \n","   creating: data/fabi-data/train/42/\n","  inflating: data/fabi-data/train/42/Fig1_Conio.JPG  \n","   creating: data/fabi-data/train/43/\n","  inflating: data/fabi-data/train/43/none-0000000001.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000002.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000003.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000004.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000005.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000006.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000008.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000013.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000014.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000015.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000016.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000019.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000021.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000022.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000023.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000024.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000025.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000026.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000027.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000028.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000029.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000030.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000032.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000033.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000034.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000036.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000037.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000039.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000040.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000042.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000043.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000044.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000045.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000047.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000048.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000049.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000053.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000055.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000056.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000057.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000058.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000059.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000061.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000062.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000063.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000064.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000065.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000066.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000067.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000068.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000069.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000070.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000071.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000072.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000074.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000075.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000076.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000077.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000080.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000081.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000082.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000083.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000084.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000085.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000086.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000088.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000089.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000090.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000091.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000093.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000094.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000095.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000097.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000098.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000099.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000100.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000101.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000102.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000103.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000104.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000106.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000107.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000108.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000109.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000110.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000111.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000112.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000113.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000114.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000116.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000117.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000118.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000119.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000120.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000121.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000123.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000124.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000125.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000126.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000127.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000128.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000129.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000132.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000134.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000135.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000136.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000137.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000139.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000140.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000141.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000143.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000145.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000146.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000147.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000148.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000151.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000153.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000154.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000155.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000157.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000158.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000160.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000161.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000162.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000163.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000165.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000166.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000167.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000168.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000169.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000170.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000171.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000173.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000174.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000175.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000176.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000178.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000179.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000180.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000181.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000182.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000185.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000186.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000188.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000190.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000192.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000193.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000195.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000196.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000197.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000198.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000199.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000200.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000201.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000202.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000203.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000204.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000205.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000206.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000207.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000208.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000209.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000210.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000211.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000212.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000213.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000214.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000215.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000217.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000218.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000219.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000220.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000221.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000222.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000223.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000224.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000225.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000226.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000227.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000229.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000230.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000234.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000235.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000238.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000239.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000241.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000242.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000243.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000245.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000246.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000247.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000248.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000249.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000250.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000251.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000252.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000253.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000255.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000256.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000257.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000258.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000259.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000260.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000263.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000266.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000267.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000268.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000269.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000270.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000271.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000273.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000274.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000275.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000276.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000277.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000280.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000281.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000282.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000283.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000284.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000285.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000286.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000287.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000288.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000289.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000290.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000293.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000294.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000295.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000296.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000297.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000298.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000299.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000301.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000302.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000303.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000304.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000305.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000306.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000307.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000309.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000310.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000312.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000313.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000314.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000316.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000317.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000318.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000319.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000320.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000322.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000324.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000326.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000327.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000328.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000329.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000330.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000331.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000332.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000333.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000335.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000336.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000337.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000338.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000339.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000340.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000341.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000343.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000344.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000345.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000347.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000348.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000349.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000350.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000351.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000353.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000354.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000356.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000357.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000358.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000359.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000360.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000361.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000362.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000363.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000365.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000366.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000367.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000368.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000370.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000371.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000372.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000373.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000374.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000375.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000376.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000377.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000379.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000380.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000381.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000383.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000385.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000387.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000388.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000389.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000390.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000391.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000392.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000393.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000394.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000395.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000396.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000397.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000398.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000400.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000401.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000402.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000403.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000404.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000405.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000406.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000407.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000408.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000409.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000410.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000411.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000412.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000413.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000414.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000416.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000417.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000418.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000419.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000420.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000421.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000423.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000424.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000425.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000426.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000427.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000428.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000429.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000430.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000431.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000432.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000433.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000434.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000435.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000436.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000437.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000439.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000440.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000441.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000442.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000443.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000444.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000445.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000446.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000447.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000449.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000450.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000452.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000453.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000454.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000455.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000457.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000458.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000459.jpg  \n","  inflating: data/fabi-data/train/43/T.destructans KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/43/T.destructans KZN GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/43/Teratoshaeria destructans.jpg  \n","   creating: data/fabi-data/train/44/\n","  inflating: data/fabi-data/train/44/none-0000000460.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000461.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000462.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000464.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000465.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000466.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000467.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000468.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000469.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000470.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000472.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000473.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000475.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000476.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000477.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000478.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000479.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000482.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000483.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000484.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000485.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000486.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000487.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000488.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000490.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000491.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000492.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000493.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000494.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000495.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000496.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000497.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000500.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000501.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000502.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000504.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000505.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000507.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000508.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000509.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000510.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000511.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000512.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000514.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000515.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000516.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000517.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000519.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000520.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000523.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000526.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000529.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000530.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000531.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000532.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000533.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000534.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000535.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000536.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000537.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000538.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000539.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000540.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000542.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000543.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000544.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000547.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000548.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000549.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000550.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000551.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000552.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000553.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000554.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000555.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000556.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000557.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000558.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000561.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000563.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000565.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000566.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000568.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000569.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000570.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000572.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000573.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000574.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000575.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000576.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000577.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000578.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000579.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000581.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000582.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000583.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000584.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000585.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000586.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000587.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000588.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000589.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000591.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000592.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000593.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000594.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000595.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000596.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000597.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000598.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000599.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000600.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000601.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000602.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000603.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000604.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000605.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000606.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000607.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000608.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000609.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000610.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000611.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000612.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000613.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000614.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000615.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000617.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000620.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000621.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000622.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000623.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000624.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000625.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000626.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000627.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000628.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000629.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000630.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000632.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000634.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000635.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000636.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000637.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000638.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000639.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000640.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000643.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000645.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000646.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000647.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000648.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000649.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000651.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000652.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000654.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000655.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000657.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000658.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000659.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000660.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000661.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000663.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000664.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000666.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000668.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000669.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000670.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000671.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000672.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000673.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000674.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000676.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000677.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000679.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000680.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000681.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000682.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000683.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000684.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000685.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000686.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000690.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000692.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000693.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000694.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000695.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000697.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000698.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000699.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000700.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000701.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000702.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000704.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000705.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000706.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000708.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000709.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000711.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000712.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000713.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000714.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000715.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000716.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000717.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000718.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000719.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000720.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000721.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000722.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000724.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000725.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000727.jpg  \n","  inflating: data/fabi-data/train/44/T.nubilosa KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/44/T.nubilosa KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/44/T.nubilosa_Bulwer_KZN_GMGranados01.JPG  \n","   creating: data/fabi-data/train/45/\n","  inflating: data/fabi-data/train/45/1_Wattle rust.jpg  \n","  inflating: data/fabi-data/train/45/2_Wattle rust.jpg  \n","  inflating: data/fabi-data/train/45/3_Wattle rust.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados01.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados04.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados05.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados06.JPG  \n","   creating: data/fabi-data/train/5/\n","  inflating: data/fabi-data/train/5/DSCN1541.JPG  \n","  inflating: data/fabi-data/train/5/DSCN1549.JPG  \n","  inflating: data/fabi-data/train/5/Euproctis cocoon B.Hurley.JPG  \n","  inflating: data/fabi-data/train/5/Euproctis terminalis B. Hurley.JPG  \n","   creating: data/fabi-data/train/6/\n","  inflating: data/fabi-data/train/6/008BC3B2-28D6-45B9-A396-8822E5E3A58B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/067988C4-4AFC-4DA3-A5D5-687876C4B4A9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/0F8B2F51-6A54-4A64-BADC-CC0EFFE0934A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/10B0A8D6-8FD2-406B-9F34-9AD5A56C9420_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/17F73DE4-F546-458F-80F1-DCC7499D37BD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/1C2840B7-C814-4172-9B3C-5BB1B7BC4757_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/1DF0306F-7101-4631-A7CE-8DF7119E0FA1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/25F81054-30FB-4ABA-A07F-40605F97A934_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/2A87BA12-D302-40C3-8569-094A73128648_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/31C7381C-E429-48C3-A6F6-9E6103B86052_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/36CFC62C-3086-4F07-9C8E-714F2384BDE9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/37A1A534-49F9-4B08-B57C-9B5D536094B5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3A63507C-1ABB-4F55-B8BB-7F2BBC9B6400_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3A83DF2D-05D0-431F-ACEA-40555D61C20A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3FC127DE-2627-46ED-8C67-D906B08A6C86_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/406B6686-9138-49E1-9773-318D220E51C0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/410388D7-E42A-4913-809A-2B8BC0442EC6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/41C7A748-8639-4342-906B-98AC55BE0ED8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/44DF8EC3-6AFC-4946-8A9B-40EA3AC95B8B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/4592FF6A-E4DA-4C36-BAF8-62DAC82F88CA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/47F54696-67B1-4D5D-9933-6178C2488712_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/4900B533-0FA9-4FC5-AEE3-D832A9976AAC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/52A34225-1C60-4680-B04D-B1792C38816F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/55A7DDA7-36BE-4A98-8AAB-FD031C8085AA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5B4D5D9D-D3AE-43C6-81A3-82392A6E3248_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5B88AFAE-D521-4BF8-ADC4-51FC951A9F25_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5C639684-900F-4D05-B13F-2843921F3055_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5D49D5A6-D1E5-4019-9651-82B6AA8EA3A1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/653DE0A2-63F5-420A-B4D0-83E2618DF34A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6699EB04-A667-40C4-9534-F78893A6DC1F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6A28C7FA-3EAE-440C-B3CE-5B6116C6701A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6D1B7897-BB90-4641-925F-EE0C4F6A26C4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6D6AE712-2E5C-49D2-80E6-002D14BCAF3D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6ED38195-2E97-4A53-A527-51988D543EA7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/714D13EC-D9E5-4502-BFA6-80A20D8F06EB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/792CC3A1-F25E-407F-AA5B-0E3232483F07_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/7F123EB0-6C9F-48AF-A128-0A8A85B79D09_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/85A96CB6-A4EC-4A6A-B5C9-8C8094EF41A5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/861FFB9D-5531-4642-8952-A54A4BF2B663_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8E797FD5-764A-4106-8397-25B71F84731C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8F62CAF6-E978-4A22-93AC-27B1B280EF45_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8FAA8E86-0973-4580-A425-312E7AB2994C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9101F939-105B-40DA-BA93-2CFEB3FA403D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/924D457F-2FD6-4267-8556-1DE0511C80EE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9289C91F-6C77-4A79-83DE-497EF75195C4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/98E942F6-E83D-4FF1-ABFF-B252C6EA3FBB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/98F72AC5-B862-4C0F-A210-56EE67431AFE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9E0C2B5F-8283-471F-BBFF-9B28A568896D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9E79BC7E-504C-4EBD-9371-9512DB8D8CA7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A06EDB66-69B9-43B5-9122-E2D069F77381_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A1C66A43-6445-4541-A771-E908E8275172_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A2A5E574-BE8F-429C-891F-56EC825E6B6F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A475D807-1F06-45BF-9ABC-EDE7B2E68DEA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/ABCDBE90-942B-423B-A0D3-75FFFC366940_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/AD1C0931-F655-43B8-A6BE-006D76310911_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 1 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 6 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 7 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult male and female pic2SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult pic1SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult pic2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/B3272EC8-1856-4251-9BA2-4D382451CA1E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/B4DCC730-0B2F-41AB-A322-F7D9A8AC1E20_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C03B96D6-3E0D-4D79-B5A4-13DE12EB5314_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C04F90D5-BD60-40D8-AAB1-843738491FA0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C05CEC9E-4080-4DFE-9EC4-1F584E249B02_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C7F56B3D-6E73-4A02-B284-18CA7174AC8E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C8BE8542-DA18-4A7B-A04C-4D9C1799AF14_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/CF814A5F-819F-4848-B4C4-3B7DA3A1AAF8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/D0C4528B-7057-454D-831B-9566E339C8C7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/DCC84013-65D4-4D77-888E-B80221B916C3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/DCDAFC70-5C5E-4FCF-B2EF-6EA4FAD852B3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/E0DB2F7F-F9DD-49A3-A7ED-45818B8C1D4F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/E93FCE5F-27C0-4B95-8DA2-586C96728BCE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F11737D3-4F4A-4339-9310-1FC4ABD59BB9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F17502FC-4180-4B66-A99F-E57D19A8B593_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F24245A6-D94E-4867-8892-F47891568EE3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F302C29D-0144-4765-9C00-09AAD371B55D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F4CB8B2E-93B4-4F99-B6C9-E997F8B29B38_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F4E1557D-7413-473D-AD1E-137CCA4788E5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F5B4CFE7-E58E-4EF7-841D-8DC4CDAAEADB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F90135E3-52DE-4D0B-B062-E1EDCB1DFA14_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/FDC8CA73-FB4F-4F94-A0F5-471B85D54A4B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/Female 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 6 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 7 two females SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 9 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 1SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 1 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 3 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth instar nymph 2 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb Fifth instar nymph 3 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 7 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 1 to 4SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 1 to 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 2 to 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 4 to 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb male 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb nymph building lerp SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb nymph inside lerp SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly second instar SBush(1).jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly second instar SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly third Instar 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis  Male 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis Female 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis SBush.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002041.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002043.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002044.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002045.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002046.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002047.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002048.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002049.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002050.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002051.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002052.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002055.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002056.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002058.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002059.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002060.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002061.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002063.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002065.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002066.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002067.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002069.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002070.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002071.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002072.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002073.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002074.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002075.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002076.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002077.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002078.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002079.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002081.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002082.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002086.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002087.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002089.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002091.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002092.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002093.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002094.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002095.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002096.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002097.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002098.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002099.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002100.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002101.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002103.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002104.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002105.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002106.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002107.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002109.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002110.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002111.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002114.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002115.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002116.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002118.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002120.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002121.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002122.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002123.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002124.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002125.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002126.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002127.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002130.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002132.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002133.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002136.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002137.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002138.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002139.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002140.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002142.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002145.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002146.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002148.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002149.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002151.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002152.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002154.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002155.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002156.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002157.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002158.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002159.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002161.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002162.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002164.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002166.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002167.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002168.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002169.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002170.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002171.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002172.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002173.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002175.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002176.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002177.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002178.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002180.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002181.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002182.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002183.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002185.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002186.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002189.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002190.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002191.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002192.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002193.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002194.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002195.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002196.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002197.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002199.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002200.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002201.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002202.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002203.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002204.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002205.jpg  \n","   creating: data/fabi-data/train/7/\n","  inflating: data/fabi-data/train/7/1DD57CB0-BFEC-45B9-B2AA-41B8D73A118C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/28691AD0-0258-4D30-8170-394D4D8A7613_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2AA5A159-FB90-4634-ADDE-21FBBB1EAA59_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2F9D85EE-96F1-462B-A174-A26CB53CB378_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2FC02089-0F14-4F28-88CB-0FEBEFC426A1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/307E680B-5376-42FD-977D-068536FBCCBC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/457015D5-3B01-4980-91D3-D642A7D4AE32_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/4A0B9067-B991-4183-B1E7-AD732F081857_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/4BE8C325-C19A-4296-939D-C57E261734CC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/5295E23D-672A-4969-BFC8-BA6521C6924C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/5F340E00-6A07-4826-8A2E-434CBDCAC71F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/73B4ABE3-C24D-4B78-8E62-741793B9F018_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/A81B0FF7-5E8E-4E2C-835B-D7F65A4B5F2C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/A8633E3F-0D82-4BB6-96F9-3E09E4F1321C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/AD52FC80-9E88-4717-8751-AB485DE2B711_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/C9511A33-B2CB-40C4-B073-7B4E9E9A03B2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/CC90ECCD-079A-4F7D-B2FE-58261862CE90_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/DBE6BEF6-8AC8-4BE9-8630-47BB21C2EF8F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/DSC_4180.JPG  \n","  inflating: data/fabi-data/train/7/DSC_4204.JPG  \n","  inflating: data/fabi-data/train/7/E27726E7-101A-446A-A3EE-90DB010D6D85_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E286D740-DCBF-4245-AC9F-AE0DC27DF482_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E2D0BE4A-37BC-4722-BABF-B27914A2FB6F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E6C17234-303F-4516-864C-11D091D235B9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/Egg capsule.jpg  \n","  inflating: data/fabi-data/train/7/Egg packet (2) - Copy.jpg  \n","  inflating: data/fabi-data/train/7/Egg packet (2).jpg  \n","  inflating: data/fabi-data/train/7/Egg packet.jpg  \n","  inflating: data/fabi-data/train/7/Egg_packet_dissected.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult1.tif  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult2.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult3.tif  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult9.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg3.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg4.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg6.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_larvae.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_mating_pair.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_oviposit2.jpg  \n","  inflating: data/fabi-data/train/7/IMG_0338.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0364.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0411.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0415.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0418.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0419.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0420.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0421.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0434.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0439.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0444.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0447.JPG  \n","  inflating: data/fabi-data/train/7/IMG_1801.JPG  \n","  inflating: data/fabi-data/train/7/IMG_1807.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4239.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4245 (2).JPG  \n","  inflating: data/fabi-data/train/7/IMG_4245.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4249.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4462.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4464.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4465.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4466.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4468 (2).JPG  \n","  inflating: data/fabi-data/train/7/IMG_4468.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4469.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4470.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4471.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4472.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4473.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4474.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6090.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6091.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6092.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6112.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6113.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6115.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6116.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6117.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6118.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6119.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6131.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6132.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6133.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6134.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6135.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6136.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6159.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6160.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6162.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6163.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6164.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6165.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6166.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6167.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6169.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6171.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6172.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6173.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6193.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6194.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6196.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6197.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6198.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6200.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6201.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6204.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6205.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6206.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6207.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6208.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6209.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6210.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6211.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6212.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6214.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6215.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6216.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6225.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6226.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6228.JPG  \n","  inflating: data/fabi-data/train/7/none-0000001489.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001490.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001493.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001494.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001495.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001497.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001498.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001499.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001500.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001502.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001503.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001504.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001507.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001508.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001509.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001510.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001511.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001512.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001513.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001514.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001517.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001518.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001519.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001520.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001521.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001522.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001524.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001525.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001526.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001527.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001528.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001529.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001530.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001531.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001532.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001533.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001536.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001537.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001539.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001541.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001542.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001543.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001544.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001545.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001546.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001547.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001548.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001549.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001550.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001552.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001553.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001554.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001555.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001557.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001558.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001561.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001562.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001564.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001566.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001567.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001569.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001570.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001571.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001572.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001573.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001574.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001575.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001576.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001577.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001578.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001579.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001580.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001581.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001582.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001583.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001584.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001585.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001586.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001587.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001588.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001591.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001592.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001593.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001594.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001596.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001599.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001600.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001601.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001603.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001604.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001606.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001607.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001609.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001610.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001612.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001614.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001616.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001617.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001618.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001619.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001620.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001621.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001622.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001624.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001625.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001626.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001627.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001628.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001629.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001630.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001631.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001632.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001633.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001634.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001635.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001636.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001637.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001639.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001640.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001641.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001642.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001644.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001645.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001646.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001647.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001649.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001650.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001651.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001653.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001654.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001655.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001656.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001657.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001658.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001662.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001663.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001664.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001666.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001668.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001669.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001670.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001671.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001672.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001673.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001674.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001676.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001677.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001679.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001680.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001682.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001683.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001684.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001685.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001686.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001687.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001689.tif  \n","  inflating: data/fabi-data/train/7/none-0000001690.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001692.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001695.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001697.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001698.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001699.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001700.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001702.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001703.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001705.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001709.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001710.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001713.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001714.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001715.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001716.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001717.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001718.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001719.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001720.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001721.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001722.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001723.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001724.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001725.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001726.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001727.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001728.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001729.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001731.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001732.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001733.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001734.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001735.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001736.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001737.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001738.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001739.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001740.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001741.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001742.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001743.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001744.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001745.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001746.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001747.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001748.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001749.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001750.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001752.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001753.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001755.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001757.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001758.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001759.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001760.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001761.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001765.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001767.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001768.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001769.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001770.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001771.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001773.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001774.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001775.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001778.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001779.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001780.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001782.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001784.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001785.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001786.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001787.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001788.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001789.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001790.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001791.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001792.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001793.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001795.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001798.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001799.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001800.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001802.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001804.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001805.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001806.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001807.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001808.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001809.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001810.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001811.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001812.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001813.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001814.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001815.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001816.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001818.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001819.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001820.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001821.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001822.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001823.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001824.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001825.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001827.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001829.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001830.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001831.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001833.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001834.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001836.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001837.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001838.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001839.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001840.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001842.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001844.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001845.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001846.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001847.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001848.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001849.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001850.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001851.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001852.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001853.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001855.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001856.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001857.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001858.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001859.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001860.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001861.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001862.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001864.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001865.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001866.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001868.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001869.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001870.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001871.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001872.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001873.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001874.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001877.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001878.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001879.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001880.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001881.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001883.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001885.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001887.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001889.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001890.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001891.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001892.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001893.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001894.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001895.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001896.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001897.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001898.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001899.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001900.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001901.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001902.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001903.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001904.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001906.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001907.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001908.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001909.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001910.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001911.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001912.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001913.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001914.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001915.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001916.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001917.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001918.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001920.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001921.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001922.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001923.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001924.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001925.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001926.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001927.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001928.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001929.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001930.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001931.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001932.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001933.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001934.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001936.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001937.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001938.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001939.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001940.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001941.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001942.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001944.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001946.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001947.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001948.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001951.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001952.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001953.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001954.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001955.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001956.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001957.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001958.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001959.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001960.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001961.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001963.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001964.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001966.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001967.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001968.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001969.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001970.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001971.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001972.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001974.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001975.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001976.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001977.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001980.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001981.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001983.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001984.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001985.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001986.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001987.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001988.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001989.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001991.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001992.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001993.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001995.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001996.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001998.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001999.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002000.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002001.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002002.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002004.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002007.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002008.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002009.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002010.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002012.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002014.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002015.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002016.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002018.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002019.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002020.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002022.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002023.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002025.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002026.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002027.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002029.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002031.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002032.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002033.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002034.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002035.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002038.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002039.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002040.jpg  \n","   creating: data/fabi-data/train/8/\n","  inflating: data/fabi-data/train/8/1790F922-A855-4344-AD90-8B36530B71B9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/34210192-72DB-4F8C-971C-0BE8B2729412_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/36A5E91F-0B22-44B3-B65E-2F9D8B385502_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/394A3D8D-E242-4644-B0CA-6B2CB88F6561_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/404AD23D-88D0-4918-9CF6-A0C4A79361E9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/52C32865-F47F-4CA7-A49B-534F8F31B8CE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/572FBADC-28B6-4376-BFF3-FC29A6C87C0C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/80E39107-4E33-4DFD-B0EF-C51B242B1E09_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/D9C1DFA0-BC15-4218-A21E-CAF2B5A8953D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/Dissected_gall.jpg  \n","  inflating: data/fabi-data/train/8/Dissected_gall2.jpg  \n","  inflating: data/fabi-data/train/8/Dissected_gall4.jpg  \n","  inflating: data/fabi-data/train/8/DSCN5873.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6606.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6608.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6610.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6612.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6614.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6615.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6617.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6618.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6619.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6620.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6621.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6622.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6624.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6625.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6626.JPG  \n","  inflating: data/fabi-data/train/8/E8B8A6B0-208E-497C-89F1-BD949E702178_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/EA08DD96-0C39-4B8A-A10B-632E321C2DFE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/ECA50AD6-98EF-4400-A749-3B7D8E6F1C2B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/IMG_0336.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0337.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0338.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0340.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0346.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0349.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0354.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0355.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0370.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1867.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1870.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1874.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1969.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1970.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1987.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1990.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1992.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1999.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2001.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2002.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2012.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe female 1.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe female 1b.tif  \n","  inflating: data/fabi-data/train/8/Leptocybe female 2a.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe female 4a.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 1.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 2.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 4.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 5.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 6.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 7.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 8.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe Samantha Bush.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe1.JPG_Oviposition_scarring.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe2.JPG_Egg.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe3.JPG_Emergence_hole.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe5.JPG_Gall_formation_on_the_leaf_midrib.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa resized.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa2.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa3.jpg  \n","   creating: data/fabi-data/train/9/\n","  inflating: data/fabi-data/train/9/Megastigmus female brown 1a.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus female brown 2a.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus female brown 2b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus Female cream 1b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus male cream 1b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus pretorianensis.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus zebrinus.jpg  \n"]}],"source":["# Downlaod and extract fabi data\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF\" -O fabi-data.zip \u0026\u0026 rm -rf /tmp/cookies.txt\n","\n","!unzip fabi-data.zip -d data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkVR32jQqNjx"},"outputs":[],"source":["# # Downlaod cnn script\n","# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1RHCjl0B3hKDtftje-wzEJxkAkzF36GPo' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1RHCjl0B3hKDtftje-wzEJxkAkzF36GPo\" -O vit.py \u0026\u0026 rm -rf /tmp/cookies.txt\n","\n","# # https://drive.google.com/file/d/1RHCjl0B3hKDtftje-wzEJxkAkzF36GPo/view?usp=sharing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14751,"status":"ok","timestamp":1696356984845,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"MmQcWPwjQv6o","outputId":"1b9e9d96-321c-4f65-e6b7-f523ea09cd9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-10-03 18:16:10--  https://docs.google.com/uc?export=download\u0026confirm=t\u0026id=1XMy56X5ASdTz92RLl660fJXNPCngA4Im\n","Resolving docs.google.com (docs.google.com)... 172.217.194.101, 172.217.194.100, 172.217.194.139, ...\n","Connecting to docs.google.com (docs.google.com)|172.217.194.101|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-14-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a37vv4hn9psj16h5napct73evshqqj2m/1696356900000/11183032721846533402/*/1XMy56X5ASdTz92RLl660fJXNPCngA4Im?e=download\u0026uuid=0d70fe83-6e32-4424-8210-3f930c7297ab [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-10-03 18:16:10--  https://doc-14-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a37vv4hn9psj16h5napct73evshqqj2m/1696356900000/11183032721846533402/*/1XMy56X5ASdTz92RLl660fJXNPCngA4Im?e=download\u0026uuid=0d70fe83-6e32-4424-8210-3f930c7297ab\n","Resolving doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)... 64.233.170.132, 2404:6800:4003:c1a::84\n","Connecting to doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)|64.233.170.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 319046827 (304M) [application/x-zip-compressed]\n","Saving to: ‘vit-model.zip’\n","\n","vit-model.zip       100%[===================\u003e] 304.27M  34.2MB/s    in 9.6s    \n","\n","2023-10-03 18:16:21 (31.7 MB/s) - ‘vit-model.zip’ saved [319046827/319046827]\n","\n","Archive:  vit-model.zip\n","  inflating: vit-model/trainer_state.json  \n","  inflating: vit-model/training_args.bin  \n","  inflating: vit-model/preprocessor_config.json  \n","  inflating: vit-model/config.json   \n","  inflating: vit-model/pytorch_model.bin  \n"]}],"source":["# Downlaod ip102 vit run 1 model\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1XMy56X5ASdTz92RLl660fJXNPCngA4Im' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1XMy56X5ASdTz92RLl660fJXNPCngA4Im\" -O vit-model.zip \u0026\u0026 rm -rf /tmp/cookies.txt\n","\n","# https://drive.google.com/file/d/1XMy56X5ASdTz92RLl660fJXNPCngA4Im/view?usp=sharing\n","# https://drive.google.com/file/d/1XMy56X5ASdTz92RLl660fJXNPCngA4Im/view?usp=sharing\n","\n","!unzip vit-model.zip -d vit-model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoZCqiF2TEQO"},"outputs":[],"source":["# Downlaod ip102 run 10 model\n","# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1Vu2u_fx3ee57ekNrD2uh7VtDNMaZzEMd' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1Vu2u_fx3ee57ekNrD2uh7VtDNMaZzEMd\" -O config.json \u0026\u0026 rm -rf /tmp/cookies.txt\n","\n","# https://drive.google.com/file/d/1Vu2u_fx3ee57ekNrD2uh7VtDNMaZzEMd/view?usp=sharing\n","\n","# config10.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej1I0zV6_yv8"},"outputs":[],"source":["\n","    # --dataset_name mnist \\\n","    # --output_dir output/cnn/mnist/mnistoutputs_1/ \\\n","    # --model_type resnet \\\n","    # --config_name vit-model\\ip102\\config.json \\\n","\n","# !python vit-ip102-fabi.py \\\n","#     --model_name vit-model/ \\\n","#     --train_dir data/fabi-data/train \\\n","#     --validation_dir data/fabi-data/test \\\n","#     --output_dir drive/MyDrive/hons-research/output/vit/ip102fabi_outputs_1/ \\\n","#     --remove_unused_columns False \\\n","#     --do_train \\\n","#     --do_eval \\\n","#     --push_to_hub False \\\n","#     --optim adamw_torch \\\n","#     --learning_rate 0.001 \\\n","#     --num_train_epochs 300 \\\n","#     --per_device_train_batch_size 64 \\\n","#     --per_device_eval_batch_size 64 \\\n","#     --logging_strategy steps \\\n","#     --logging_steps 10 \\\n","#     --evaluation_strategy epoch \\\n","#     --save_strategy epoch \\\n","#     --load_best_model_at_end True \\\n","#     --save_total_limit 3 \\\n","#     --save_steps 500 \\\n","#     --data_seed 1 \\\n","#     --seed 1 \\\n","#     --report_to wandb \\\n","#     --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2425771,"status":"ok","timestamp":1696320159901,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"XcUNmUUjaOCe","outputId":"65febcce-b981-4e75-8fd0-2e5f9627c000"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 07:22:20.422190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: W\u0026B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_072225-cumrmqrx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-glitter-1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/cumrmqrx\u001b[0m\n","10/03/2023 07:22:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 07:22:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=1,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/runs/Oct03_07-22-26_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=1,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 381040.66it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 262780.05it/s]\n","Downloading data files: 100% 2796/2796 [00:00\u003c00:00, 81249.82it/s]\n","Downloading data files: 100% 5/5 [00:00\u003c00:00, 17712.43it/s]\n","Extracting data files: 100% 5/5 [00:00\u003c00:00, 2154.68it/s]\n","Downloading data files: 100% 722/722 [00:00\u003c00:00, 78972.71it/s]\n","Downloading data files: 100% 1/1 [00:00\u003c00:00, 5084.00it/s]\n","Extracting data files: 100% 1/1 [00:00\u003c00:00, 1405.13it/s]\n","Generating train split: 2796 examples [00:00, 9998.50 examples/s] \n","Generating validation split: 722 examples [00:00, 12320.23 examples/s]\n","Casting the dataset: 100% 2796/2796 [00:00\u003c00:00, 69869.67 examples/s]\n","Casting the dataset: 100% 722/722 [00:00\u003c00:00, 18535.35 examples/s]\n","Downloading builder script: 100% 4.20k/4.20k [00:00\u003c00:00, 10.7MB/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 07:22:31,980 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 07:22:31,981 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 07:22:31,984 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 07:22:32,955 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 07:22:32,955 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 07:22:32,967 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 07:22:32,969 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 07:22:37,801 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 07:22:37,802 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 07:22:37,802 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 07:22:37,802 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 07:22:37,802 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 07:22:37,802 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 07:22:37,802 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 07:22:37,803 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 07:22:37,804 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:28\u003c5:00:44,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 07:24:06,389 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:24:06,389 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:24:06,389 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3995873928070068, 'eval_accuracy': 0.6177285318559557, 'eval_runtime': 21.3532, 'eval_samples_per_second': 33.812, 'eval_steps_per_second': 0.14, 'epoch': 1.0}\n","  0% 11/3300 [01:49\u003c5:00:44,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:24:27,748 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 07:24:27,754 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:24:28,463 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:24:28,468 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:12\u003c5:08:51,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 07:25:50,479 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:25:50,479 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:25:50,479 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.066725254058838, 'eval_accuracy': 0.6980609418282548, 'eval_runtime': 20.9014, 'eval_samples_per_second': 34.543, 'eval_steps_per_second': 0.144, 'epoch': 2.0}\n","  1% 22/3300 [03:33\u003c5:08:51,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:26:11,386 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 07:26:11,392 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:26:12,086 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:26:12,090 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:53\u003c5:04:10,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 07:27:31,667 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:27:31,667 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:27:31,668 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8332756161689758, 'eval_accuracy': 0.7562326869806094, 'eval_runtime': 20.8369, 'eval_samples_per_second': 34.65, 'eval_steps_per_second': 0.144, 'epoch': 3.0}\n","  1% 33/3300 [05:14\u003c5:04:10,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:27:52,509 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 07:27:52,514 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:27:53,211 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:27:53,216 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:35\u003c4:45:50,  5.27s/it][INFO|trainer.py:3213] 2023-10-03 07:29:13,206 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:29:13,206 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:29:13,206 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7838122248649597, 'eval_accuracy': 0.7673130193905817, 'eval_runtime': 20.7589, 'eval_samples_per_second': 34.78, 'eval_steps_per_second': 0.145, 'epoch': 4.0}\n","  1% 44/3300 [06:56\u003c4:45:50,  5.27s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:29:33,971 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 07:29:33,976 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:29:34,665 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:29:34,669 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:29:36,185 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:16\u003c5:01:49,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 07:30:54,355 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:30:54,355 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:30:54,355 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6656699180603027, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 20.6864, 'eval_samples_per_second': 34.902, 'eval_steps_per_second': 0.145, 'epoch': 5.0}\n","  2% 55/3300 [08:37\u003c5:01:49,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:31:15,048 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 07:31:15,053 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:31:15,736 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:31:15,740 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:31:17,149 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:57\u003c4:57:07,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 07:32:35,357 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:32:35,358 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:32:35,358 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.86s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7377678751945496, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.7579, 'eval_samples_per_second': 34.782, 'eval_steps_per_second': 0.145, 'epoch': 6.0}\n","  2% 66/3300 [10:18\u003c4:57:07,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:32:56,120 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 07:32:56,126 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:32:56,821 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:32:56,825 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:33:01,252 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:41\u003c5:08:40,  5.75s/it][INFO|trainer.py:3213] 2023-10-03 07:34:19,404 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:34:19,405 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:34:19,405 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6938226222991943, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 20.8676, 'eval_samples_per_second': 34.599, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n","  2% 77/3300 [12:02\u003c5:08:40,  5.75s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:34:40,277 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 07:34:40,283 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:34:40,987 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:34:40,992 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:34:42,442 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:23\u003c4:53:03,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 07:36:00,925 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:36:00,925 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:36:00,925 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6172541975975037, 'eval_accuracy': 0.8268698060941828, 'eval_runtime': 20.7358, 'eval_samples_per_second': 34.819, 'eval_steps_per_second': 0.145, 'epoch': 8.0}\n","  3% 88/3300 [13:43\u003c4:53:03,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:36:21,668 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 07:36:21,673 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:36:22,376 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:36:22,380 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:36:23,890 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:04\u003c4:53:19,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 07:37:42,607 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:37:42,608 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:37:42,608 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.86s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.655663013458252, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 21.1548, 'eval_samples_per_second': 34.129, 'eval_steps_per_second': 0.142, 'epoch': 9.0}\n","  3% 99/3300 [15:25\u003c4:53:19,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:38:03,767 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 07:38:03,772 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:38:04,477 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:38:04,482 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:38:06,063 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.645, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:47\u003c5:00:36,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 07:39:24,870 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:39:24,870 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:39:24,871 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6106027364730835, 'eval_accuracy': 0.8213296398891967, 'eval_runtime': 20.9975, 'eval_samples_per_second': 34.385, 'eval_steps_per_second': 0.143, 'epoch': 10.0}\n","  3% 110/3300 [17:08\u003c5:00:36,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:39:45,873 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 07:39:45,878 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:39:46,581 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:39:46,585 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:39:48,057 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [18:31\u003c4:54:31,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 07:41:09,441 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:41:09,441 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:41:09,441 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6674240827560425, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 20.6648, 'eval_samples_per_second': 34.939, 'eval_steps_per_second': 0.145, 'epoch': 11.0}\n","  4% 121/3300 [18:52\u003c4:54:31,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:41:30,110 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 07:41:30,116 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:41:30,814 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:41:30,818 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:41:32,348 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [20:13\u003c5:01:33,  5.71s/it][INFO|trainer.py:3213] 2023-10-03 07:42:51,087 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:42:51,088 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:42:51,088 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5837766528129578, 'eval_accuracy': 0.8296398891966759, 'eval_runtime': 20.9955, 'eval_samples_per_second': 34.388, 'eval_steps_per_second': 0.143, 'epoch': 12.0}\n","  4% 132/3300 [20:34\u003c5:01:33,  5.71s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:43:12,089 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 07:43:12,094 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:43:12,817 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:43:12,821 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:43:14,342 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [21:55\u003c4:56:10,  5.63s/it][INFO|trainer.py:3213] 2023-10-03 07:44:33,120 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:44:33,121 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:44:33,121 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5284110903739929, 'eval_accuracy': 0.8240997229916898, 'eval_runtime': 20.8274, 'eval_samples_per_second': 34.666, 'eval_steps_per_second': 0.144, 'epoch': 13.0}\n","  4% 143/3300 [22:16\u003c4:56:10,  5.63s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:44:53,953 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 07:44:53,959 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:44:54,691 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:44:54,696 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:44:59,159 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [23:39\u003c4:55:37,  5.64s/it][INFO|trainer.py:3213] 2023-10-03 07:46:17,406 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:46:17,406 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:46:17,406 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.62408846616745, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.8522, 'eval_samples_per_second': 34.625, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [24:00\u003c4:55:37,  5.64s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:46:38,269 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 07:46:38,275 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:46:39,014 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:46:39,019 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:46:40,556 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [25:21\u003c4:52:50,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 07:47:58,987 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:47:58,987 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:47:58,987 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.666561484336853, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.9202, 'eval_samples_per_second': 34.512, 'eval_steps_per_second': 0.143, 'epoch': 15.0}\n","  5% 165/3300 [25:42\u003c4:52:50,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:48:19,912 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 07:48:19,918 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:48:20,686 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:48:20,691 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:48:22,230 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-132] due to args.save_total_limit\n","  5% 176/3300 [27:03\u003c4:51:41,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 07:49:41,159 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:49:41,159 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:49:41,159 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7936710715293884, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.8868, 'eval_samples_per_second': 34.567, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [27:24\u003c4:51:41,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:50:02,050 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 07:50:02,055 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:50:02,782 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:50:02,786 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:50:04,295 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:45\u003c4:54:14,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 07:51:23,547 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:51:23,547 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:51:23,547 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.90s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6845430731773376, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 21.1343, 'eval_samples_per_second': 34.163, 'eval_steps_per_second': 0.142, 'epoch': 17.0}\n","  6% 187/3300 [29:06\u003c4:54:14,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.89s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:51:44,686 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 07:51:44,692 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:51:45,416 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:51:45,420 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:51:46,926 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:30\u003c5:01:48,  5.84s/it][INFO|trainer.py:3213] 2023-10-03 07:53:08,761 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:53:08,761 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:53:08,762 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.87s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.702226996421814, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 21.0595, 'eval_samples_per_second': 34.284, 'eval_steps_per_second': 0.142, 'epoch': 18.0}\n","  6% 198/3300 [30:52\u003c5:01:48,  5.84s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:53:29,826 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 07:53:29,832 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:53:30,549 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:53:30,554 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:53:32,105 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2101, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [32:12\u003c4:53:08,  5.69s/it][INFO|trainer.py:3213] 2023-10-03 07:54:50,675 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:54:50,675 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:54:50,676 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6699013113975525, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.6695, 'eval_samples_per_second': 34.931, 'eval_steps_per_second': 0.145, 'epoch': 19.0}\n","  6% 209/3300 [32:33\u003c4:53:08,  5.69s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:55:11,350 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 07:55:11,365 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:55:12,076 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:55:12,080 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:55:13,564 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:53\u003c4:40:09,  5.46s/it][INFO|trainer.py:3213] 2023-10-03 07:56:31,617 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:56:31,617 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:56:31,617 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6151466369628906, 'eval_accuracy': 0.8213296398891967, 'eval_runtime': 20.8115, 'eval_samples_per_second': 34.692, 'eval_steps_per_second': 0.144, 'epoch': 20.0}\n","  7% 220/3300 [34:14\u003c4:40:09,  5.46s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:56:52,436 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 07:56:52,442 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:56:53,136 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:56:53,141 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:56:57,639 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [35:38\u003c4:51:46,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 07:58:16,504 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:58:16,504 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:58:16,504 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5865048170089722, 'eval_accuracy': 0.8337950138504155, 'eval_runtime': 20.874, 'eval_samples_per_second': 34.588, 'eval_steps_per_second': 0.144, 'epoch': 21.0}\n","  7% 231/3300 [35:59\u003c4:51:46,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 07:58:37,383 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 07:58:37,388 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 07:58:38,087 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 07:58:38,091 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 07:58:39,569 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [37:20\u003c4:46:07,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 07:59:57,926 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 07:59:57,926 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 07:59:57,927 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.622367799282074, 'eval_accuracy': 0.8268698060941828, 'eval_runtime': 20.6959, 'eval_samples_per_second': 34.886, 'eval_steps_per_second': 0.145, 'epoch': 22.0}\n","  7% 242/3300 [37:40\u003c4:46:07,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:00:18,626 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 08:00:18,631 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:00:19,343 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:00:19,348 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:00:20,842 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [39:01\u003c4:45:22,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 08:01:39,514 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:01:39,514 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:01:39,514 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6859464049339294, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 20.8246, 'eval_samples_per_second': 34.67, 'eval_steps_per_second': 0.144, 'epoch': 23.0}\n","  8% 253/3300 [39:22\u003c4:45:22,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:02:00,345 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 08:02:00,351 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:02:01,044 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:02:01,049 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:02:05,227 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-231] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 08:02:05,238 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 08:02:05,238 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/checkpoint-143 (score: 0.5284110903739929).\n","{'train_runtime': 2367.8791, 'train_samples_per_second': 354.241, 'train_steps_per_second': 1.394, 'train_loss': 0.3736831416254458, 'epoch': 23.0}\n","  8% 253/3300 [39:27\u003c7:55:17,  9.36s/it]\n","[INFO|trainer.py:2939] 2023-10-03 08:02:05,686 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/\n","[INFO|configuration_utils.py:460] 2023-10-03 08:02:05,692 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:02:06,421 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:02:06,425 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       23.0\n","  train_loss               =     0.3737\n","  train_runtime            = 0:39:27.87\n","  train_samples_per_second =    354.241\n","  train_steps_per_second   =      1.394\n","[INFO|trainer.py:3213] 2023-10-03 08:02:06,439 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:02:06,439 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:02:06,440 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.04s/it]\n","***** eval metrics *****\n","  epoch                   =       23.0\n","  eval_accuracy           =     0.8241\n","  eval_loss               =     0.5284\n","  eval_runtime            = 0:00:21.11\n","  eval_samples_per_second =     34.198\n","  eval_steps_per_second   =      0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▅▆▇▇▇█▇████▇▇▇▇▇████▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▃▂▃▂▂▂▂▂▁▁▂▂▃▂▂▂▂▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▃▂▁▂▃▂▆▄▁▄▃▃▄▃▆▅▁▂▃▁▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▆▇█▇▆▇▃▅█▅▆▆▅▆▃▄█▆▆█▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▇▇███▇█▄▅█▅▇▇▅▇▄▄█▇▇█▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.8241\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.52841\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.1126\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.198\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 23.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 253\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2101\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.985320094406328e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.37368\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2367.8791\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 354.241\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.394\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mroyal-glitter-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/cumrmqrx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_072225-cumrmqrx/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_1/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 1 \\\n","    --seed 1 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes'''\n","0.64,0.65,0.64,0.62,0.64,0.64,0.64,0.65,0.64,0.64\n","1.43,1.41,1.42,1.43,1.42,1.43,1.42,1.42,1.43,1.44\n","116.74,118.22,116.70,116.75,116.54,116.78,117.30,117.75,117.14, 117.3426\n","193.76,191.33,193.82,193.74,194.08,193.68,192.83,192.09,193.09,192.76\n","0.76,0.75,0.76,0.76,0.76,0.76,0.76,0.76,0.76,0.758\n","26.00,27.00,25.00,20.00,24.00,26.00,25.00,26.00,24.00,24\n","\"5,356.00\",\"5,562.00\",\"5,150.00\",\"4,120.00\",\"4,944.00\",\"5,356.00\",\"5,150.00\",\"5,356.00\",\"4,944.00\",4944\n","0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00092\n","0.79,0.77,0.82,0.92,0.84,0.78,0.81,0.79,0.85,0.8249\n","1.39E+19,1.44E+19,1.33E+19,1.07E+19,1.28E+19,1.39E+19,1.33E+19,1.39E+19,1.28E+19,1.28E+19\n","1.26,1.24,1.28,1.39,1.30,1.26,1.28,1.26,1.30,1.30\n","\"9,524.29\",\"9,900.05\",\"9,179.16\",\"7,310.97\",\"8,754.70\",\"9,524.44\",\"9,205.59\",\"9,534.33\",\"8,793.17\",8808.464\n","\"1,656.91\",\"1,594.02\",\"1,719.21\",\"2,158.52\",\"1,802.56\",\"1,656.88\",\"1,714.27\",\"1,655.17\",\"1,794.68\",1791.561\n","6.49,6.24,6.73,8.45,7.06,6.49,6.71,6.48,7.03,7.016\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2100544,"status":"ok","timestamp":1696322260432,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"gAU8ZpJXhnFG","outputId":"44efa425-f9a4-4c52-bf89-8ba029821117"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 08:02:42.734764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_080247-qekz3373\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevoted-planet-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/qekz3373\u001b[0m\n","10/03/2023 08:02:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 08:02:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=2,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/runs/Oct03_08-02-47_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=2,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 148638.59it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 346648.58it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 08:02:52,047 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 08:02:52,049 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 08:02:52,051 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 08:02:52,983 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 08:02:52,984 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 08:02:52,998 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 08:02:52,999 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 08:02:54,896 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 08:02:54,896 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 08:02:54,896 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 08:02:54,896 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 08:02:54,896 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 08:02:54,897 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 08:02:54,897 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 08:02:54,898 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 08:02:54,899 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:22\u003c5:01:08,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 08:04:17,364 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:04:17,364 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:04:17,364 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4908480644226074, 'eval_accuracy': 0.5858725761772853, 'eval_runtime': 21.137, 'eval_samples_per_second': 34.158, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:43\u003c5:01:08,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:04:38,507 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 08:04:38,513 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:04:39,240 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:04:39,245 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:04\u003c5:05:33,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 08:05:59,413 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:05:59,413 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:05:59,413 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9781104326248169, 'eval_accuracy': 0.724376731301939, 'eval_runtime': 20.8325, 'eval_samples_per_second': 34.657, 'eval_steps_per_second': 0.144, 'epoch': 2.0}\n","  1% 22/3300 [03:25\u003c5:05:33,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:06:20,251 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 08:06:20,256 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:06:20,938 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:06:20,941 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:46\u003c5:11:00,  5.71s/it][INFO|trainer.py:3213] 2023-10-03 08:07:41,499 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:07:41,500 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:07:41,500 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.896854817867279, 'eval_accuracy': 0.7313019390581718, 'eval_runtime': 20.8564, 'eval_samples_per_second': 34.618, 'eval_steps_per_second': 0.144, 'epoch': 3.0}\n","  1% 33/3300 [05:07\u003c5:11:00,  5.71s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:08:02,360 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 08:08:02,365 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:08:03,068 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:08:03,074 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:28\u003c5:01:17,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 08:09:23,340 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:09:23,340 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:09:23,340 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7388518452644348, 'eval_accuracy': 0.775623268698061, 'eval_runtime': 20.7591, 'eval_samples_per_second': 34.78, 'eval_steps_per_second': 0.145, 'epoch': 4.0}\n","  1% 44/3300 [06:49\u003c5:01:17,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:09:44,103 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 08:09:44,108 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:09:44,805 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:09:44,810 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:09:49,308 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:12\u003c5:14:19,  5.81s/it][INFO|trainer.py:3213] 2023-10-03 08:11:07,811 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:11:07,811 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:11:07,811 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6699082255363464, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.7021, 'eval_samples_per_second': 34.876, 'eval_steps_per_second': 0.145, 'epoch': 5.0}\n","  2% 55/3300 [08:33\u003c5:14:19,  5.81s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:11:28,518 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 08:11:28,523 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:11:29,240 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:11:29,244 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:11:30,803 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:54\u003c4:58:45,  5.54s/it][INFO|trainer.py:3213] 2023-10-03 08:12:49,822 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:12:49,822 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:12:49,823 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6835715174674988, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 20.873, 'eval_samples_per_second': 34.59, 'eval_steps_per_second': 0.144, 'epoch': 6.0}\n","  2% 66/3300 [10:15\u003c4:58:45,  5.54s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:13:10,700 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 08:13:10,705 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:13:11,436 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:13:11,440 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:13:13,012 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:37\u003c5:06:20,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 08:14:31,950 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:14:31,951 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:14:31,951 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6404728889465332, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 20.7435, 'eval_samples_per_second': 34.806, 'eval_steps_per_second': 0.145, 'epoch': 7.0}\n","  2% 77/3300 [11:57\u003c5:06:20,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:14:52,698 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 08:14:52,704 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:14:53,441 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:14:53,445 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:14:54,997 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:18\u003c4:52:36,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 08:16:13,857 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:16:13,857 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:16:13,858 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6406186819076538, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 20.8204, 'eval_samples_per_second': 34.678, 'eval_steps_per_second': 0.144, 'epoch': 8.0}\n","  3% 88/3300 [13:39\u003c4:52:36,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:16:34,683 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 08:16:34,688 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:16:35,416 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:16:35,420 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:16:36,951 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:03\u003c4:53:36,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 08:17:58,380 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:17:58,380 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:17:58,380 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6886729001998901, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.7343, 'eval_samples_per_second': 34.821, 'eval_steps_per_second': 0.145, 'epoch': 9.0}\n","  3% 99/3300 [15:24\u003c4:53:36,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:18:19,119 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 08:18:19,125 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:18:19,826 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:18:19,830 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:18:21,354 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6547, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:44\u003c4:51:36,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 08:19:39,731 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:19:39,731 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:19:39,731 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5415478944778442, 'eval_accuracy': 0.8379501385041551, 'eval_runtime': 20.8253, 'eval_samples_per_second': 34.669, 'eval_steps_per_second': 0.144, 'epoch': 10.0}\n","  3% 110/3300 [17:05\u003c4:51:36,  5.48s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:20:00,561 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 08:20:00,566 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:20:01,267 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:20:01,272 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:20:02,778 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [18:26\u003c4:53:49,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 08:21:21,360 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:21:21,360 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:21:21,360 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6468976140022278, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.7245, 'eval_samples_per_second': 34.838, 'eval_steps_per_second': 0.145, 'epoch': 11.0}\n","  4% 121/3300 [18:47\u003c4:53:49,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:21:42,089 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 08:21:42,095 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:21:42,792 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:21:42,797 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:21:47,283 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [20:11\u003c5:11:09,  5.89s/it][INFO|trainer.py:3213] 2023-10-03 08:23:05,909 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:23:05,910 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:23:05,910 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.625385046005249, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 20.7072, 'eval_samples_per_second': 34.867, 'eval_steps_per_second': 0.145, 'epoch': 12.0}\n","  4% 132/3300 [20:31\u003c5:11:09,  5.89s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:23:26,621 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 08:23:26,628 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:23:27,328 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:23:27,332 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:23:28,847 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [21:52\u003c4:49:10,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 08:24:47,359 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:24:47,360 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:24:47,360 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6544348001480103, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 21.095, 'eval_samples_per_second': 34.226, 'eval_steps_per_second': 0.142, 'epoch': 13.0}\n","  4% 143/3300 [22:13\u003c4:49:10,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:25:08,459 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 08:25:08,464 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:25:09,161 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:25:09,165 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:25:10,650 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [23:34\u003c4:53:38,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 08:26:29,069 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:26:29,070 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:26:29,070 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6020524501800537, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 20.6847, 'eval_samples_per_second': 34.905, 'eval_steps_per_second': 0.145, 'epoch': 14.0}\n","  5% 154/3300 [23:54\u003c4:53:38,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:26:49,759 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 08:26:49,765 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:26:50,470 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:26:50,474 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:26:52,149 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [25:16\u003c4:55:50,  5.66s/it][INFO|trainer.py:3213] 2023-10-03 08:28:10,980 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:28:10,980 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:28:10,980 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5751367211341858, 'eval_accuracy': 0.8296398891966759, 'eval_runtime': 20.7778, 'eval_samples_per_second': 34.749, 'eval_steps_per_second': 0.144, 'epoch': 15.0}\n","  5% 165/3300 [25:36\u003c4:55:50,  5.66s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:28:31,762 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 08:28:31,767 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:28:32,489 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:28:32,494 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:28:34,023 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [27:00\u003c4:56:25,  5.69s/it][INFO|trainer.py:3213] 2023-10-03 08:29:55,769 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:29:55,769 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:29:55,769 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.643126368522644, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.7636, 'eval_samples_per_second': 34.772, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [27:21\u003c4:56:25,  5.69s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:30:16,539 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 08:30:16,544 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:30:17,264 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:30:17,268 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:30:18,809 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:42\u003c4:48:01,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 08:31:37,200 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:31:37,200 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:31:37,200 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6387538313865662, 'eval_accuracy': 0.8213296398891967, 'eval_runtime': 20.7114, 'eval_samples_per_second': 34.86, 'eval_steps_per_second': 0.145, 'epoch': 17.0}\n","  6% 187/3300 [29:03\u003c4:48:01,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:31:57,917 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 08:31:57,922 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:31:58,636 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:31:58,640 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:32:00,150 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:23\u003c4:47:10,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 08:33:18,661 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:33:18,662 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:33:18,662 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.68182373046875, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 20.7461, 'eval_samples_per_second': 34.802, 'eval_steps_per_second': 0.145, 'epoch': 18.0}\n","  6% 198/3300 [30:44\u003c4:47:10,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:33:39,414 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 08:33:39,420 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:33:40,120 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:33:40,124 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:33:41,639 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2033, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [32:08\u003c4:43:58,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 08:35:03,125 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:35:03,125 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:35:03,125 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6433953642845154, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.654, 'eval_samples_per_second': 34.957, 'eval_steps_per_second': 0.145, 'epoch': 19.0}\n","  6% 209/3300 [32:28\u003c4:43:58,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:35:23,783 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 08:35:23,789 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:35:24,504 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:35:24,509 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:35:26,084 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:49\u003c4:47:53,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 08:36:44,512 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:36:44,512 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:36:44,512 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6766799688339233, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.7082, 'eval_samples_per_second': 34.865, 'eval_steps_per_second': 0.145, 'epoch': 20.0}\n","  7% 220/3300 [34:10\u003c4:47:53,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:37:05,226 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 08:37:05,232 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:37:05,950 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:37:05,955 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:37:07,417 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-198] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 08:37:07,427 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 08:37:07,428 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/checkpoint-110 (score: 0.5415478944778442).\n","{'train_runtime': 2053.0075, 'train_samples_per_second': 408.571, 'train_steps_per_second': 1.607, 'train_loss': 0.40606822859157216, 'epoch': 20.0}\n","  7% 220/3300 [34:13\u003c7:59:02,  9.33s/it]\n","[INFO|trainer.py:2939] 2023-10-03 08:37:07,909 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/\n","[INFO|configuration_utils.py:460] 2023-10-03 08:37:07,915 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:37:08,780 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:37:08,784 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       20.0\n","  train_loss               =     0.4061\n","  train_runtime            = 0:34:13.00\n","  train_samples_per_second =    408.571\n","  train_steps_per_second   =      1.607\n","[INFO|trainer.py:3213] 2023-10-03 08:37:08,800 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:37:08,800 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:37:08,800 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.04s/it]\n","***** eval metrics *****\n","  epoch                   =       20.0\n","  eval_accuracy           =      0.838\n","  eval_loss               =     0.5415\n","  eval_runtime            = 0:00:21.41\n","  eval_samples_per_second =     33.719\n","  eval_steps_per_second   =       0.14\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▅▆▇▇▇█▇█▇█▇▇█▇██▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▄▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▅▃▃▂▁▃▂▃▂▃▂▁▅▁▂▂▂▂▁▂█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▃▆▆▇█▆▇▆▇▆▇▇▄█▇▇▇▇█▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▄▇▇██▇█▇█▇██▄█▇▇████▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.83795\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.54155\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.4121\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 33.719\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.14\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 20.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 220\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2033\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.335060951657677e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.40607\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2053.0075\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 408.571\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.607\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdevoted-planet-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/qekz3373\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_080247-qekz3373/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_2/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 2 \\\n","    --seed 2 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1895820,"status":"ok","timestamp":1696324156240,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"EuAxN-9TFIeO","outputId":"2ded6915-a90e-424f-ace4-2bf352cfac41"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 08:37:43.473407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_083747-obd26bv5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexpert-plasma-3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/obd26bv5\u001b[0m\n","10/03/2023 08:37:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 08:37:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=3,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/runs/Oct03_08-37-48_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=3,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 109067.87it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 311887.46it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 08:37:52,297 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 08:37:52,298 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 08:37:52,300 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 08:37:53,236 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 08:37:53,237 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 08:37:53,251 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 08:37:53,253 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 08:37:55,161 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 08:37:55,161 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 08:37:55,162 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 08:37:55,162 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 08:37:55,162 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 08:37:55,162 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 08:37:55,162 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 08:37:55,163 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 08:37:55,164 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:22\u003c5:08:39,  5.63s/it][INFO|trainer.py:3213] 2023-10-03 08:39:18,151 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:39:18,151 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:39:18,151 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4193912744522095, 'eval_accuracy': 0.610803324099723, 'eval_runtime': 21.0577, 'eval_samples_per_second': 34.287, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:44\u003c5:08:39,  5.63s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:39:39,212 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 08:39:39,217 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:39:39,899 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:39:39,903 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:04\u003c4:58:10,  5.46s/it][INFO|trainer.py:3213] 2023-10-03 08:40:59,820 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:40:59,820 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:40:59,820 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0412589311599731, 'eval_accuracy': 0.6925207756232687, 'eval_runtime': 20.6959, 'eval_samples_per_second': 34.886, 'eval_steps_per_second': 0.145, 'epoch': 2.0}\n","  1% 22/3300 [03:25\u003c4:58:10,  5.46s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:41:20,520 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 08:41:20,526 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:41:21,235 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:41:21,254 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:46\u003c5:08:49,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 08:42:41,400 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:42:41,400 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:42:41,400 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7949052453041077, 'eval_accuracy': 0.7714681440443213, 'eval_runtime': 20.6734, 'eval_samples_per_second': 34.924, 'eval_steps_per_second': 0.145, 'epoch': 3.0}\n","  1% 33/3300 [05:06\u003c5:08:49,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:43:02,078 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 08:43:02,084 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:43:02,793 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:43:02,797 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:27\u003c4:58:13,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 08:44:22,703 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:44:22,704 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:44:22,704 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7710494995117188, 'eval_accuracy': 0.7631578947368421, 'eval_runtime': 20.7705, 'eval_samples_per_second': 34.761, 'eval_steps_per_second': 0.144, 'epoch': 4.0}\n","  1% 44/3300 [06:48\u003c4:58:13,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:44:43,478 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 08:44:43,484 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:44:44,185 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:44:44,189 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:44:45,621 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:08\u003c5:10:10,  5.74s/it][INFO|trainer.py:3213] 2023-10-03 08:46:03,762 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:46:03,763 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:46:03,763 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7102094888687134, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.718, 'eval_samples_per_second': 34.849, 'eval_steps_per_second': 0.145, 'epoch': 5.0}\n","  2% 55/3300 [08:29\u003c5:10:10,  5.74s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:46:24,484 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 08:46:24,489 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:46:25,185 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:46:25,189 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:46:26,623 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:53\u003c5:05:48,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 08:47:48,733 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:47:48,733 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:47:48,733 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6381954550743103, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.576, 'eval_samples_per_second': 35.089, 'eval_steps_per_second': 0.146, 'epoch': 6.0}\n","  2% 66/3300 [10:14\u003c5:05:48,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:48:09,313 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 08:48:09,318 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:48:10,044 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:48:10,049 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:48:11,552 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:35\u003c5:20:51,  5.97s/it][INFO|trainer.py:3213] 2023-10-03 08:49:30,348 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:49:30,349 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:49:30,349 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6651971340179443, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.8538, 'eval_samples_per_second': 34.622, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n","  2% 77/3300 [11:56\u003c5:20:51,  5.97s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:49:51,207 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 08:49:51,225 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:49:51,942 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:49:51,947 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:49:53,438 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:16\u003c4:55:46,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 08:51:11,851 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:51:11,851 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:51:11,851 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.594879686832428, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 20.845, 'eval_samples_per_second': 34.637, 'eval_steps_per_second': 0.144, 'epoch': 8.0}\n","  3% 88/3300 [13:37\u003c4:55:46,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:51:32,700 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 08:51:32,707 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:51:33,417 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:51:33,422 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:51:37,977 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:01\u003c4:57:45,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 08:52:56,986 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:52:56,987 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:52:56,987 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.86s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6869106888771057, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 20.9186, 'eval_samples_per_second': 34.515, 'eval_steps_per_second': 0.143, 'epoch': 9.0}\n","  3% 99/3300 [15:22\u003c4:57:45,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:53:17,910 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 08:53:17,917 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:53:18,714 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:53:18,720 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:53:20,284 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6463, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:43\u003c4:56:29,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 08:54:39,101 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:54:39,101 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:54:39,101 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.87s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6674445271492004, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 21.0739, 'eval_samples_per_second': 34.26, 'eval_steps_per_second': 0.142, 'epoch': 10.0}\n","  3% 110/3300 [17:05\u003c4:56:29,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:55:00,180 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 08:55:00,186 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:55:00,937 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:55:00,941 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:55:02,409 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [18:26\u003c4:52:16,  5.52s/it][INFO|trainer.py:3213] 2023-10-03 08:56:21,402 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:56:21,402 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:56:21,402 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6695613861083984, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.8942, 'eval_samples_per_second': 34.555, 'eval_steps_per_second': 0.144, 'epoch': 11.0}\n","  4% 121/3300 [18:47\u003c4:52:16,  5.52s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:56:42,300 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 08:56:42,306 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:56:43,004 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:56:43,008 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:56:47,456 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [20:10\u003c4:51:51,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 08:58:05,478 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:58:05,478 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:58:05,479 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.94s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6271252632141113, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 21.1239, 'eval_samples_per_second': 34.179, 'eval_steps_per_second': 0.142, 'epoch': 12.0}\n","  4% 132/3300 [20:31\u003c4:51:51,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.91s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 08:58:26,606 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 08:58:26,612 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 08:58:27,330 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 08:58:27,334 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 08:58:28,819 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [21:51\u003c4:49:49,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 08:59:47,006 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 08:59:47,006 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 08:59:47,006 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6762733459472656, 'eval_accuracy': 0.8240997229916898, 'eval_runtime': 20.8266, 'eval_samples_per_second': 34.667, 'eval_steps_per_second': 0.144, 'epoch': 13.0}\n","  4% 143/3300 [22:12\u003c4:49:49,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:00:07,837 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 09:00:07,842 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:00:08,539 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:00:08,543 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:00:10,039 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [23:33\u003c4:47:54,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 09:01:28,537 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:01:28,538 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:01:28,538 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6459274291992188, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.7862, 'eval_samples_per_second': 34.735, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [23:54\u003c4:47:54,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:01:49,329 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 09:01:49,334 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:01:50,028 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:01:50,033 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:01:51,552 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [25:14\u003c4:51:23,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 09:03:09,975 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:03:09,976 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:03:09,976 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6266916394233704, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 20.7327, 'eval_samples_per_second': 34.824, 'eval_steps_per_second': 0.145, 'epoch': 15.0}\n","  5% 165/3300 [25:35\u003c4:51:23,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:03:30,712 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 09:03:30,718 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:03:31,410 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:03:31,414 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:03:32,841 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [26:58\u003c4:55:56,  5.68s/it][INFO|trainer.py:3213] 2023-10-03 09:04:53,662 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:04:53,662 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:04:53,662 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.631849467754364, 'eval_accuracy': 0.8254847645429363, 'eval_runtime': 20.7704, 'eval_samples_per_second': 34.761, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [27:19\u003c4:55:56,  5.68s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:05:14,438 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 09:05:14,443 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:05:15,156 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:05:15,161 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:05:16,690 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:40\u003c4:56:16,  5.71s/it][INFO|trainer.py:3213] 2023-10-03 09:06:35,398 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:06:35,398 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:06:35,398 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6993625164031982, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 20.7339, 'eval_samples_per_second': 34.822, 'eval_steps_per_second': 0.145, 'epoch': 17.0}\n","  6% 187/3300 [29:00\u003c4:56:16,  5.71s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:06:56,137 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 09:06:56,143 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:06:56,838 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:06:56,854 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:06:58,355 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:22\u003c5:04:34,  5.89s/it][INFO|trainer.py:3213] 2023-10-03 09:08:17,563 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:08:17,563 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:08:17,563 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6612458229064941, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 21.023, 'eval_samples_per_second': 34.343, 'eval_steps_per_second': 0.143, 'epoch': 18.0}\n","  6% 198/3300 [30:43\u003c5:04:34,  5.89s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:08:38,592 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 09:08:38,598 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:08:39,326 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:08:39,331 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:08:40,883 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-176] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 09:08:43,882 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 09:08:43,882 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/checkpoint-88 (score: 0.594879686832428).\n","{'train_runtime': 1849.1729, 'train_samples_per_second': 453.608, 'train_steps_per_second': 1.785, 'train_loss': 0.43074951749859436, 'epoch': 18.0}\n","  6% 198/3300 [30:49\u003c8:02:50,  9.34s/it]\n","[INFO|trainer.py:2939] 2023-10-03 09:08:44,341 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/\n","[INFO|configuration_utils.py:460] 2023-10-03 09:08:44,347 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:08:45,091 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:08:45,096 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       18.0\n","  train_loss               =     0.4307\n","  train_runtime            = 0:30:49.17\n","  train_samples_per_second =    453.608\n","  train_steps_per_second   =      1.785\n","[INFO|trainer.py:3213] 2023-10-03 09:08:45,113 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:08:45,113 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:08:45,113 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.06s/it]\n","***** eval metrics *****\n","  epoch                   =       18.0\n","  eval_accuracy           =     0.8144\n","  eval_loss               =     0.5949\n","  eval_runtime            = 0:00:21.16\n","  eval_samples_per_second =     34.115\n","  eval_steps_per_second   =      0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▆▇███▇▇█▇█▇▇█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▂▂▁▂▁▂▂▂▁▂▁▁▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▇▂▂▃▃▁▄▄▅▇▅█▄▄▃▃▃▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▂▇▇▆▆█▅▅▄▂▄▁▅▅▆▆▆▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆▆▄▆█▄▄▃▁▄▁▄▄▆▄▆▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.8144\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.59488\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.1637\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.115\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 18.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 198\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00097\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.6463\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 3.901554856491909e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.43075\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1849.1729\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 453.608\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.785\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mexpert-plasma-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/obd26bv5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_083747-obd26bv5/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_3/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 3 \\\n","    --seed 3 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2308289,"status":"ok","timestamp":1696326464514,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"hw_zqhJyFKun","outputId":"f40a92b4-e2b2-481e-ebaf-9504bcd69419"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 09:09:19.527765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_090923-r1g5gv7m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcerulean-grass-4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/r1g5gv7m\u001b[0m\n","10/03/2023 09:09:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 09:09:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=4,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/runs/Oct03_09-09-24_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=4,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 85613.01it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 329689.26it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 09:09:29,178 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 09:09:29,180 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 09:09:29,182 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 09:09:30,127 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 09:09:30,128 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 09:09:30,143 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 09:09:30,144 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 09:09:32,047 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 09:09:32,047 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 09:09:32,047 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 09:09:32,047 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 09:09:32,048 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 09:09:32,048 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 09:09:32,048 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 09:09:32,049 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 09:09:32,050 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:23\u003c4:46:38,  5.23s/it][INFO|trainer.py:3213] 2023-10-03 09:10:55,430 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:10:55,431 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:10:55,431 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4083009958267212, 'eval_accuracy': 0.5969529085872576, 'eval_runtime': 21.1943, 'eval_samples_per_second': 34.066, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:44\u003c4:46:38,  5.23s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:11:16,639 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 09:11:16,648 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:11:17,350 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:11:17,354 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:05\u003c5:04:18,  5.57s/it][INFO|trainer.py:3213] 2023-10-03 09:12:37,066 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:12:37,066 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:12:37,066 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0173580646514893, 'eval_accuracy': 0.7063711911357341, 'eval_runtime': 20.7008, 'eval_samples_per_second': 34.878, 'eval_steps_per_second': 0.145, 'epoch': 2.0}\n","  1% 22/3300 [03:25\u003c5:04:18,  5.57s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:12:57,771 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 09:12:57,777 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:12:58,460 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:12:58,464 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:45\u003c5:02:23,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 09:14:17,795 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:14:17,795 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:14:17,795 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.896133542060852, 'eval_accuracy': 0.7590027700831025, 'eval_runtime': 20.7101, 'eval_samples_per_second': 34.862, 'eval_steps_per_second': 0.145, 'epoch': 3.0}\n","  1% 33/3300 [05:06\u003c5:02:23,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:14:38,510 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 09:14:38,516 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:14:39,204 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:14:39,208 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:27\u003c5:04:24,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 09:15:59,496 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:15:59,497 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:15:59,497 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7521979212760925, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.9006, 'eval_samples_per_second': 34.544, 'eval_steps_per_second': 0.144, 'epoch': 4.0}\n","  1% 44/3300 [06:48\u003c5:04:24,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:16:20,402 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 09:16:20,408 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:16:21,101 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:16:21,106 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:16:22,561 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:11\u003c5:11:21,  5.76s/it][INFO|trainer.py:3213] 2023-10-03 09:17:43,759 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:17:43,759 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:17:43,759 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6621193289756775, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 20.6712, 'eval_samples_per_second': 34.928, 'eval_steps_per_second': 0.145, 'epoch': 5.0}\n","  2% 55/3300 [08:32\u003c5:11:21,  5.76s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:18:04,435 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 09:18:04,440 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:18:05,148 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:18:05,152 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:18:06,660 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:52\u003c4:55:37,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 09:19:25,008 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:19:25,008 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:19:25,008 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7265843749046326, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.8497, 'eval_samples_per_second': 34.629, 'eval_steps_per_second': 0.144, 'epoch': 6.0}\n","  2% 66/3300 [10:13\u003c4:55:37,  5.48s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:19:45,864 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 09:19:45,870 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:19:46,565 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:19:46,569 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:19:48,048 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:34\u003c5:05:42,  5.69s/it][INFO|trainer.py:3213] 2023-10-03 09:21:06,481 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:21:06,481 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:21:06,481 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6231977939605713, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.7887, 'eval_samples_per_second': 34.73, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n","  2% 77/3300 [11:55\u003c5:05:42,  5.69s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:21:27,274 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 09:21:27,279 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:21:27,986 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:21:27,991 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:21:29,496 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:19\u003c5:22:27,  6.02s/it][INFO|trainer.py:3213] 2023-10-03 09:22:51,681 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:22:51,681 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:22:51,681 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6392885446548462, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 20.7711, 'eval_samples_per_second': 34.76, 'eval_steps_per_second': 0.144, 'epoch': 8.0}\n","  3% 88/3300 [13:40\u003c5:22:27,  6.02s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:23:12,456 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 09:23:12,462 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:23:13,167 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:23:13,171 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:23:14,756 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:01\u003c5:10:42,  5.82s/it][INFO|trainer.py:3213] 2023-10-03 09:24:33,230 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:24:33,231 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:24:33,231 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.680919349193573, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.6667, 'eval_samples_per_second': 34.935, 'eval_steps_per_second': 0.145, 'epoch': 9.0}\n","  3% 99/3300 [15:21\u003c5:10:42,  5.82s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:24:53,903 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 09:24:53,920 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:24:54,626 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:24:54,631 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:24:56,156 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6427, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:42\u003c5:00:36,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 09:26:14,963 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:26:14,963 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:26:14,963 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.632530152797699, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 20.6892, 'eval_samples_per_second': 34.897, 'eval_steps_per_second': 0.145, 'epoch': 10.0}\n","  3% 110/3300 [17:03\u003c5:00:36,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:26:35,657 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 09:26:35,662 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:26:36,376 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:26:36,381 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:26:40,954 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [18:27\u003c4:56:21,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 09:27:59,938 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:27:59,938 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:27:59,939 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6209336519241333, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 20.9027, 'eval_samples_per_second': 34.541, 'eval_steps_per_second': 0.144, 'epoch': 11.0}\n","  4% 121/3300 [18:48\u003c4:56:21,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:28:20,845 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 09:28:20,851 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:28:21,561 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:28:21,565 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:28:23,038 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-77] due to args.save_total_limit\n","  4% 132/3300 [20:10\u003c4:53:03,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 09:29:42,240 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:29:42,240 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:29:42,240 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5460779666900635, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 20.8415, 'eval_samples_per_second': 34.642, 'eval_steps_per_second': 0.144, 'epoch': 12.0}\n","  4% 132/3300 [20:31\u003c4:53:03,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:30:03,086 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 09:30:03,091 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:30:03,784 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:30:03,788 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:30:05,252 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [21:51\u003c4:47:57,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 09:31:23,866 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:31:23,866 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:31:23,866 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.90s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5582593679428101, 'eval_accuracy': 0.8351800554016621, 'eval_runtime': 20.9075, 'eval_samples_per_second': 34.533, 'eval_steps_per_second': 0.143, 'epoch': 13.0}\n","  4% 143/3300 [22:12\u003c4:47:57,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.89s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:31:44,779 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 09:31:44,784 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:31:45,468 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:31:45,472 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:31:49,918 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [23:35\u003c4:42:57,  5.40s/it][INFO|trainer.py:3213] 2023-10-03 09:33:08,054 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:33:08,055 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:33:08,055 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6822990775108337, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.8289, 'eval_samples_per_second': 34.663, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [23:56\u003c4:42:57,  5.40s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:33:28,888 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 09:33:28,894 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:33:29,623 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:33:29,627 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:33:31,116 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [25:17\u003c4:55:46,  5.66s/it][INFO|trainer.py:3213] 2023-10-03 09:34:49,874 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:34:49,875 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:34:49,875 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6830312609672546, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.9314, 'eval_samples_per_second': 34.494, 'eval_steps_per_second': 0.143, 'epoch': 15.0}\n","  5% 165/3300 [25:38\u003c4:55:46,  5.66s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:35:10,813 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 09:35:10,819 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:35:11,534 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:35:11,539 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:35:13,089 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [27:00\u003c5:03:08,  5.82s/it][INFO|trainer.py:3213] 2023-10-03 09:36:32,501 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:36:32,501 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:36:32,501 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6432330012321472, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.8767, 'eval_samples_per_second': 34.584, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [27:21\u003c5:03:08,  5.82s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:36:53,382 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 09:36:53,389 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:36:54,103 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:36:54,108 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:36:55,628 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:42\u003c4:54:39,  5.68s/it][INFO|trainer.py:3213] 2023-10-03 09:38:14,715 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:38:14,715 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:38:14,715 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6894325017929077, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.84, 'eval_samples_per_second': 34.645, 'eval_steps_per_second': 0.144, 'epoch': 17.0}\n","  6% 187/3300 [29:03\u003c4:54:39,  5.68s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:38:35,561 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 09:38:35,567 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:38:36,267 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:38:36,271 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:38:37,743 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:27\u003c4:52:59,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 09:39:59,182 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:39:59,182 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:39:59,183 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6578512191772461, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.6274, 'eval_samples_per_second': 35.002, 'eval_steps_per_second': 0.145, 'epoch': 18.0}\n","  6% 198/3300 [30:47\u003c4:52:59,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:40:19,814 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 09:40:19,819 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:40:20,513 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:40:20,518 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:40:22,003 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2106, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [32:08\u003c5:02:15,  5.87s/it][INFO|trainer.py:3213] 2023-10-03 09:41:40,584 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:41:40,584 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:41:40,584 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6404518485069275, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 20.8002, 'eval_samples_per_second': 34.711, 'eval_steps_per_second': 0.144, 'epoch': 19.0}\n","  6% 209/3300 [32:29\u003c5:02:15,  5.87s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:42:01,389 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 09:42:01,395 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:42:02,098 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:42:02,344 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:42:03,766 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:49\u003c4:53:45,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 09:43:21,921 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:43:21,921 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:43:21,921 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6451705098152161, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.6192, 'eval_samples_per_second': 35.016, 'eval_steps_per_second': 0.145, 'epoch': 20.0}\n","  7% 220/3300 [34:10\u003c4:53:45,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:43:42,545 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 09:43:42,551 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:43:43,246 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:43:43,251 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:43:44,726 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [35:33\u003c5:01:13,  5.89s/it][INFO|trainer.py:3213] 2023-10-03 09:45:06,004 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:45:06,004 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:45:06,005 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.634920060634613, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.7375, 'eval_samples_per_second': 34.816, 'eval_steps_per_second': 0.145, 'epoch': 21.0}\n","  7% 231/3300 [35:54\u003c5:01:13,  5.89s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:45:26,748 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 09:45:26,753 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:45:27,481 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:45:27,485 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:45:29,028 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [37:15\u003c4:58:16,  5.85s/it][INFO|trainer.py:3213] 2023-10-03 09:46:47,915 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:46:47,915 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:46:47,915 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7482312321662903, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.7063, 'eval_samples_per_second': 34.869, 'eval_steps_per_second': 0.145, 'epoch': 22.0}\n","  7% 242/3300 [37:36\u003c4:58:16,  5.85s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:47:08,627 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 09:47:08,633 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:47:09,371 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:47:09,376 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:47:10,955 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-220] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 09:47:10,967 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 09:47:10,968 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/checkpoint-132 (score: 0.5460779666900635).\n","{'train_runtime': 2259.3892, 'train_samples_per_second': 371.251, 'train_steps_per_second': 1.461, 'train_loss': 0.3839342357698551, 'epoch': 22.0}\n","  7% 242/3300 [37:39\u003c7:55:50,  9.34s/it]\n","[INFO|trainer.py:2939] 2023-10-03 09:47:11,442 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/\n","[INFO|configuration_utils.py:460] 2023-10-03 09:47:11,447 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:47:12,244 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:47:12,248 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       22.0\n","  train_loss               =     0.3839\n","  train_runtime            = 0:37:39.38\n","  train_samples_per_second =    371.251\n","  train_steps_per_second   =      1.461\n","[INFO|trainer.py:3213] 2023-10-03 09:47:12,263 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:47:12,263 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:47:12,263 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.02s/it]\n","***** eval metrics *****\n","  epoch                   =       22.0\n","  eval_accuracy           =     0.8283\n","  eval_loss               =     0.5461\n","  eval_runtime            = 0:00:21.28\n","  eval_samples_per_second =     33.925\n","  eval_steps_per_second   =      0.141\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▇▇▇▇▆▇▇▇██▇▇▇▇▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▄▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▇▂▂▄▂▃▃▃▂▂▄▃▄▃▄▄▃▁▃▁▂▂█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▂▇▇▅▇▆▆▆▇▇▅▆▅▆▅▅▆█▆█▇▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▃██▆█▆▆▆██▆▆▅▆▅▆▆█▆███▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.82825\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.54608\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.2822\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 33.925\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.141\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 22.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 242\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2106\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.768567046823444e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.38393\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2259.3892\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 371.251\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.461\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcerulean-grass-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/r1g5gv7m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_090923-r1g5gv7m/logs\u001b[0m\n","Exception in thread ChkStopThr:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 285, in check_stop_status\n","    self._loop_check_status(\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n","    local_handle = request()\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 727, in deliver_stop_status\n","    return self._deliver_stop_status(status)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 450, in _deliver_stop_status\n","    return self._deliver_record(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n","    handle = mailbox._deliver_record(record, interface=self)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n","    interface._publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n","    self._sock_client.send_record_publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n","    self.send_server_request(server_req)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n","    self._send_message(msg)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n","    self._sendall_with_error_handle(header + data)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n","    sent = self._sock.send(data)\n","BrokenPipeError: [Errno 32] Broken pipe\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_4/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 4 \\\n","    --seed 4 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2307998,"status":"ok","timestamp":1696328772508,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"lGabtHrcFOTk","outputId":"6fbebe1a-99eb-4fdc-93c3-b3fed5651ff5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 09:47:47.574775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_094751-g3vaw08j\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-river-5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/g3vaw08j\u001b[0m\n","10/03/2023 09:47:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 09:47:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=5,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/runs/Oct03_09-47-52_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=5,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 46432.08it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 179798.52it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 09:47:56,958 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 09:47:56,959 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 09:47:56,962 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 09:47:57,872 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 09:47:57,872 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 09:47:57,886 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 09:47:57,888 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 09:47:59,830 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 09:47:59,830 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 09:47:59,830 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 09:47:59,830 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 09:47:59,830 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 09:47:59,831 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 09:47:59,831 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 09:47:59,831 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 09:47:59,832 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:22\u003c5:07:33,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 09:49:22,526 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:49:22,527 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:49:22,527 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3765838146209717, 'eval_accuracy': 0.6274238227146814, 'eval_runtime': 21.2061, 'eval_samples_per_second': 34.047, 'eval_steps_per_second': 0.141, 'epoch': 1.0}\n","  0% 11/3300 [01:43\u003c5:07:33,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:49:43,738 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 09:49:43,743 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:49:44,468 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:49:44,473 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:04\u003c5:06:42,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 09:51:04,565 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:51:04,565 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:51:04,565 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.86s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0725966691970825, 'eval_accuracy': 0.7008310249307479, 'eval_runtime': 20.9776, 'eval_samples_per_second': 34.418, 'eval_steps_per_second': 0.143, 'epoch': 2.0}\n","  1% 22/3300 [03:25\u003c5:06:42,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:51:25,549 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 09:51:25,555 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:51:26,262 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:51:26,267 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:49\u003c5:13:10,  5.75s/it][INFO|trainer.py:3213] 2023-10-03 09:52:49,262 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:52:49,262 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:52:49,262 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8707621693611145, 'eval_accuracy': 0.7562326869806094, 'eval_runtime': 20.6486, 'eval_samples_per_second': 34.966, 'eval_steps_per_second': 0.145, 'epoch': 3.0}\n","  1% 33/3300 [05:10\u003c5:13:10,  5.75s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:53:09,916 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 09:53:09,922 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:53:10,642 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:53:10,647 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:30\u003c5:06:11,  5.64s/it][INFO|trainer.py:3213] 2023-10-03 09:54:30,723 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:54:30,723 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:54:30,723 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7512447237968445, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.654, 'eval_samples_per_second': 34.957, 'eval_steps_per_second': 0.145, 'epoch': 4.0}\n","  1% 44/3300 [06:51\u003c5:06:11,  5.64s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:54:51,382 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 09:54:51,387 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:54:52,080 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:54:52,085 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:54:56,555 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:15\u003c4:56:39,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 09:56:14,891 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:56:14,891 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:56:14,891 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7306953072547913, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 20.6317, 'eval_samples_per_second': 34.995, 'eval_steps_per_second': 0.145, 'epoch': 5.0}\n","  2% 55/3300 [08:35\u003c4:56:39,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:56:35,527 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 09:56:35,534 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:56:36,262 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:56:36,266 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:56:37,734 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:59\u003c5:09:11,  5.74s/it][INFO|trainer.py:3213] 2023-10-03 09:57:59,371 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:57:59,371 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:57:59,371 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.88s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6683593988418579, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.888, 'eval_samples_per_second': 34.565, 'eval_steps_per_second': 0.144, 'epoch': 6.0}\n","  2% 66/3300 [10:20\u003c5:09:11,  5.74s/it]\n","100% 3/3 [00:06\u003c00:00,  1.88s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 09:58:20,263 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 09:58:20,269 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 09:58:20,998 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 09:58:21,002 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 09:58:22,503 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:41\u003c4:55:11,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 09:59:41,278 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 09:59:41,278 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 09:59:41,278 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6710489988327026, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 20.7956, 'eval_samples_per_second': 34.719, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n","  2% 77/3300 [12:02\u003c4:55:11,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:00:02,078 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 10:00:02,083 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:00:02,793 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:00:02,807 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:00:04,268 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:23\u003c5:06:47,  5.73s/it][INFO|trainer.py:3213] 2023-10-03 10:01:23,028 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:01:23,028 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:01:23,028 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.86s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.609596312046051, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.9966, 'eval_samples_per_second': 34.387, 'eval_steps_per_second': 0.143, 'epoch': 8.0}\n","  3% 88/3300 [13:44\u003c5:06:47,  5.73s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:01:44,029 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 10:01:44,035 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:01:44,740 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:01:44,745 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:01:46,211 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:08\u003c4:59:52,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 10:03:07,954 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:03:07,954 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:03:07,954 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.68631511926651, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.9124, 'eval_samples_per_second': 34.525, 'eval_steps_per_second': 0.143, 'epoch': 9.0}\n","  3% 99/3300 [15:29\u003c4:59:52,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:03:28,871 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 10:03:28,876 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:03:29,580 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:03:29,584 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:03:31,114 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6419, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:50\u003c4:56:27,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 10:04:49,844 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:04:49,844 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:04:49,844 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6670516729354858, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.7177, 'eval_samples_per_second': 34.849, 'eval_steps_per_second': 0.145, 'epoch': 10.0}\n","  3% 110/3300 [17:10\u003c4:56:27,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:05:10,566 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 10:05:10,584 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:05:11,276 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:05:11,280 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:05:12,771 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [18:31\u003c5:02:10,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 10:06:31,177 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:06:31,178 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:06:31,178 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6805883646011353, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.6763, 'eval_samples_per_second': 34.919, 'eval_steps_per_second': 0.145, 'epoch': 11.0}\n","  4% 121/3300 [18:52\u003c5:02:10,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:06:51,858 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 10:06:51,864 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:06:52,569 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:06:52,574 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:06:57,066 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [20:15\u003c4:47:47,  5.45s/it][INFO|trainer.py:3213] 2023-10-03 10:08:15,412 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:08:15,412 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:08:15,412 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.587462842464447, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.7813, 'eval_samples_per_second': 34.743, 'eval_steps_per_second': 0.144, 'epoch': 12.0}\n","  4% 132/3300 [20:36\u003c4:47:47,  5.45s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:08:36,198 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 10:08:36,204 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:08:36,918 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:08:36,923 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:08:38,361 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-88] due to args.save_total_limit\n","  4% 143/3300 [21:56\u003c4:50:13,  5.52s/it][INFO|trainer.py:3213] 2023-10-03 10:09:56,811 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:09:56,812 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:09:56,812 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6476256847381592, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.642, 'eval_samples_per_second': 34.977, 'eval_steps_per_second': 0.145, 'epoch': 13.0}\n","  4% 143/3300 [22:17\u003c4:50:13,  5.52s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:10:17,458 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 10:10:17,465 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:10:18,187 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:10:18,191 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:10:19,629 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [23:37\u003c4:51:33,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 10:11:37,669 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:11:37,670 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:11:37,670 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6090132594108582, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.7362, 'eval_samples_per_second': 34.818, 'eval_steps_per_second': 0.145, 'epoch': 14.0}\n","  5% 154/3300 [23:58\u003c4:51:33,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:11:58,410 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 10:11:58,416 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:11:59,160 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:11:59,164 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:12:03,654 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [25:21\u003c4:47:25,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 10:13:21,818 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:13:21,818 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:13:21,818 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7067775726318359, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.8222, 'eval_samples_per_second': 34.674, 'eval_steps_per_second': 0.144, 'epoch': 15.0}\n","  5% 165/3300 [25:42\u003c4:47:25,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:13:42,646 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 10:13:42,652 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:13:43,382 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:13:43,386 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:13:44,864 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [27:03\u003c4:47:14,  5.52s/it][INFO|trainer.py:3213] 2023-10-03 10:15:03,417 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:15:03,417 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:15:03,417 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6593742966651917, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 20.7234, 'eval_samples_per_second': 34.84, 'eval_steps_per_second': 0.145, 'epoch': 16.0}\n","  5% 176/3300 [27:24\u003c4:47:14,  5.52s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:15:24,144 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 10:15:24,150 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:15:24,844 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:15:24,848 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:15:26,341 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:44\u003c4:49:11,  5.57s/it][INFO|trainer.py:3213] 2023-10-03 10:16:44,544 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:16:44,544 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:16:44,544 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7105326652526855, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.755, 'eval_samples_per_second': 34.787, 'eval_steps_per_second': 0.145, 'epoch': 17.0}\n","  6% 187/3300 [29:05\u003c4:49:11,  5.57s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:17:05,304 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 10:17:05,309 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:17:06,020 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:17:06,024 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:17:07,593 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:26\u003c4:53:21,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 10:18:26,124 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:18:26,124 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:18:26,125 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6542408466339111, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.7179, 'eval_samples_per_second': 34.849, 'eval_steps_per_second': 0.145, 'epoch': 18.0}\n","  6% 198/3300 [30:47\u003c4:53:21,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:18:46,848 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 10:18:46,854 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:18:47,555 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:18:47,559 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:18:48,989 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2173, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [32:07\u003c4:44:53,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 10:20:07,636 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:20:07,636 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:20:07,636 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6920669078826904, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.8374, 'eval_samples_per_second': 34.649, 'eval_steps_per_second': 0.144, 'epoch': 19.0}\n","  6% 209/3300 [32:28\u003c4:44:53,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:20:28,478 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 10:20:28,484 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:20:29,205 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:20:29,209 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:20:30,705 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:48\u003c4:38:25,  5.42s/it][INFO|trainer.py:3213] 2023-10-03 10:21:48,817 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:21:48,817 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:21:48,817 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6678288578987122, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 20.7823, 'eval_samples_per_second': 34.741, 'eval_steps_per_second': 0.144, 'epoch': 20.0}\n","  7% 220/3300 [34:09\u003c4:38:25,  5.42s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:22:09,604 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 10:22:09,610 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:22:10,312 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:22:10,317 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:22:11,867 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [35:30\u003c4:46:18,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 10:23:30,186 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:23:30,186 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:23:30,187 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6739117503166199, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.7828, 'eval_samples_per_second': 34.74, 'eval_steps_per_second': 0.144, 'epoch': 21.0}\n","  7% 231/3300 [35:51\u003c4:46:18,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:23:50,974 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 10:23:50,979 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:23:51,706 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:23:51,711 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:23:53,250 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [37:14\u003c4:39:02,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 10:25:14,758 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:25:14,758 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:25:14,758 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6201579570770264, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 20.7297, 'eval_samples_per_second': 34.829, 'eval_steps_per_second': 0.145, 'epoch': 22.0}\n","  7% 242/3300 [37:35\u003c4:39:02,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:25:35,494 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 10:25:35,499 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:25:36,210 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:25:36,214 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:25:37,737 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-220] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 10:25:37,746 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 10:25:37,746 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/checkpoint-132 (score: 0.587462842464447).\n","{'train_runtime': 2258.3982, 'train_samples_per_second': 371.414, 'train_steps_per_second': 1.461, 'train_loss': 0.38539755245870794, 'epoch': 22.0}\n","  7% 242/3300 [37:38\u003c7:55:37,  9.33s/it]\n","[INFO|trainer.py:2939] 2023-10-03 10:25:38,234 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/\n","[INFO|configuration_utils.py:460] 2023-10-03 10:25:38,240 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:25:39,088 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:25:39,093 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       22.0\n","  train_loss               =     0.3854\n","  train_runtime            = 0:37:38.39\n","  train_samples_per_second =    371.414\n","  train_steps_per_second   =      1.461\n","[INFO|trainer.py:3213] 2023-10-03 10:25:42,618 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:25:42,618 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:25:42,618 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.06s/it]\n","***** eval metrics *****\n","  epoch                   =       22.0\n","  eval_accuracy           =     0.8102\n","  eval_loss               =     0.5875\n","  eval_runtime            = 0:00:21.04\n","  eval_samples_per_second =     34.308\n","  eval_steps_per_second   =      0.143\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▇▇▇▇█▇▇██▇█▇█▇▇██▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▄▂▂▂▂▁▂▂▂▁▂▁▂▂▂▂▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▅▁▁▁▄▃▅▄▂▂▃▁▂▃▂▃▂▄▃▃▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▄███▅▆▄▅▇▇▆█▇▆▇▆▇▅▆▆▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▅███▆▆▅▅██▆██▆███▆▆▆█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.81025\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.58746\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.0449\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.308\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.143\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 22.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 242\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2173\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.768567046823444e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.3854\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2258.3982\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 371.414\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.461\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgallant-river-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/g3vaw08j\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_094751-g3vaw08j/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_5/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 5 \\\n","    --seed 5 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2108025,"status":"ok","timestamp":1696330880518,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"uWs01trKFRid","outputId":"b3be21f0-23bd-488b-e8fb-3e6a3a9119da"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 10:26:15.520828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_102619-ysspzndh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglad-disco-6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/ysspzndh\u001b[0m\n","10/03/2023 10:26:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 10:26:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=6,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/runs/Oct03_10-26-20_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=6,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 60044.19it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 339964.33it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 10:26:24,404 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 10:26:24,406 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 10:26:24,408 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 10:26:25,337 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 10:26:25,337 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 10:26:25,353 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 10:26:25,355 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 10:26:27,310 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 10:26:27,310 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 10:26:27,311 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 10:26:27,311 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 10:26:27,311 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 10:26:27,311 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 10:26:27,311 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 10:26:27,312 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 10:26:27,313 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:22\u003c4:52:12,  5.33s/it][INFO|trainer.py:3213] 2023-10-03 10:27:50,016 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:27:50,016 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:27:50,016 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4126529693603516, 'eval_accuracy': 0.6024930747922438, 'eval_runtime': 21.1013, 'eval_samples_per_second': 34.216, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:43\u003c4:52:12,  5.33s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:28:11,122 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 10:28:11,127 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:28:11,817 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:28:11,822 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:05\u003c5:02:20,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 10:29:32,366 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:29:32,367 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:29:32,367 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.87s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0483016967773438, 'eval_accuracy': 0.6869806094182825, 'eval_runtime': 20.9789, 'eval_samples_per_second': 34.416, 'eval_steps_per_second': 0.143, 'epoch': 2.0}\n","  1% 22/3300 [03:26\u003c5:02:20,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:29:53,350 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 10:29:53,356 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:29:54,081 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:29:54,086 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:47\u003c4:51:57,  5.36s/it][INFO|trainer.py:3213] 2023-10-03 10:31:14,369 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:31:14,370 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:31:14,370 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8612489104270935, 'eval_accuracy': 0.7686980609418282, 'eval_runtime': 20.9974, 'eval_samples_per_second': 34.385, 'eval_steps_per_second': 0.143, 'epoch': 3.0}\n","  1% 33/3300 [05:08\u003c4:51:57,  5.36s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:31:35,372 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 10:31:35,378 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:31:36,101 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:31:36,105 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:32\u003c4:55:23,  5.44s/it][INFO|trainer.py:3213] 2023-10-03 10:32:59,907 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:32:59,907 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:32:59,908 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7166081666946411, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.7519, 'eval_samples_per_second': 34.792, 'eval_steps_per_second': 0.145, 'epoch': 4.0}\n","  1% 44/3300 [06:53\u003c4:55:23,  5.44s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:33:20,664 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 10:33:20,669 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:33:21,372 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:33:21,376 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:33:22,849 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:14\u003c4:53:24,  5.43s/it][INFO|trainer.py:3213] 2023-10-03 10:34:41,844 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:34:41,845 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:34:41,845 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6822990775108337, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.845, 'eval_samples_per_second': 34.637, 'eval_steps_per_second': 0.144, 'epoch': 5.0}\n","  2% 55/3300 [08:35\u003c4:53:24,  5.43s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:35:02,694 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 10:35:02,699 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:35:03,415 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:35:03,419 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:35:04,947 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:56\u003c5:03:35,  5.63s/it][INFO|trainer.py:3213] 2023-10-03 10:36:23,916 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:36:23,917 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:36:23,917 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6544274091720581, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.7502, 'eval_samples_per_second': 34.795, 'eval_steps_per_second': 0.145, 'epoch': 6.0}\n","  2% 66/3300 [10:17\u003c5:03:35,  5.63s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:36:44,673 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 10:36:44,679 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:36:45,406 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:36:45,411 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:36:46,992 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:41\u003c5:07:11,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 10:38:08,536 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:38:08,536 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:38:08,536 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6424761414527893, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.7767, 'eval_samples_per_second': 34.75, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n","  2% 77/3300 [12:01\u003c5:07:11,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:38:29,319 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 10:38:29,325 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:38:30,064 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:38:30,070 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:38:31,664 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:22\u003c4:53:18,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 10:39:49,762 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:39:49,762 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:39:49,763 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.87s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7382727265357971, 'eval_accuracy': 0.7686980609418282, 'eval_runtime': 21.0273, 'eval_samples_per_second': 34.336, 'eval_steps_per_second': 0.143, 'epoch': 8.0}\n","  3% 88/3300 [13:43\u003c4:53:18,  5.48s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:40:10,804 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 10:40:10,813 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:40:11,568 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:40:11,573 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:40:13,116 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:03\u003c4:59:19,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 10:41:31,259 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:41:31,259 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:41:31,259 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.668204665184021, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.9558, 'eval_samples_per_second': 34.453, 'eval_steps_per_second': 0.143, 'epoch': 9.0}\n","  3% 99/3300 [15:24\u003c4:59:19,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:41:52,220 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 10:41:52,226 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:41:52,952 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:41:52,957 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:41:57,474 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.654, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:48\u003c5:02:53,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 10:43:16,077 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:43:16,077 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:43:16,077 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5550879240036011, 'eval_accuracy': 0.8240997229916898, 'eval_runtime': 21.0202, 'eval_samples_per_second': 34.348, 'eval_steps_per_second': 0.143, 'epoch': 10.0}\n","  3% 110/3300 [17:09\u003c5:02:53,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:43:37,101 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 10:43:37,107 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:43:37,794 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:43:37,799 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:43:39,304 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [18:30\u003c4:57:31,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 10:44:58,050 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:44:58,051 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:44:58,051 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6329257488250732, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.8005, 'eval_samples_per_second': 34.711, 'eval_steps_per_second': 0.144, 'epoch': 11.0}\n","  4% 121/3300 [18:51\u003c4:57:31,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:45:18,855 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 10:45:18,860 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:45:19,557 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:45:19,561 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:45:21,046 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [20:12\u003c4:56:42,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 10:46:39,365 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:46:39,365 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:46:39,365 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6959182620048523, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.8546, 'eval_samples_per_second': 34.621, 'eval_steps_per_second': 0.144, 'epoch': 12.0}\n","  4% 132/3300 [20:32\u003c4:56:42,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:47:00,225 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 10:47:00,231 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:47:00,920 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:47:00,924 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:47:02,563 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [21:54\u003c4:43:13,  5.38s/it][INFO|trainer.py:3213] 2023-10-03 10:48:22,037 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:48:22,037 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:48:22,038 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5825999975204468, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.7718, 'eval_samples_per_second': 34.759, 'eval_steps_per_second': 0.144, 'epoch': 13.0}\n","  4% 143/3300 [22:15\u003c4:43:13,  5.38s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:48:42,815 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 10:48:42,820 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:48:43,535 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:48:43,539 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:48:45,056 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [23:39\u003c4:43:39,  5.41s/it][INFO|trainer.py:3213] 2023-10-03 10:50:06,922 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:50:06,922 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:50:06,922 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6042520403862, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.8819, 'eval_samples_per_second': 34.575, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [24:00\u003c4:43:39,  5.41s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:50:27,808 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 10:50:27,813 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:50:28,532 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:50:28,536 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:50:30,039 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [25:20\u003c4:58:03,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 10:51:48,294 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:51:48,295 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:51:48,295 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6623059511184692, 'eval_accuracy': 0.8240997229916898, 'eval_runtime': 20.8002, 'eval_samples_per_second': 34.711, 'eval_steps_per_second': 0.144, 'epoch': 15.0}\n","  5% 165/3300 [25:41\u003c4:58:03,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:52:09,099 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 10:52:09,104 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:52:09,794 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:52:09,811 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:52:11,298 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [27:02\u003c4:51:34,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 10:53:29,724 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:53:29,724 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:53:29,725 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.603758692741394, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 20.7154, 'eval_samples_per_second': 34.853, 'eval_steps_per_second': 0.145, 'epoch': 16.0}\n","  5% 176/3300 [27:23\u003c4:51:34,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:53:50,448 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 10:53:50,453 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:53:51,153 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:53:51,157 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:53:52,598 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:46\u003c4:50:31,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 10:55:14,205 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:55:14,205 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:55:14,205 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6531258821487427, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.9311, 'eval_samples_per_second': 34.494, 'eval_steps_per_second': 0.143, 'epoch': 17.0}\n","  6% 187/3300 [29:07\u003c4:50:31,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:55:35,140 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 10:55:35,145 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:55:35,849 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:55:35,853 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:55:37,355 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:28\u003c4:48:23,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 10:56:56,220 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:56:56,220 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:56:56,220 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6180599927902222, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.8742, 'eval_samples_per_second': 34.588, 'eval_steps_per_second': 0.144, 'epoch': 18.0}\n","  6% 198/3300 [30:49\u003c4:48:23,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:57:17,099 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 10:57:17,104 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:57:17,829 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:57:17,833 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:57:19,390 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.196, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [32:10\u003c4:55:47,  5.74s/it][INFO|trainer.py:3213] 2023-10-03 10:58:37,728 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 10:58:37,729 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 10:58:37,729 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7082116007804871, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 21.0988, 'eval_samples_per_second': 34.22, 'eval_steps_per_second': 0.142, 'epoch': 19.0}\n","  6% 209/3300 [32:31\u003c4:55:47,  5.74s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 10:58:58,832 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 10:58:58,838 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 10:58:59,568 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 10:58:59,573 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 10:59:04,143 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:55\u003c4:53:52,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 11:00:22,349 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:00:22,349 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:00:22,349 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.664177656173706, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.7923, 'eval_samples_per_second': 34.724, 'eval_steps_per_second': 0.144, 'epoch': 20.0}\n","  7% 220/3300 [34:15\u003c4:53:52,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:00:43,145 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 11:00:43,151 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:00:43,882 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:00:43,886 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:00:45,467 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-198] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 11:00:45,517 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 11:00:45,517 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/checkpoint-110 (score: 0.5550879240036011).\n","{'train_runtime': 2058.69, 'train_samples_per_second': 407.444, 'train_steps_per_second': 1.603, 'train_loss': 0.4039554411714727, 'epoch': 20.0}\n","  7% 220/3300 [34:18\u003c8:00:21,  9.36s/it]\n","[INFO|trainer.py:2939] 2023-10-03 11:00:46,006 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/\n","[INFO|configuration_utils.py:460] 2023-10-03 11:00:47,905 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:00:48,631 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:00:48,636 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       20.0\n","  train_loss               =      0.404\n","  train_runtime            = 0:34:18.69\n","  train_samples_per_second =    407.444\n","  train_steps_per_second   =      1.603\n","[INFO|trainer.py:3213] 2023-10-03 11:00:48,650 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:00:48,650 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:00:48,650 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.06s/it]\n","***** eval metrics *****\n","  epoch                   =       20.0\n","  eval_accuracy           =     0.8241\n","  eval_loss               =     0.5551\n","  eval_runtime            = 0:00:21.10\n","  eval_samples_per_second =     34.204\n","  eval_steps_per_second   =      0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▇▇▇▇▆███▇████▇█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▂▂▂▂▂▂▁▂▂▁▁▂▁▂▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▆▆▂▃▂▂▇▅▆▃▃▂▄▃▁▅▄█▂█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▃▃▇▆▇▇▂▄▃▆▅▇▅▆█▄▅▁▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▃▃█▆█▆▃▃▃▆▆▆▆▆█▃▆▁▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.8241\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.55509\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.1087\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.204\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 20.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 220\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.196\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.335060951657677e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.40396\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2058.69\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 407.444\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.603\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mglad-disco-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/ysspzndh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_102619-ysspzndh/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_6/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 6 \\\n","    --seed 6 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"C5cP8uzzFVIB"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 18:20:56.773126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: W\u0026B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_182101-co78f2vt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-dust-11\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/co78f2vt\u001b[0m\n","10/03/2023 18:21:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 18:21:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=7,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/runs/Oct03_18-21-02_3e6bcf396725,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=7,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 216545.55it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 271338.74it/s]\n","Downloading data files: 100% 2796/2796 [00:00\u003c00:00, 79912.19it/s]\n","Downloading data files: 100% 5/5 [00:00\u003c00:00, 19897.08it/s]\n","Extracting data files: 100% 5/5 [00:00\u003c00:00, 2226.99it/s]\n","Downloading data files: 100% 722/722 [00:00\u003c00:00, 78569.07it/s]\n","Downloading data files: 100% 1/1 [00:00\u003c00:00, 6150.01it/s]\n","Extracting data files: 100% 1/1 [00:00\u003c00:00, 1370.69it/s]\n","Generating train split: 2796 examples [00:00, 9850.58 examples/s]\n","Generating validation split: 722 examples [00:00, 12695.73 examples/s]\n","Casting the dataset: 100% 2796/2796 [00:00\u003c00:00, 68240.94 examples/s]\n","Casting the dataset: 100% 722/722 [00:00\u003c00:00, 18922.18 examples/s]\n","Downloading builder script: 100% 4.20k/4.20k [00:00\u003c00:00, 6.03MB/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 18:21:08,596 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 18:21:08,597 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.35.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2990] 2023-10-03 18:21:08,600 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3775] 2023-10-03 18:21:09,578 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3796] 2023-10-03 18:21:09,578 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 18:21:09,591 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 18:21:09,592 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 18:21:14,458 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 18:21:14,458 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 18:21:14,459 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 18:21:14,459 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 18:21:14,459 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 18:21:14,459 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 18:21:14,459 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 18:21:14,460 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 18:21:14,461 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:27\u003c4:56:06,  5.40s/it][INFO|trainer.py:3213] 2023-10-03 18:22:41,857 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:22:41,857 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:22:41,857 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4584426879882812, 'eval_accuracy': 0.5886426592797784, 'eval_runtime': 21.1567, 'eval_samples_per_second': 34.126, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:48\u003c4:56:06,  5.40s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:23:03,017 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 18:23:03,022 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:23:03,716 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:23:03,721 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:09\u003c4:52:24,  5.35s/it][INFO|trainer.py:3213] 2023-10-03 18:24:24,224 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:24:24,224 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:24:24,224 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0454514026641846, 'eval_accuracy': 0.6939058171745153, 'eval_runtime': 20.6946, 'eval_samples_per_second': 34.888, 'eval_steps_per_second': 0.145, 'epoch': 2.0}\n","  1% 22/3300 [03:30\u003c4:52:24,  5.35s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:24:44,924 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 18:24:44,929 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:24:45,599 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:24:45,603 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:54\u003c4:59:09,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 18:26:08,528 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:26:08,528 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:26:08,528 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.77s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8393080234527588, 'eval_accuracy': 0.7493074792243767, 'eval_runtime': 20.6098, 'eval_samples_per_second': 35.032, 'eval_steps_per_second': 0.146, 'epoch': 3.0}\n","  1% 33/3300 [05:14\u003c4:59:09,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.81s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:26:29,143 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 18:26:29,149 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:26:29,872 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:26:29,877 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:34\u003c4:46:00,  5.27s/it][INFO|trainer.py:3213] 2023-10-03 18:27:49,176 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:27:49,176 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:27:49,176 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7416099309921265, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 20.6122, 'eval_samples_per_second': 35.028, 'eval_steps_per_second': 0.146, 'epoch': 4.0}\n","  1% 44/3300 [06:55\u003c4:46:00,  5.27s/it]\n","100% 3/3 [00:06\u003c00:00,  1.81s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:28:09,796 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 18:28:09,801 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:28:10,510 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:28:10,514 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:28:12,002 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:15\u003c4:57:41,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 18:29:29,610 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:29:29,611 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:29:29,611 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.89s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7193344235420227, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.8726, 'eval_samples_per_second': 34.591, 'eval_steps_per_second': 0.144, 'epoch': 5.0}\n","  2% 55/3300 [08:36\u003c4:57:41,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.88s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:29:50,487 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 18:29:50,493 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:29:51,173 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:29:51,177 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:29:52,633 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:59\u003c5:04:34,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 18:31:13,471 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:31:13,472 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:31:13,472 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6155366897583008, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 20.7056, 'eval_samples_per_second': 34.87, 'eval_steps_per_second': 0.145, 'epoch': 6.0}\n","  2% 66/3300 [10:19\u003c5:04:34,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:31:34,185 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 18:31:34,191 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:31:34,915 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:31:34,920 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:31:36,415 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:40\u003c5:06:44,  5.71s/it][INFO|trainer.py:3213] 2023-10-03 18:32:54,758 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:32:54,759 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:32:54,759 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6370583176612854, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.6254, 'eval_samples_per_second': 35.005, 'eval_steps_per_second': 0.145, 'epoch': 7.0}\n","  2% 77/3300 [12:00\u003c5:06:44,  5.71s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:33:15,390 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 18:33:15,407 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:33:16,103 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:33:16,107 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:33:17,624 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:20\u003c5:04:08,  5.68s/it][INFO|trainer.py:3213] 2023-10-03 18:34:35,135 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:34:35,135 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:34:35,135 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.620820164680481, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 20.7161, 'eval_samples_per_second': 34.852, 'eval_steps_per_second': 0.145, 'epoch': 8.0}\n","  3% 88/3300 [13:41\u003c5:04:08,  5.68s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:34:55,856 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 18:34:55,861 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:34:56,567 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:34:56,572 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:35:01,047 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [15:04\u003c4:53:21,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 18:36:18,804 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:36:18,804 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:36:18,804 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.77s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7272672653198242, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 20.6307, 'eval_samples_per_second': 34.996, 'eval_steps_per_second': 0.145, 'epoch': 9.0}\n","  3% 99/3300 [15:24\u003c4:53:21,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.81s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:36:39,441 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 18:36:39,446 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:36:40,124 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:36:40,128 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:36:41,584 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.6312, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:45\u003c4:52:46,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 18:37:59,986 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:37:59,986 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:37:59,987 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.639232337474823, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.6445, 'eval_samples_per_second': 34.973, 'eval_steps_per_second': 0.145, 'epoch': 10.0}\n","  3% 110/3300 [17:06\u003c4:52:46,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:38:20,636 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 18:38:20,642 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:38:21,354 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:38:21,359 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:38:22,826 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [18:27\u003c4:49:45,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 18:39:41,754 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:39:41,754 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:39:41,755 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6517173051834106, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.9113, 'eval_samples_per_second': 34.527, 'eval_steps_per_second': 0.143, 'epoch': 11.0}\n","  4% 121/3300 [18:48\u003c4:49:45,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:40:02,671 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 18:40:02,676 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:40:03,382 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:40:03,400 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:40:04,872 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [20:08\u003c4:48:21,  5.46s/it][INFO|trainer.py:3213] 2023-10-03 18:41:22,981 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:41:22,982 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:41:22,982 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5970571637153625, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.6217, 'eval_samples_per_second': 35.012, 'eval_steps_per_second': 0.145, 'epoch': 12.0}\n","  4% 132/3300 [20:29\u003c4:48:21,  5.46s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:41:43,608 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 18:41:43,613 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:41:44,303 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:41:44,308 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:41:45,753 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-66] due to args.save_total_limit\n","  4% 143/3300 [21:52\u003c4:48:13,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 18:43:07,262 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:43:07,262 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:43:07,262 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.76s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6963607668876648, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.5638, 'eval_samples_per_second': 35.11, 'eval_steps_per_second': 0.146, 'epoch': 13.0}\n","  4% 143/3300 [22:13\u003c4:48:13,  5.48s/it]\n","100% 3/3 [00:05\u003c00:00,  1.80s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:43:27,830 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 18:43:27,835 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:43:28,533 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:43:28,537 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:43:30,076 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [23:34\u003c4:51:42,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 18:44:48,555 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:44:48,556 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:44:48,556 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7495563626289368, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 20.741, 'eval_samples_per_second': 34.81, 'eval_steps_per_second': 0.145, 'epoch': 14.0}\n","  5% 154/3300 [23:54\u003c4:51:42,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:45:09,302 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 18:45:09,308 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:45:10,033 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:45:10,037 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:45:11,547 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [25:16\u003c4:51:17,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 18:46:30,496 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:46:30,497 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:46:30,497 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6953898668289185, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.7387, 'eval_samples_per_second': 34.814, 'eval_steps_per_second': 0.145, 'epoch': 15.0}\n","  5% 165/3300 [25:36\u003c4:51:17,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:46:51,241 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 18:46:51,246 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:46:51,947 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:46:51,951 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:46:53,447 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [26:57\u003c4:53:36,  5.64s/it][INFO|trainer.py:3213] 2023-10-03 18:48:12,190 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:48:12,190 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:48:12,190 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7001627683639526, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 20.632, 'eval_samples_per_second': 34.994, 'eval_steps_per_second': 0.145, 'epoch': 16.0}\n","  5% 176/3300 [27:18\u003c4:53:36,  5.64s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:48:32,828 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 18:48:32,833 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:48:33,534 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:48:33,538 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:48:35,083 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:41\u003c4:53:31,  5.66s/it][INFO|trainer.py:3213] 2023-10-03 18:49:56,203 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:49:56,203 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:49:56,204 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.655767023563385, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.6408, 'eval_samples_per_second': 34.979, 'eval_steps_per_second': 0.145, 'epoch': 17.0}\n","  6% 187/3300 [29:02\u003c4:53:31,  5.66s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:50:16,850 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 18:50:16,857 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:50:17,549 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:50:17,554 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:50:19,053 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:23\u003c4:48:43,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 18:51:37,842 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:51:37,843 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:51:37,843 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6536957025527954, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.6875, 'eval_samples_per_second': 34.9, 'eval_steps_per_second': 0.145, 'epoch': 18.0}\n","  6% 198/3300 [30:44\u003c4:48:43,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:51:58,536 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 18:51:58,554 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:51:59,237 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:51:59,242 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:52:00,676 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2085, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [32:04\u003c4:49:30,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 18:53:18,802 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:53:18,802 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:53:18,802 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6206307411193848, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.6384, 'eval_samples_per_second': 34.983, 'eval_steps_per_second': 0.145, 'epoch': 19.0}\n","  6% 209/3300 [32:24\u003c4:49:30,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:53:39,446 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 18:53:39,452 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:53:40,146 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:53:40,150 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:53:44,650 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:48\u003c4:46:14,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 18:55:03,016 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:55:03,016 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:55:03,016 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6431124210357666, 'eval_accuracy': 0.8157894736842105, 'eval_runtime': 20.7916, 'eval_samples_per_second': 34.726, 'eval_steps_per_second': 0.144, 'epoch': 20.0}\n","  7% 220/3300 [34:09\u003c4:46:14,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:55:23,812 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 18:55:23,818 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:55:24,515 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:55:24,519 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:55:26,036 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [35:29\u003c4:51:11,  5.69s/it][INFO|trainer.py:3213] 2023-10-03 18:56:44,335 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:56:44,335 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:56:44,335 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6656123995780945, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.7177, 'eval_samples_per_second': 34.849, 'eval_steps_per_second': 0.145, 'epoch': 21.0}\n","  7% 231/3300 [35:50\u003c4:51:11,  5.69s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:57:05,059 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 18:57:05,064 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:57:05,757 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:57:05,761 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:57:07,209 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [37:10\u003c4:40:59,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 18:58:25,082 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:58:25,083 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:58:25,083 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.78s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8112930059432983, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 20.6259, 'eval_samples_per_second': 35.005, 'eval_steps_per_second': 0.145, 'epoch': 22.0}\n","  7% 242/3300 [37:31\u003c4:40:59,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.82s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 18:58:45,714 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 18:58:45,719 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:58:46,401 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:58:46,405 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 18:58:50,894 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-220] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 18:58:50,903 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 18:58:50,903 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/checkpoint-132 (score: 0.5970571637153625).\n","{'train_runtime': 2256.8796, 'train_samples_per_second': 371.664, 'train_steps_per_second': 1.462, 'train_loss': 0.37784564593606745, 'epoch': 22.0}\n","  7% 242/3300 [37:36\u003c7:55:18,  9.33s/it]\n","[INFO|trainer.py:2939] 2023-10-03 18:58:51,344 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/\n","[INFO|configuration_utils.py:460] 2023-10-03 18:58:51,350 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/config.json\n","[INFO|modeling_utils.py:2118] 2023-10-03 18:58:52,068 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 18:58:52,072 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       22.0\n","  train_loss               =     0.3778\n","  train_runtime            = 0:37:36.87\n","  train_samples_per_second =    371.664\n","  train_steps_per_second   =      1.462\n","[INFO|trainer.py:3213] 2023-10-03 18:58:52,087 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 18:58:52,087 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 18:58:52,087 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.03s/it]\n","***** eval metrics *****\n","  epoch                   =       22.0\n","  eval_accuracy           =     0.8047\n","  eval_loss               =     0.5971\n","  eval_runtime            = 0:00:21.13\n","  eval_samples_per_second =     34.169\n","  eval_steps_per_second   =      0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▇▇███▇██▇▇▇▇▇█▇██▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▂▂▁▁▁▂▁▁▁▂▂▂▂▁▁▁▁▂▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▂▂▅▃▂▃▂▂▅▂▁▃▃▂▂▂▂▄▃▂█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▇▇▄▆▇▆▇▇▄▇█▆▆▇▇▇▇▅▆▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆██▄▆▆▆▆▆▃▆█▆▆▆▆▆▆▄▆▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.80471\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.59706\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 21.1301\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.169\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.142\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 22.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 242\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2085\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.768567046823444e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.37785\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2256.8796\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 371.664\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.462\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwhole-dust-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/co78f2vt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_182101-co78f2vt/logs\u001b[0m\n","Exception in thread IntMsgThr:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","Exception in thread ChkStopThr:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 299, in check_internal_messages\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 285, in check_stop_status\n","    self._loop_check_status(\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n","    self._loop_check_status(\n","    local_handle = request()\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 743, in deliver_internal_messages\n","    local_handle = request()\n","    return self._deliver_internal_messages(internal_message)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 727, in deliver_stop_status\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 472, in _deliver_internal_messages\n","    return self._deliver_stop_status(status)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 450, in _deliver_stop_status\n","    return self._deliver_record(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n","    return self._deliver_record(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n","    handle = mailbox._deliver_record(record, interface=self)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n","    handle = mailbox._deliver_record(record, interface=self)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n","    interface._publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n","    interface._publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n","    self._sock_client.send_record_publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n","    self._sock_client.send_record_publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n","    self.send_server_request(server_req)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n","    self.send_server_request(server_req)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n","    self._send_message(msg)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n","    self._send_message(msg)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n","    self._sendall_with_error_handle(header + data)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n","    self._sendall_with_error_handle(header + data)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n","    sent = self._sock.send(data)\n","BrokenPipeError: [Errno 32] Broken pipe\n","    sent = self._sock.send(data)\n","BrokenPipeError: [Errno 32] Broken pipe\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_7/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 7 \\\n","    --seed 7 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2467383,"status":"ok","timestamp":1696334502473,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"-nDNgY-PFYRZ","outputId":"2ef74e17-4626-4b15-cce9-d46bceda5722"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 11:20:38.491736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_112042-vkip3qkd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfloral-mountain-8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/vkip3qkd\u001b[0m\n","10/03/2023 11:20:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 11:20:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=8,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/runs/Oct03_11-20-43_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=8,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 58847.15it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 354593.29it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 11:20:48,211 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 11:20:48,212 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 11:20:48,214 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 11:20:49,182 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 11:20:49,183 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 11:20:49,200 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 11:20:49,201 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 11:20:51,135 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 11:20:51,136 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 11:20:51,136 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 11:20:51,136 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 11:20:51,136 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 11:20:51,136 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 11:20:51,136 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 11:20:51,137 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 11:20:51,138 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:22\u003c5:06:26,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 11:22:14,122 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:22:14,122 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:22:14,122 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.443265676498413, 'eval_accuracy': 0.5844875346260388, 'eval_runtime': 21.1802, 'eval_samples_per_second': 34.089, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:44\u003c5:06:26,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:22:35,304 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 11:22:35,305 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:22:35,735 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:22:35,736 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:03\u003c4:59:43,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 11:23:54,601 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:23:54,601 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:23:54,601 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.021414875984192, 'eval_accuracy': 0.7160664819944599, 'eval_runtime': 20.6758, 'eval_samples_per_second': 34.92, 'eval_steps_per_second': 0.145, 'epoch': 2.0}\n","  1% 22/3300 [03:24\u003c4:59:43,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:24:15,279 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 11:24:15,281 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:24:15,692 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:24:15,693 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:43\u003c5:09:51,  5.69s/it][INFO|trainer.py:3213] 2023-10-03 11:25:34,294 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:25:34,294 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:25:34,294 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7606306076049805, 'eval_accuracy': 0.7797783933518005, 'eval_runtime': 20.6267, 'eval_samples_per_second': 35.003, 'eval_steps_per_second': 0.145, 'epoch': 3.0}\n","  1% 33/3300 [05:03\u003c5:09:51,  5.69s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:25:54,922 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 11:25:54,924 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:25:55,330 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:25:55,331 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:22\u003c4:57:30,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 11:27:13,912 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:27:13,912 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:27:13,912 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7431321144104004, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 20.8166, 'eval_samples_per_second': 34.684, 'eval_steps_per_second': 0.144, 'epoch': 4.0}\n","  1% 44/3300 [06:43\u003c4:57:30,  5.48s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:27:34,730 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 11:27:34,731 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:27:35,142 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:27:35,143 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:27:35,990 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:03\u003c5:04:42,  5.63s/it][INFO|trainer.py:3213] 2023-10-03 11:28:54,425 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:28:54,425 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:28:54,425 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7361953258514404, 'eval_accuracy': 0.7853185595567868, 'eval_runtime': 20.8197, 'eval_samples_per_second': 34.679, 'eval_steps_per_second': 0.144, 'epoch': 5.0}\n","  2% 55/3300 [08:24\u003c5:04:42,  5.63s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:29:15,246 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 11:29:15,247 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:29:15,658 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:29:15,659 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:29:16,516 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:43\u003c5:05:14,  5.66s/it][INFO|trainer.py:3213] 2023-10-03 11:30:34,691 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:30:34,691 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:30:34,691 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.90s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7499303221702576, 'eval_accuracy': 0.7714681440443213, 'eval_runtime': 21.0569, 'eval_samples_per_second': 34.288, 'eval_steps_per_second': 0.142, 'epoch': 6.0}\n","  2% 66/3300 [10:04\u003c5:05:14,  5.66s/it]\n","100% 3/3 [00:06\u003c00:00,  1.89s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:30:55,749 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 11:30:55,751 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:30:56,174 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:30:56,175 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:30:57,046 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:24\u003c5:02:59,  5.64s/it][INFO|trainer.py:3213] 2023-10-03 11:32:15,335 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:32:15,335 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:32:15,336 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6409269571304321, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.9033, 'eval_samples_per_second': 34.54, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n","  2% 77/3300 [11:45\u003c5:02:59,  5.64s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:32:36,240 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 11:32:36,241 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:32:36,663 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:32:36,664 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:32:37,549 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:05\u003c5:01:55,  5.64s/it][INFO|trainer.py:3213] 2023-10-03 11:33:56,275 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:33:56,275 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:33:56,275 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.677753210067749, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.6954, 'eval_samples_per_second': 34.887, 'eval_steps_per_second': 0.145, 'epoch': 8.0}\n","  3% 88/3300 [13:25\u003c5:01:55,  5.64s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:34:16,972 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 11:34:16,973 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:34:17,380 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:34:17,380 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:34:18,216 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [14:45\u003c5:10:03,  5.81s/it][INFO|trainer.py:3213] 2023-10-03 11:35:36,785 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:35:36,785 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:35:36,785 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.88s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6794976592063904, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.9295, 'eval_samples_per_second': 34.497, 'eval_steps_per_second': 0.143, 'epoch': 9.0}\n","  3% 99/3300 [15:06\u003c5:10:03,  5.81s/it]\n","100% 3/3 [00:06\u003c00:00,  1.88s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:35:57,716 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 11:35:57,717 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:35:58,129 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:35:58,130 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:35:58,991 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6316, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:26\u003c4:56:54,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 11:37:17,858 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:37:17,859 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:37:17,859 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.703184187412262, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.87, 'eval_samples_per_second': 34.595, 'eval_steps_per_second': 0.144, 'epoch': 10.0}\n","  3% 110/3300 [16:47\u003c4:56:54,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:37:38,730 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 11:37:38,732 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:37:39,163 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:37:39,163 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:37:40,007 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [18:07\u003c4:52:48,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 11:38:58,596 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:38:58,596 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:38:58,596 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6983829140663147, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 20.9672, 'eval_samples_per_second': 34.435, 'eval_steps_per_second': 0.143, 'epoch': 11.0}\n","  4% 121/3300 [18:28\u003c4:52:48,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:39:19,565 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 11:39:19,567 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:39:19,996 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:39:19,997 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:39:20,896 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [19:48\u003c4:59:04,  5.66s/it][INFO|trainer.py:3213] 2023-10-03 11:40:39,622 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:40:39,623 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:40:39,623 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.656847357749939, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.7229, 'eval_samples_per_second': 34.841, 'eval_steps_per_second': 0.145, 'epoch': 12.0}\n","  4% 132/3300 [20:09\u003c4:59:04,  5.66s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:41:00,348 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 11:41:00,349 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:41:00,762 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:41:00,763 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:41:01,629 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [21:29\u003c4:48:07,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 11:42:20,211 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:42:20,211 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:42:20,211 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7261964678764343, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 20.8495, 'eval_samples_per_second': 34.629, 'eval_steps_per_second': 0.144, 'epoch': 13.0}\n","  4% 143/3300 [21:49\u003c4:48:07,  5.48s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:42:41,062 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 11:42:41,063 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:42:41,478 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:42:41,479 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:42:42,377 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [23:09\u003c4:59:44,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 11:44:00,783 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:44:00,783 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:44:00,783 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6249679923057556, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.7676, 'eval_samples_per_second': 34.766, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [23:30\u003c4:59:44,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:44:21,553 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 11:44:21,554 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:44:21,963 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:44:21,964 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:44:22,842 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-77] due to args.save_total_limit\n","  5% 165/3300 [24:49\u003c4:54:39,  5.64s/it][INFO|trainer.py:3213] 2023-10-03 11:45:41,101 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:45:41,102 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:45:41,102 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.87s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7270514369010925, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 20.8693, 'eval_samples_per_second': 34.596, 'eval_steps_per_second': 0.144, 'epoch': 15.0}\n","  5% 165/3300 [25:10\u003c4:54:39,  5.64s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:46:01,973 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 11:46:01,974 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:46:02,382 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:46:02,383 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:46:03,226 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-132] due to args.save_total_limit\n","  5% 176/3300 [26:30\u003c5:03:11,  5.82s/it][INFO|trainer.py:3213] 2023-10-03 11:47:21,707 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:47:21,707 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:47:21,708 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6556240320205688, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.8604, 'eval_samples_per_second': 34.611, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [26:51\u003c5:03:11,  5.82s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:47:42,570 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 11:47:42,571 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:47:42,988 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:47:42,989 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:47:43,864 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-143] due to args.save_total_limit\n","  6% 187/3300 [28:11\u003c4:53:19,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 11:49:02,632 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:49:02,633 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:49:02,633 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6314004063606262, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.7691, 'eval_samples_per_second': 34.763, 'eval_steps_per_second': 0.144, 'epoch': 17.0}\n","  6% 187/3300 [28:32\u003c4:53:19,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:49:23,403 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 11:49:23,405 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:49:23,819 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:49:23,820 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:49:24,666 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [29:51\u003c4:53:48,  5.68s/it][INFO|trainer.py:3213] 2023-10-03 11:50:43,084 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:50:43,084 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:50:43,085 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.626331090927124, 'eval_accuracy': 0.8268698060941828, 'eval_runtime': 20.982, 'eval_samples_per_second': 34.41, 'eval_steps_per_second': 0.143, 'epoch': 18.0}\n","  6% 198/3300 [30:12\u003c4:53:48,  5.68s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:51:04,068 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 11:51:04,069 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:51:04,481 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:51:04,482 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:51:05,328 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2075, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [31:32\u003c4:42:58,  5.49s/it][INFO|trainer.py:3213] 2023-10-03 11:52:23,826 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:52:23,826 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:52:23,826 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6916636824607849, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.9288, 'eval_samples_per_second': 34.498, 'eval_steps_per_second': 0.143, 'epoch': 19.0}\n","  6% 209/3300 [31:53\u003c4:42:58,  5.49s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:52:44,757 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 11:52:44,759 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:52:45,179 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:52:45,180 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:52:46,070 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:13\u003c4:41:19,  5.48s/it][INFO|trainer.py:3213] 2023-10-03 11:54:04,929 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:54:04,929 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:54:04,930 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6637141704559326, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 20.8784, 'eval_samples_per_second': 34.581, 'eval_steps_per_second': 0.144, 'epoch': 20.0}\n","  7% 220/3300 [33:34\u003c4:41:19,  5.48s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:54:25,809 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 11:54:25,811 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:54:26,241 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:54:26,242 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:54:27,124 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [34:54\u003c4:39:48,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 11:55:45,478 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:55:45,478 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:55:45,478 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.87s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8147292137145996, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 21.0309, 'eval_samples_per_second': 34.33, 'eval_steps_per_second': 0.143, 'epoch': 21.0}\n","  7% 231/3300 [35:15\u003c4:39:48,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:56:06,511 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 11:56:06,512 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:56:06,927 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:56:06,928 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:56:07,801 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [36:35\u003c4:42:04,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 11:57:26,670 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:57:26,671 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:57:26,671 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6951216459274292, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 20.806, 'eval_samples_per_second': 34.701, 'eval_steps_per_second': 0.144, 'epoch': 22.0}\n","  7% 242/3300 [36:56\u003c4:42:04,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:57:47,478 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 11:57:47,480 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:57:47,904 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:57:47,905 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:57:48,768 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [38:16\u003c4:39:31,  5.50s/it][INFO|trainer.py:3213] 2023-10-03 11:59:07,401 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 11:59:07,402 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 11:59:07,402 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6524882316589355, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.6818, 'eval_samples_per_second': 34.91, 'eval_steps_per_second': 0.145, 'epoch': 23.0}\n","  8% 253/3300 [38:36\u003c4:39:31,  5.50s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 11:59:28,085 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 11:59:28,087 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 11:59:28,495 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 11:59:28,495 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 11:59:29,343 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [39:56\u003c4:38:34,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 12:00:47,548 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:00:47,548 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:00:47,549 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7331382632255554, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 20.7962, 'eval_samples_per_second': 34.718, 'eval_steps_per_second': 0.144, 'epoch': 24.0}\n","  8% 264/3300 [40:17\u003c4:38:34,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:01:08,346 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-03 12:01:08,347 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:01:08,753 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:01:08,754 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:01:09,584 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-242] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 12:01:09,706 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 12:01:09,706 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/checkpoint-154 (score: 0.6249679923057556).\n","{'train_runtime': 2418.7521, 'train_samples_per_second': 346.79, 'train_steps_per_second': 1.364, 'train_loss': 0.3625876795161854, 'epoch': 24.0}\n","  8% 264/3300 [40:18\u003c7:43:35,  9.16s/it]\n","[INFO|trainer.py:2939] 2023-10-03 12:01:09,891 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/\n","[INFO|configuration_utils.py:460] 2023-10-03 12:01:09,893 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:01:10,453 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:01:10,454 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       24.0\n","  train_loss               =     0.3626\n","  train_runtime            = 0:40:18.75\n","  train_samples_per_second =     346.79\n","  train_steps_per_second   =      1.364\n","[INFO|trainer.py:3213] 2023-10-03 12:01:10,457 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:01:10,457 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:01:10,457 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.05s/it]\n","***** eval metrics *****\n","  epoch                   =       24.0\n","  eval_accuracy           =     0.7978\n","  eval_loss               =      0.625\n","  eval_runtime            = 0:00:20.71\n","  eval_samples_per_second =     34.853\n","  eval_steps_per_second   =      0.145\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▇▇▇▆█▇▇▇▇█▇▇▇▇████▇▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▂▂▂▂▁▁▁▂▂▁▂▁▂▁▁▁▂▁▃▂▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▂▁▃▃▆▄▂▅▄▅▂▄▃▄▄▃▅▅▄▆▃▂▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▇█▆▆▃▄▇▄▅▄▇▅▆▅▅▆▃▄▅▃▆▇▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁██▆▆▁▆█▃▆▃█▆▆▆▆▆▃▃▆▃▆█▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.79778\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.62497\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.7159\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.853\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.145\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 24.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 264\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2075\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.202073141989212e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.36259\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2418.7521\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 346.79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.364\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfloral-mountain-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/vkip3qkd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_112042-vkip3qkd/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_8/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 8 \\\n","    --seed 8 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3383861,"status":"ok","timestamp":1696337886319,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"GJ6zuVm0Fa-J","outputId":"7ccd7f31-a66e-4737-a919-04da8c9d359b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 12:01:45.283448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_120149-upcltc93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-shape-9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/upcltc93\u001b[0m\n","10/03/2023 12:01:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 12:01:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=9,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/runs/Oct03_12-01-50_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=9,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 106902.33it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 359128.59it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 12:01:54,672 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 12:01:54,673 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 12:01:54,676 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 12:01:55,593 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 12:01:55,593 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 12:01:55,609 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 12:01:55,611 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 12:01:57,490 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 12:01:57,490 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 12:01:57,490 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 12:01:57,491 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 12:01:57,491 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 12:01:57,491 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 12:01:57,491 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 12:01:57,492 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 12:01:57,493 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:22\u003c4:59:21,  5.46s/it][INFO|trainer.py:3213] 2023-10-03 12:03:19,905 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:03:19,905 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:03:19,905 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4082905054092407, 'eval_accuracy': 0.610803324099723, 'eval_runtime': 21.1601, 'eval_samples_per_second': 34.121, 'eval_steps_per_second': 0.142, 'epoch': 1.0}\n","  0% 11/3300 [01:43\u003c4:59:21,  5.46s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:03:41,067 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 12:03:41,068 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:03:41,477 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:03:41,478 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:02\u003c5:08:36,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 12:05:00,414 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:05:00,414 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:05:00,414 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0685861110687256, 'eval_accuracy': 0.6980609418282548, 'eval_runtime': 20.8367, 'eval_samples_per_second': 34.65, 'eval_steps_per_second': 0.144, 'epoch': 2.0}\n","  1% 22/3300 [03:23\u003c5:08:36,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:05:21,253 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 12:05:21,254 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:05:21,685 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:05:21,686 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:43\u003c5:03:43,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 12:06:41,288 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:06:41,288 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:06:41,288 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.891061544418335, 'eval_accuracy': 0.7520775623268698, 'eval_runtime': 20.7437, 'eval_samples_per_second': 34.806, 'eval_steps_per_second': 0.145, 'epoch': 3.0}\n","  1% 33/3300 [05:04\u003c5:03:43,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:07:02,034 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 12:07:02,035 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:07:02,462 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:07:02,463 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:24\u003c5:11:46,  5.75s/it][INFO|trainer.py:3213] 2023-10-03 12:08:21,936 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:08:21,936 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:08:21,937 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7567278742790222, 'eval_accuracy': 0.760387811634349, 'eval_runtime': 20.7606, 'eval_samples_per_second': 34.777, 'eval_steps_per_second': 0.145, 'epoch': 4.0}\n","  1% 44/3300 [06:45\u003c5:11:46,  5.75s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:08:42,699 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 12:08:42,701 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:08:43,104 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:08:43,105 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:08:43,948 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:04\u003c4:52:57,  5.42s/it][INFO|trainer.py:3213] 2023-10-03 12:10:01,937 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:10:01,937 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:10:01,938 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6580923199653625, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.6625, 'eval_samples_per_second': 34.942, 'eval_steps_per_second': 0.145, 'epoch': 5.0}\n","  2% 55/3300 [08:25\u003c4:52:57,  5.42s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:10:22,602 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 12:10:22,604 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:10:23,014 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:10:23,015 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:10:23,879 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:44\u003c5:04:34,  5.65s/it][INFO|trainer.py:3213] 2023-10-03 12:11:42,214 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:11:42,214 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:11:42,214 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.79s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6340081095695496, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.8211, 'eval_samples_per_second': 34.676, 'eval_steps_per_second': 0.144, 'epoch': 6.0}\n","  2% 66/3300 [10:05\u003c5:04:34,  5.65s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:12:03,037 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 12:12:03,038 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:12:03,455 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:12:03,456 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:12:04,361 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:25\u003c5:00:07,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 12:13:22,794 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:13:22,795 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:13:22,795 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6894057989120483, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.7078, 'eval_samples_per_second': 34.866, 'eval_steps_per_second': 0.145, 'epoch': 7.0}\n","  2% 77/3300 [11:46\u003c5:00:07,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:13:43,504 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 12:13:43,505 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:13:43,909 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:13:43,910 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:13:44,751 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:05\u003c4:54:55,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 12:15:02,758 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:15:02,758 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:15:02,758 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6588912606239319, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 22.5497, 'eval_samples_per_second': 32.018, 'eval_steps_per_second': 0.133, 'epoch': 8.0}\n","  3% 88/3300 [13:27\u003c4:54:55,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:15:25,310 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 12:15:25,311 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:15:25,744 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:15:25,745 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:15:26,626 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [14:55\u003c5:14:34,  5.90s/it][INFO|trainer.py:3213] 2023-10-03 12:16:52,890 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:16:52,891 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:16:52,891 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6518534421920776, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 20.7305, 'eval_samples_per_second': 34.828, 'eval_steps_per_second': 0.145, 'epoch': 9.0}\n","  3% 99/3300 [15:16\u003c5:14:34,  5.90s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:17:13,623 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 12:17:13,624 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:17:14,027 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:17:14,028 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:17:14,880 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.6329, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:35\u003c5:04:09,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 12:18:32,991 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:18:32,991 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:18:32,991 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6234413385391235, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 20.8593, 'eval_samples_per_second': 34.613, 'eval_steps_per_second': 0.144, 'epoch': 10.0}\n","  3% 110/3300 [16:56\u003c5:04:09,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:18:53,852 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 12:18:53,854 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:18:54,270 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:18:54,271 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:18:55,115 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-66] due to args.save_total_limit\n","  4% 121/3300 [18:16\u003c4:52:07,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 12:20:13,846 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:20:13,846 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:20:13,846 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.91s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6197280883789062, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 21.1045, 'eval_samples_per_second': 34.211, 'eval_steps_per_second': 0.142, 'epoch': 11.0}\n","  4% 121/3300 [18:37\u003c4:52:07,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.89s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:20:34,953 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 12:20:34,954 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:20:35,389 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:20:35,390 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:20:36,282 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [19:57\u003c4:51:36,  5.52s/it][INFO|trainer.py:3213] 2023-10-03 12:21:54,578 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:21:54,578 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:21:54,579 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7059202790260315, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 20.9711, 'eval_samples_per_second': 34.428, 'eval_steps_per_second': 0.143, 'epoch': 12.0}\n","  4% 132/3300 [20:18\u003c4:51:36,  5.52s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:22:15,552 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 12:22:15,553 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:22:15,969 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:22:15,970 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:22:16,834 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [21:37\u003c4:53:28,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 12:23:35,481 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:23:35,481 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:23:35,481 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6000157594680786, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 20.8997, 'eval_samples_per_second': 34.546, 'eval_steps_per_second': 0.144, 'epoch': 13.0}\n","  4% 143/3300 [21:58\u003c4:53:28,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:23:56,382 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 12:23:56,384 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:23:56,818 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:23:56,819 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:23:57,694 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [23:18\u003c4:54:52,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 12:25:16,269 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:25:16,269 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:25:16,270 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6477533578872681, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 20.8277, 'eval_samples_per_second': 34.665, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [23:39\u003c4:54:52,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:25:37,099 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 12:25:37,100 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:25:37,520 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:25:37,521 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:25:38,425 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [24:59\u003c4:50:31,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 12:26:57,133 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:26:57,133 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:26:57,134 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7048801779747009, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 20.9137, 'eval_samples_per_second': 34.523, 'eval_steps_per_second': 0.143, 'epoch': 15.0}\n","  5% 165/3300 [25:20\u003c4:50:31,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:27:18,049 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 12:27:18,050 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:27:18,487 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:27:18,488 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:27:19,417 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-132] due to args.save_total_limit\n","  5% 176/3300 [26:40\u003c4:58:10,  5.73s/it][INFO|trainer.py:3213] 2023-10-03 12:28:38,408 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:28:38,408 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:28:38,408 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6092897653579712, 'eval_accuracy': 0.8157894736842105, 'eval_runtime': 20.8558, 'eval_samples_per_second': 34.619, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [27:01\u003c4:58:10,  5.73s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:28:59,265 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 12:28:59,267 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:28:59,680 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:28:59,681 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:29:00,528 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:21\u003c4:45:43,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 12:30:19,322 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:30:19,323 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:30:19,323 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6296217441558838, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.8727, 'eval_samples_per_second': 34.591, 'eval_steps_per_second': 0.144, 'epoch': 17.0}\n","  6% 187/3300 [28:42\u003c4:45:43,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:30:40,197 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 12:30:40,199 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:30:40,625 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:30:40,626 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:30:41,473 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [30:02\u003c4:47:07,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 12:32:00,217 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:32:00,217 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:32:00,217 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6520741581916809, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.842, 'eval_samples_per_second': 34.642, 'eval_steps_per_second': 0.144, 'epoch': 18.0}\n","  6% 198/3300 [30:23\u003c4:47:07,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:32:21,060 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 12:32:21,061 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:32:21,470 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:32:21,471 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:32:22,318 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2086, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [31:43\u003c4:53:29,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 12:33:40,532 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:33:40,532 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:33:40,533 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6519251465797424, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.8397, 'eval_samples_per_second': 34.645, 'eval_steps_per_second': 0.144, 'epoch': 19.0}\n","  6% 209/3300 [32:03\u003c4:53:29,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:34:01,373 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 12:34:01,375 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:34:01,794 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:34:01,795 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:34:02,640 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [33:23\u003c4:45:54,  5.57s/it][INFO|trainer.py:3213] 2023-10-03 12:35:21,180 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:35:21,181 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:35:21,181 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6569328904151917, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 20.8768, 'eval_samples_per_second': 34.584, 'eval_steps_per_second': 0.144, 'epoch': 20.0}\n","  7% 220/3300 [33:44\u003c4:45:54,  5.57s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:35:42,059 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 12:35:42,061 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:35:42,478 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:35:42,479 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:35:43,345 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [35:04\u003c4:45:53,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 12:37:01,891 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:37:01,891 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:37:01,891 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6735242009162903, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.8466, 'eval_samples_per_second': 34.634, 'eval_steps_per_second': 0.144, 'epoch': 21.0}\n","  7% 231/3300 [35:25\u003c4:45:53,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:37:22,740 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 12:37:22,741 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:37:23,155 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:37:23,156 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:37:24,010 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [36:45\u003c4:42:01,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 12:38:42,584 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:38:42,584 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:38:42,584 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.620352029800415, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.8206, 'eval_samples_per_second': 34.677, 'eval_steps_per_second': 0.144, 'epoch': 22.0}\n","  7% 242/3300 [37:05\u003c4:42:01,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:39:03,407 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 12:39:03,408 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:39:03,827 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:39:03,828 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:39:04,676 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [38:25\u003c4:40:30,  5.52s/it][INFO|trainer.py:3213] 2023-10-03 12:40:22,852 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:40:22,852 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:40:22,852 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5591697692871094, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 20.8166, 'eval_samples_per_second': 34.684, 'eval_steps_per_second': 0.144, 'epoch': 23.0}\n","  8% 253/3300 [38:46\u003c4:40:30,  5.52s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:40:43,670 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 12:40:43,671 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:40:44,091 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:40:44,092 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:40:44,957 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-143] due to args.save_total_limit\n","  8% 264/3300 [40:05\u003c4:49:15,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 12:42:03,214 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:42:03,214 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:42:03,215 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6661218404769897, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.7934, 'eval_samples_per_second': 34.723, 'eval_steps_per_second': 0.144, 'epoch': 24.0}\n","  8% 264/3300 [40:26\u003c4:49:15,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:42:24,010 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-03 12:42:24,012 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:42:24,424 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:42:24,425 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:42:25,271 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-231] due to args.save_total_limit\n","  8% 275/3300 [41:46\u003c4:51:24,  5.78s/it][INFO|trainer.py:3213] 2023-10-03 12:43:43,835 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:43:43,835 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:43:43,835 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7021632194519043, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.8546, 'eval_samples_per_second': 34.621, 'eval_steps_per_second': 0.144, 'epoch': 25.0}\n","  8% 275/3300 [42:07\u003c4:51:24,  5.78s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:44:04,691 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-03 12:44:04,692 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:44:05,128 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:44:05,129 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:44:06,001 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-242] due to args.save_total_limit\n","  9% 286/3300 [43:27\u003c4:46:15,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 12:45:25,309 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:45:25,309 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:45:25,309 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6626558303833008, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 20.7983, 'eval_samples_per_second': 34.714, 'eval_steps_per_second': 0.144, 'epoch': 26.0}\n","  9% 286/3300 [43:48\u003c4:46:15,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:45:46,109 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-03 12:45:46,110 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:45:46,524 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:45:46,525 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:45:47,391 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [45:08\u003c4:46:20,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 12:47:06,116 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:47:06,116 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:47:06,116 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6473354697227478, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 20.9686, 'eval_samples_per_second': 34.432, 'eval_steps_per_second': 0.143, 'epoch': 27.0}\n","  9% 297/3300 [45:29\u003c4:46:20,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:47:27,087 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-03 12:47:27,089 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:47:27,508 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:47:27,509 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:47:28,371 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1731, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [46:49\u003c4:42:09,  5.66s/it][INFO|trainer.py:3213] 2023-10-03 12:48:47,153 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:48:47,154 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:48:47,154 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6687086224555969, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.7991, 'eval_samples_per_second': 34.713, 'eval_steps_per_second': 0.144, 'epoch': 28.0}\n","  9% 308/3300 [47:10\u003c4:42:09,  5.66s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:49:07,955 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-03 12:49:07,957 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:49:08,375 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:49:08,376 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:49:09,240 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [48:30\u003c4:42:03,  5.68s/it][INFO|trainer.py:3213] 2023-10-03 12:50:27,892 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:50:27,892 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:50:27,893 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6905797123908997, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.7655, 'eval_samples_per_second': 34.769, 'eval_steps_per_second': 0.144, 'epoch': 29.0}\n"," 10% 319/3300 [48:51\u003c4:42:03,  5.68s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:50:48,659 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-03 12:50:48,661 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:50:49,078 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:50:49,079 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:50:49,945 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-297] due to args.save_total_limit\n"," 10% 330/3300 [50:11\u003c4:41:29,  5.69s/it][INFO|trainer.py:3213] 2023-10-03 12:52:08,612 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:52:08,612 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:52:08,612 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.88s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5962691307067871, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 21.0753, 'eval_samples_per_second': 34.258, 'eval_steps_per_second': 0.142, 'epoch': 30.0}\n"," 10% 330/3300 [50:32\u003c4:41:29,  5.69s/it]\n","100% 3/3 [00:06\u003c00:00,  1.88s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:52:29,689 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-330\n","[INFO|configuration_utils.py:460] 2023-10-03 12:52:29,690 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-330/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:52:30,139 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-330/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:52:30,139 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-330/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:52:31,013 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-308] due to args.save_total_limit\n"," 10% 341/3300 [51:51\u003c4:29:37,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 12:53:49,496 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:53:49,496 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:53:49,496 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7527464032173157, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 21.1067, 'eval_samples_per_second': 34.207, 'eval_steps_per_second': 0.142, 'epoch': 31.0}\n"," 10% 341/3300 [52:13\u003c4:29:37,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:54:10,604 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-341\n","[INFO|configuration_utils.py:460] 2023-10-03 12:54:10,605 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-341/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:54:11,017 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-341/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:54:11,018 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-341/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:54:11,871 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-319] due to args.save_total_limit\n"," 11% 352/3300 [53:33\u003c4:41:16,  5.72s/it][INFO|trainer.py:3213] 2023-10-03 12:55:30,973 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:55:30,973 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:55:30,974 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6737691164016724, 'eval_accuracy': 0.8254847645429363, 'eval_runtime': 20.8127, 'eval_samples_per_second': 34.69, 'eval_steps_per_second': 0.144, 'epoch': 32.0}\n"," 11% 352/3300 [53:54\u003c4:41:16,  5.72s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:55:51,788 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-352\n","[INFO|configuration_utils.py:460] 2023-10-03 12:55:51,789 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-352/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:55:52,224 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-352/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:55:52,225 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-352/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:55:53,143 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-330] due to args.save_total_limit\n"," 11% 363/3300 [55:13\u003c4:29:47,  5.51s/it][INFO|trainer.py:3213] 2023-10-03 12:57:11,039 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:57:11,039 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:57:11,039 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.89s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7334277033805847, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.9088, 'eval_samples_per_second': 34.531, 'eval_steps_per_second': 0.143, 'epoch': 33.0}\n"," 11% 363/3300 [55:34\u003c4:29:47,  5.51s/it]\n","100% 3/3 [00:06\u003c00:00,  1.88s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 12:57:31,949 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-363\n","[INFO|configuration_utils.py:460] 2023-10-03 12:57:31,951 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-363/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:57:32,370 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-363/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:57:32,371 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-363/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 12:57:33,205 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-341] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 12:57:33,327 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 12:57:33,328 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/checkpoint-253 (score: 0.5591697692871094).\n","{'train_runtime': 3336.0203, 'train_samples_per_second': 251.437, 'train_steps_per_second': 0.989, 'train_loss': 0.3052486177975153, 'epoch': 33.0}\n"," 11% 363/3300 [55:36\u003c7:29:51,  9.19s/it]\n","[INFO|trainer.py:2939] 2023-10-03 12:57:33,514 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/\n","[INFO|configuration_utils.py:460] 2023-10-03 12:57:33,515 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 12:57:34,073 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 12:57:34,074 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       33.0\n","  train_loss               =     0.3052\n","  train_runtime            = 0:55:36.02\n","  train_samples_per_second =    251.437\n","  train_steps_per_second   =      0.989\n","[INFO|trainer.py:3213] 2023-10-03 12:57:34,077 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:57:34,078 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:57:34,078 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.02s/it]\n","***** eval metrics *****\n","  epoch                   =       33.0\n","  eval_accuracy           =     0.8227\n","  eval_loss               =     0.5592\n","  eval_runtime            = 0:00:20.73\n","  eval_samples_per_second =     34.822\n","  eval_steps_per_second   =      0.145\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▆▇▇▇▇▇▇█▆█████▇▇█▇██▇▇▇█▇▇█▇█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▄▃▂▂▂▂▂▂▁▂▁▂▂▁▂▂▂▂▂▂▁▂▂▂▂▂▂▁▃▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▃▂▁▁▁▂▁█▁▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▃▃▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▆▇███▇█▁█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▆▆▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▆▇███▇█▁█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.82271\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.55917\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.7338\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.822\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.145\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 33.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 363\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1731\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 7.152850570235167e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.30525\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 3336.0203\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 251.437\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.989\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwhole-shape-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/upcltc93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_120149-upcltc93/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_9/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 9 \\\n","    --seed 9 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLPgVSaRFd--","outputId":"e6991505-35df-4473-bf1d-07de0112f055"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 12:58:09.378286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_125813-vtm2fyk7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnoble-shape-10\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/vtm2fyk7\u001b[0m\n","10/03/2023 12:58:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 12:58:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=10,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/runs/Oct03_12-58-14_2b3667363777,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=10,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 59331.28it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 267461.79it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 12:58:18,377 \u003e\u003e loading configuration file vit-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 12:58:18,379 \u003e\u003e Model config ViTConfig {\n","  \"_name_or_path\": \"vit-model/\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"problem_type\": \"single_label_classification\",\n","  \"qkv_bias\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 12:58:18,381 \u003e\u003e loading weights file vit-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 12:58:19,296 \u003e\u003e All model checkpoint weights were used when initializing ViTForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 12:58:19,296 \u003e\u003e Some weights of ViTForImageClassification were not initialized from the model checkpoint at vit-model/ and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([102, 768]) in the checkpoint and torch.Size([46, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 12:58:19,311 \u003e\u003e loading configuration file vit-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 12:58:19,313 \u003e\u003e Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 12:58:21,247 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 12:58:21,248 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 12:58:21,248 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 12:58:21,248 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 12:58:21,248 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 12:58:21,248 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 12:58:21,248 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 12:58:21,249 \u003e\u003e   Number of trainable parameters = 85,834,030\n","[INFO|integration_utils.py:722] 2023-10-03 12:58:21,250 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:23\u003c5:07:25,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 12:59:45,171 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 12:59:45,171 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 12:59:45,171 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4063942432403564, 'eval_accuracy': 0.6232686980609419, 'eval_runtime': 21.3203, 'eval_samples_per_second': 33.865, 'eval_steps_per_second': 0.141, 'epoch': 1.0}\n","  0% 11/3300 [01:45\u003c5:07:25,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:00:06,493 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 13:00:06,494 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:00:06,900 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:00:06,901 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [03:04\u003c5:03:59,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 13:01:25,932 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:01:25,932 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:01:25,932 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0528830289840698, 'eval_accuracy': 0.7119113573407202, 'eval_runtime': 20.8361, 'eval_samples_per_second': 34.651, 'eval_steps_per_second': 0.144, 'epoch': 2.0}\n","  1% 22/3300 [03:25\u003c5:03:59,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:01:46,770 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 13:01:46,771 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:01:47,184 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:01:47,185 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [04:47\u003c5:13:14,  5.75s/it][INFO|trainer.py:3213] 2023-10-03 13:03:08,821 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:03:08,822 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:03:08,822 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8540506362915039, 'eval_accuracy': 0.7576177285318559, 'eval_runtime': 20.8663, 'eval_samples_per_second': 34.601, 'eval_steps_per_second': 0.144, 'epoch': 3.0}\n","  1% 33/3300 [05:08\u003c5:13:14,  5.75s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:03:29,690 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 13:03:29,692 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:03:30,102 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:03:30,103 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [06:28\u003c5:03:59,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 13:04:49,506 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:04:49,506 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:04:49,506 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7216716408729553, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 21.0748, 'eval_samples_per_second': 34.259, 'eval_steps_per_second': 0.142, 'epoch': 4.0}\n","  1% 44/3300 [06:49\u003c5:03:59,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:05:10,583 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 13:05:10,584 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:05:11,010 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:05:11,011 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:05:11,898 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [08:09\u003c5:00:37,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 13:06:30,297 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:06:30,297 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:06:30,297 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6717398166656494, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 20.7954, 'eval_samples_per_second': 34.719, 'eval_steps_per_second': 0.144, 'epoch': 5.0}\n","  2% 55/3300 [08:29\u003c5:00:37,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:06:51,094 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 13:06:51,096 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:06:51,515 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:06:51,516 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:06:52,391 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [09:49\u003c5:12:57,  5.81s/it][INFO|trainer.py:3213] 2023-10-03 13:08:11,111 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:08:11,111 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:08:11,111 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7896137833595276, 'eval_accuracy': 0.7659279778393352, 'eval_runtime': 20.8749, 'eval_samples_per_second': 34.587, 'eval_steps_per_second': 0.144, 'epoch': 6.0}\n","  2% 66/3300 [10:10\u003c5:12:57,  5.81s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:08:31,988 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 13:08:31,990 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:08:32,392 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:08:32,394 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:08:33,236 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [11:30\u003c4:59:55,  5.58s/it][INFO|trainer.py:3213] 2023-10-03 13:09:51,697 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:09:51,697 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:09:51,698 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6619874835014343, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.7361, 'eval_samples_per_second': 34.818, 'eval_steps_per_second': 0.145, 'epoch': 7.0}\n","  2% 77/3300 [11:51\u003c4:59:55,  5.58s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:10:12,435 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 13:10:12,436 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:10:12,849 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:10:12,850 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:10:13,688 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [13:10\u003c5:00:59,  5.62s/it][INFO|trainer.py:3213] 2023-10-03 13:11:32,140 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:11:32,141 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:11:32,141 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7051557302474976, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 20.767, 'eval_samples_per_second': 34.767, 'eval_steps_per_second': 0.144, 'epoch': 8.0}\n","  3% 88/3300 [13:31\u003c5:00:59,  5.62s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:11:52,909 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 13:11:52,910 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:11:53,313 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:11:53,314 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:11:54,144 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [14:51\u003c4:59:31,  5.61s/it][INFO|trainer.py:3213] 2023-10-03 13:13:12,468 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:13:12,468 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:13:12,468 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6518068313598633, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 21.0167, 'eval_samples_per_second': 34.354, 'eval_steps_per_second': 0.143, 'epoch': 9.0}\n","  3% 99/3300 [15:12\u003c4:59:31,  5.61s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:13:33,486 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 13:13:33,488 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:13:33,920 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:13:33,921 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:13:34,793 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6482, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [16:32\u003c5:08:31,  5.80s/it][INFO|trainer.py:3213] 2023-10-03 13:14:54,061 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:14:54,062 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:14:54,062 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.85s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6202144026756287, 'eval_accuracy': 0.8254847645429363, 'eval_runtime': 20.9678, 'eval_samples_per_second': 34.434, 'eval_steps_per_second': 0.143, 'epoch': 10.0}\n","  3% 110/3300 [16:53\u003c5:08:31,  5.80s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:15:15,031 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 13:15:15,033 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:15:15,449 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:15:15,450 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:15:16,305 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [18:13\u003c4:56:47,  5.60s/it][INFO|trainer.py:3213] 2023-10-03 13:16:34,904 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:16:34,905 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:16:34,905 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6521174907684326, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 20.9002, 'eval_samples_per_second': 34.545, 'eval_steps_per_second': 0.144, 'epoch': 11.0}\n","  4% 121/3300 [18:34\u003c4:56:47,  5.60s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:16:55,807 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 13:16:55,808 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:16:56,227 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:16:56,228 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:16:57,078 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [19:54\u003c4:52:44,  5.54s/it][INFO|trainer.py:3213] 2023-10-03 13:18:15,564 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:18:15,564 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:18:15,564 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.649881899356842, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.8323, 'eval_samples_per_second': 34.658, 'eval_steps_per_second': 0.144, 'epoch': 12.0}\n","  4% 132/3300 [20:15\u003c4:52:44,  5.54s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:18:36,398 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 13:18:36,399 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:18:36,817 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:18:36,818 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:18:37,668 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [21:34\u003c4:47:36,  5.47s/it][INFO|trainer.py:3213] 2023-10-03 13:19:55,741 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:19:55,741 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:19:55,741 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7469365000724792, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.853, 'eval_samples_per_second': 34.623, 'eval_steps_per_second': 0.144, 'epoch': 13.0}\n","  4% 143/3300 [21:55\u003c4:47:36,  5.47s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:20:16,596 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 13:20:16,598 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:20:17,009 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:20:17,010 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:20:17,875 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [23:15\u003c4:59:07,  5.70s/it][INFO|trainer.py:3213] 2023-10-03 13:21:36,507 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:21:36,507 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:21:36,507 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.640190839767456, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.7762, 'eval_samples_per_second': 34.751, 'eval_steps_per_second': 0.144, 'epoch': 14.0}\n","  5% 154/3300 [23:36\u003c4:59:07,  5.70s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:21:57,285 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 13:21:57,286 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:21:57,695 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:21:57,696 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:21:58,566 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [24:55\u003c4:59:42,  5.74s/it][INFO|trainer.py:3213] 2023-10-03 13:23:17,104 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:23:17,105 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:23:17,105 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.90s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6312741637229919, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 21.0045, 'eval_samples_per_second': 34.374, 'eval_steps_per_second': 0.143, 'epoch': 15.0}\n","  5% 165/3300 [25:16\u003c4:59:42,  5.74s/it]\n","100% 3/3 [00:06\u003c00:00,  1.89s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:23:38,111 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 13:23:38,113 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:23:38,530 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:23:38,531 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:23:39,410 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [26:36\u003c4:55:02,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 13:24:57,817 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:24:57,817 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:24:57,817 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6289490461349487, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.8077, 'eval_samples_per_second': 34.699, 'eval_steps_per_second': 0.144, 'epoch': 16.0}\n","  5% 176/3300 [26:57\u003c4:55:02,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:25:18,626 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 13:25:18,627 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:25:19,037 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:25:19,038 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:25:19,880 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [28:17\u003c4:59:05,  5.76s/it][INFO|trainer.py:3213] 2023-10-03 13:26:38,450 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:26:38,450 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:26:38,450 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6236777901649475, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 20.7285, 'eval_samples_per_second': 34.831, 'eval_steps_per_second': 0.145, 'epoch': 17.0}\n","  6% 187/3300 [28:37\u003c4:59:05,  5.76s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:26:59,180 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 13:26:59,181 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:26:59,591 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:26:59,592 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:27:00,452 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [29:58\u003c4:56:51,  5.74s/it][INFO|trainer.py:3213] 2023-10-03 13:28:19,352 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:28:19,352 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:28:19,353 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.82s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5974421501159668, 'eval_accuracy': 0.8379501385041551, 'eval_runtime': 20.8765, 'eval_samples_per_second': 34.584, 'eval_steps_per_second': 0.144, 'epoch': 18.0}\n","  6% 198/3300 [30:18\u003c4:56:51,  5.74s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:28:40,230 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 13:28:40,232 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:28:40,651 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:28:40,652 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:28:41,522 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-110] due to args.save_total_limit\n","{'loss': 0.2005, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [31:38\u003c4:45:55,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 13:30:00,166 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:30:00,166 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:30:00,166 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.86s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6080483794212341, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 20.9944, 'eval_samples_per_second': 34.39, 'eval_steps_per_second': 0.143, 'epoch': 19.0}\n","  6% 209/3300 [31:59\u003c4:45:55,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.86s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:30:21,162 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 13:30:21,163 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:30:21,576 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:30:21,577 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:30:22,433 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-176] due to args.save_total_limit\n","  7% 220/3300 [33:19\u003c4:58:29,  5.81s/it][INFO|trainer.py:3213] 2023-10-03 13:31:41,150 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:31:41,150 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:31:41,151 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.653435230255127, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 20.9791, 'eval_samples_per_second': 34.415, 'eval_steps_per_second': 0.143, 'epoch': 20.0}\n","  7% 220/3300 [33:40\u003c4:58:29,  5.81s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:32:02,132 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 13:32:02,133 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:32:02,576 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:32:02,577 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:32:03,489 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [35:01\u003c4:46:02,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 13:33:22,383 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:33:22,384 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:33:22,384 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6519385576248169, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 20.7661, 'eval_samples_per_second': 34.768, 'eval_steps_per_second': 0.144, 'epoch': 21.0}\n","  7% 231/3300 [35:21\u003c4:46:02,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:33:43,152 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 13:33:43,153 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:33:43,566 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:33:43,567 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:33:44,423 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [36:41\u003c4:48:58,  5.67s/it][INFO|trainer.py:3213] 2023-10-03 13:35:02,585 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:35:02,585 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:35:02,585 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.81s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6343134641647339, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 20.8057, 'eval_samples_per_second': 34.702, 'eval_steps_per_second': 0.144, 'epoch': 22.0}\n","  7% 242/3300 [37:02\u003c4:48:58,  5.67s/it]\n","100% 3/3 [00:06\u003c00:00,  1.84s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:35:23,392 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 13:35:23,394 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:35:23,804 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:35:23,805 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:35:24,658 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [38:22\u003c4:41:56,  5.55s/it][INFO|trainer.py:3213] 2023-10-03 13:36:43,571 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:36:43,571 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:36:43,572 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6380999088287354, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 20.7653, 'eval_samples_per_second': 34.77, 'eval_steps_per_second': 0.144, 'epoch': 23.0}\n","  8% 253/3300 [38:43\u003c4:41:56,  5.55s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:37:04,338 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 13:37:04,340 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:37:04,748 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:37:04,749 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:37:05,580 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [40:02\u003c4:41:31,  5.56s/it][INFO|trainer.py:3213] 2023-10-03 13:38:24,212 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:38:24,213 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:38:24,213 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6859850287437439, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 21.0098, 'eval_samples_per_second': 34.365, 'eval_steps_per_second': 0.143, 'epoch': 24.0}\n","  8% 264/3300 [40:23\u003c4:41:31,  5.56s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:38:45,227 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-03 13:38:45,229 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:38:45,674 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:38:45,675 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:38:46,579 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [41:43\u003c4:50:54,  5.77s/it][INFO|trainer.py:3213] 2023-10-03 13:40:04,877 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:40:04,878 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:40:04,878 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.83s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6766029596328735, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 20.8402, 'eval_samples_per_second': 34.645, 'eval_steps_per_second': 0.144, 'epoch': 25.0}\n","  8% 275/3300 [42:04\u003c4:50:54,  5.77s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:40:25,720 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-03 13:40:25,721 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:40:26,139 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:40:26,140 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:40:27,009 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [43:23\u003c4:34:02,  5.46s/it][INFO|trainer.py:3213] 2023-10-03 13:41:45,112 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:41:45,112 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:41:45,112 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.84s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7148101329803467, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 20.8765, 'eval_samples_per_second': 34.584, 'eval_steps_per_second': 0.144, 'epoch': 26.0}\n","  9% 286/3300 [43:44\u003c4:34:02,  5.46s/it]\n","100% 3/3 [00:06\u003c00:00,  1.85s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:42:05,990 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-03 13:42:05,992 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:42:06,402 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:42:06,403 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:42:07,273 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [45:05\u003c4:36:57,  5.53s/it][INFO|trainer.py:3213] 2023-10-03 13:43:26,561 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:43:26,562 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:43:26,562 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.80s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7101728916168213, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 20.7125, 'eval_samples_per_second': 34.858, 'eval_steps_per_second': 0.145, 'epoch': 27.0}\n","  9% 297/3300 [45:26\u003c4:36:57,  5.53s/it]\n","100% 3/3 [00:06\u003c00:00,  1.83s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:43:47,276 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-03 13:43:47,278 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:43:47,687 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:43:47,688 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:43:48,547 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1695, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [46:45\u003c4:38:37,  5.59s/it][INFO|trainer.py:3213] 2023-10-03 13:45:06,487 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:45:06,488 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:45:06,488 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n"," 67% 2/3 [00:05\u003c00:02,  2.88s/it]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.702583909034729, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 21.0178, 'eval_samples_per_second': 34.352, 'eval_steps_per_second': 0.143, 'epoch': 28.0}\n","  9% 308/3300 [47:06\u003c4:38:37,  5.59s/it]\n","100% 3/3 [00:06\u003c00:00,  1.87s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 13:45:27,507 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-03 13:45:27,509 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:45:27,937 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:45:27,938 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 13:45:28,821 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-286] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 13:45:28,944 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 13:45:28,945 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/checkpoint-198 (score: 0.5974421501159668).\n","{'train_runtime': 2827.878, 'train_samples_per_second': 296.618, 'train_steps_per_second': 1.167, 'train_loss': 0.334974247913856, 'epoch': 28.0}\n","  9% 308/3300 [47:07\u003c7:37:50,  9.18s/it]\n","[INFO|trainer.py:2939] 2023-10-03 13:45:29,129 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/\n","[INFO|configuration_utils.py:460] 2023-10-03 13:45:29,131 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 13:45:29,662 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 13:45:29,663 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       28.0\n","  train_loss               =      0.335\n","  train_runtime            = 0:47:07.87\n","  train_samples_per_second =    296.618\n","  train_steps_per_second   =      1.167\n","[INFO|trainer.py:3213] 2023-10-03 13:45:29,667 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 13:45:29,668 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 13:45:29,668 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:06\u003c00:00,  2.05s/it]\n","***** eval metrics *****\n","  epoch                   =       28.0\n","  eval_accuracy           =      0.838\n","  eval_loss               =     0.5974\n","  eval_runtime            = 0:00:20.99\n","  eval_samples_per_second =     34.385\n","  eval_steps_per_second   =      0.143\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▅▇▇▆▇▆▇█▆▇▇▇▇▇▇██▇▇▇█▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▂▂▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▂▃▅▂▃▁▂▅▄▃▂▃▂▄▂▁▃▄▄▂▂▂▄▂▃▁▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▇▆▄▇▆█▇▄▅▆▇▆▇▅▇█▆▅▅▇▇▇▅▆▆█▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆▆▃▆▆█▆▅▅▆▆▆▆▅▆█▆▅▅▆▆▆▅▆▆█▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.83795\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.59744\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.9974\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 34.385\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.143\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 28.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 308\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1695\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 6.069085332320748e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.33497\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2827.878\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 296.618\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.167\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mnoble-shape-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/vitIp102Fabi/runs/vtm2fyk7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_125813-vtm2fyk7/logs\u001b[0m\n"]}],"source":["!python vit.py \\\n","    --model_name vit-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/vit/ip102fabi/fabiPandD_outputs_10/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 10 \\\n","    --seed 10 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZ68hh4xKPEq"},"outputs":[],"source":["# End run\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","provenance":[{"file_id":"1QsQGzJ9Vves722mz4mXcexMPUDXSgsed","timestamp":1694534744320},{"file_id":"17Xhtplfpdqdz1TwipHI60aqky1CvSd1x","timestamp":1693214691473}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}