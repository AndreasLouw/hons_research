{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55130,"status":"ok","timestamp":1696278156801,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"sMTacwHyZP-0","outputId":"84c2ba79-b776-4968-9627-00ed497ba4ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (2.1.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchvision) (2023.7.22)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch) (1.3.0)\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill\u003c0.3.8,\u003e=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]\u003c2023.9.0,\u003e=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Collecting huggingface-hub\u003c1.0.0,\u003e=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Collecting responses\u003c0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,\u003e=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk\u003e=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds\u003e=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs\u003e=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,\u003c5,\u003e=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six\u003e=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds\u003e=0.4.0-\u003ewandb) (1.16.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (3.2.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.3)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.4.0)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Collecting gitdb\u003c5,\u003e=4.0.1 (from GitPython!=3.1.29,\u003e=1.0.0-\u003ewandb)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (3.12.2)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.14.0-\u003edatasets) (4.5.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2023.3.post1)\n","Collecting smmap\u003c6,\u003e=3.0.1 (from gitdb\u003c5,\u003e=4.0.1-\u003eGitPython!=3.1.29,\u003e=1.0.0-\u003ewandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=eb61a07ffd5449d769f61f49b8d0da55eab374f901ebe65a568443202702a914\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built pathtools\n","Installing collected packages: pathtools, xxhash, smmap, setproctitle, sentry-sdk, docker-pycreds, dill, responses, multiprocess, huggingface-hub, gitdb, GitPython, wandb, datasets, evaluate\n","Successfully installed GitPython-3.1.37 datasets-2.14.5 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.0 gitdb-4.0.10 huggingface-hub-0.17.3 multiprocess-0.70.15 pathtools-0.1.2 responses-0.18.0 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.1 wandb-0.15.11 xxhash-3.3.0\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-f6u8t6ix\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-f6u8t6ix\n","  Resolved https://github.com/huggingface/transformers to commit 9ed538f2e67ee10323d96c97284cf83d44f0c507\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (3.12.2)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.17.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2.31.0)\n","Collecting tokenizers\u003c0.15,\u003e=0.14 (from transformers==4.34.0.dev0)\n","  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors\u003e=0.3.1 (from transformers==4.34.0.dev0)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.16.4-\u003etransformers==4.34.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.16.4-\u003etransformers==4.34.0.dev0) (4.5.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.16.4 (from transformers==4.34.0.dev0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.34.0.dev0) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.34.0.dev0) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.34.0.dev0) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.34.0.dev0) (2023.7.22)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7745541 sha256=2c99a9f7ad14741a62d15c7dc2b3ee468dccdf14382d56bd773baa336c97e899\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qxeuvvss/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n","Successfully built transformers\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.17.3\n","    Uninstalling huggingface-hub-0.17.3:\n","      Successfully uninstalled huggingface-hub-0.17.3\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0.dev0\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.11)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.37)\n","Requirement already satisfied: requests\u003c3,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.31.0)\n","Requirement already satisfied: docker-pycreds\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs\u003e=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,\u003c5,\u003e=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six\u003e=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds\u003e=0.4.0-\u003ewandb) (1.16.0)\n","Requirement already satisfied: gitdb\u003c5,\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,\u003e=1.0.0-\u003ewandb) (4.0.10)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2023.7.22)\n","Requirement already satisfied: smmap\u003c6,\u003e=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb\u003c5,\u003e=4.0.1-\u003eGitPython!=3.1.29,\u003e=1.0.0-\u003ewandb) (5.0.1)\n","Collecting accelerate\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch\u003e=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.10.0-\u003eaccelerate) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.10.0-\u003eaccelerate) (16.0.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.10.0-\u003eaccelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (2023.7.22)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.10.0-\u003eaccelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.23.0\n"]}],"source":["!pip install torch torchvision\n","!pip install datasets evaluate wandb\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install wandb\n","!pip install accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39954,"status":"ok","timestamp":1696278196747,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"xu5wBqMkkf62","outputId":"c3af6c5c-4daf-43d4-c789-f28eb969bf76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68424,"status":"ok","timestamp":1696278265164,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"SDb4tIi39i1u","outputId":"1715d336-18b6-4f4e-af6b-66f01149bdf8"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-10-02 20:23:16--  https://docs.google.com/uc?export=download\u0026confirm=t\u0026id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF\n","Resolving docs.google.com (docs.google.com)... 74.125.130.138, 74.125.130.139, 74.125.130.100, ...\n","Connecting to docs.google.com (docs.google.com)|74.125.130.138|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-04-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fmrn7o0iot0lcfkoe2ctt1p1ea0fovsp/1696278150000/11183032721846533402/*/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF?e=download\u0026uuid=6f22fc6d-7038-46b8-962f-b16676a04b27 [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-10-02 20:23:16--  https://doc-04-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fmrn7o0iot0lcfkoe2ctt1p1ea0fovsp/1696278150000/11183032721846533402/*/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF?e=download\u0026uuid=6f22fc6d-7038-46b8-962f-b16676a04b27\n","Resolving doc-04-bk-docs.googleusercontent.com (doc-04-bk-docs.googleusercontent.com)... 74.125.130.132, 2404:6800:4003:c01::84\n","Connecting to doc-04-bk-docs.googleusercontent.com (doc-04-bk-docs.googleusercontent.com)|74.125.130.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2246169376 (2.1G) [application/x-zip-compressed]\n","Saving to: ‘fabi-data.zip’\n","\n","fabi-data.zip       100%[===================\u003e]   2.09G  46.8MB/s    in 48s     \n","\n","2023-10-02 20:24:05 (44.4 MB/s) - ‘fabi-data.zip’ saved [2246169376/2246169376]\n","\n","Archive:  fabi-data.zip\n","   creating: data/fabi-data/test/\n","   creating: data/fabi-data/test/0/\n","  inflating: data/fabi-data/test/0/Anaphes nitens 1 Female on Gonipterus egg capsule.jpg  \n","   creating: data/fabi-data/test/1/\n","  inflating: data/fabi-data/test/1/SBush_Blastopsylla occidentalis adult 3.jpg  \n","  inflating: data/fabi-data/test/1/SBush_Blastopsylla occidentalis adult2.jpg  \n","   creating: data/fabi-data/test/10/\n","  inflating: data/fabi-data/test/10/Heavily infested lower branches.jpeg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus 1a.jpg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus 1b.jpg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus galls Selitrichodes female 1a.jpg  \n","   creating: data/fabi-data/test/11/\n","  inflating: data/fabi-data/test/11/DSC_9298.NEF  \n","  inflating: data/fabi-data/test/11/DSC_9300.jpg  \n","  inflating: data/fabi-data/test/11/IMG_3426.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3429.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3469.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3481.JPG  \n","   creating: data/fabi-data/test/12/\n","  inflating: data/fabi-data/test/12/145F5006-CF53-4BFC-81DC-F1EC372DB56D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/1714C5B0-4599-4DC9-BB9A-FB6C2FD8A7CF_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/2DFA84C6-5FB3-431B-B6B2-56A4FC8F4C52_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/322C9D7E-E06B-44B9-B7AE-E36993B32062_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/4C6FB127-A670-40B6-BEF5-A4451AFA5521_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/593AF0C5-A2FA-4062-A271-B5A168B1D497_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/6866D250-37C9-4478-845F-B32662F4028E_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/7C4CA639-5682-49AA-8E90-47BE10C8621C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/C8B432D2-317E-4B5A-A020-F0182775ABEB_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/F05178E4-EAE9-4C5D-96AC-D72451666010_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/F233A982-D15C-4E02-B366-23D6CC03F3BA_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/FFCB3191-F78F-489C-B098-6E356E695FAE_1_105_c.jpeg  \n","   creating: data/fabi-data/test/13/\n","  inflating: data/fabi-data/test/13/Male F1 3 April 2015 resized.jpg  \n","  inflating: data/fabi-data/test/13/Male F1 April 2015.jpg  \n","  inflating: data/fabi-data/test/13/Male mystery wasp 8 Oct 2013.jpg  \n","  inflating: data/fabi-data/test/13/Male mystery wasp 9 Oct2013.jpg  \n","  inflating: data/fabi-data/test/13/Mummy April 2015 resized.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Female Oct 2013 5b.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 10.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 11.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 9.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 1.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 4.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 6.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Parasitised nymph 1.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus pupae underneath 3a.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus two parasitised nymphs 4a.jpg  \n","  inflating: data/fabi-data/test/13/Pb possibly hyper.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02b.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02c adjusted.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02e.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr03.jpg  \n","  inflating: data/fabi-data/test/13/SpObsSA05c.jpg  \n","  inflating: data/fabi-data/test/13/SpObsSA09 1.tif  \n","  inflating: data/fabi-data/test/13/SpObsSA09 3.tif  \n","  inflating: data/fabi-data/test/13/SpObsSA09 4.tif  \n","   creating: data/fabi-data/test/14/\n","  inflating: data/fabi-data/test/14/Possibly Quadristichus resized for email.jpg  \n","  inflating: data/fabi-data/test/14/Qmendeli.jpg  \n","  inflating: data/fabi-data/test/14/Qmendeli7.jpg  \n","   creating: data/fabi-data/test/15/\n","  inflating: data/fabi-data/test/15/Selitrichodes female 2.tif  \n","  inflating: data/fabi-data/test/15/Selitrichodes female 7a.jpg  \n","   creating: data/fabi-data/test/16/\n","  inflating: data/fabi-data/test/16/Image_122b.jpg  \n","  inflating: data/fabi-data/test/16/Image_126b.jpg  \n","   creating: data/fabi-data/test/17/\n","  inflating: data/fabi-data/test/17/Close up of browned larvae.jpg  \n","  inflating: data/fabi-data/test/17/Close up of strange eggs.jpg  \n","  inflating: data/fabi-data/test/17/DSCN0198.JPG  \n","  inflating: data/fabi-data/test/17/DSCN1806.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7030.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7031.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7476.JPG  \n","  inflating: data/fabi-data/test/17/Larvae1 no4.jpg  \n","  inflating: data/fabi-data/test/17/Larvae3 no2.jpg  \n","  inflating: data/fabi-data/test/17/Scarred Sirex Larvae 1.jpg  \n","  inflating: data/fabi-data/test/17/Sirex larvae scarred head.jpg  \n","  inflating: data/fabi-data/test/17/Sirex scarred tail 2.jpg  \n","  inflating: data/fabi-data/test/17/Sirex scarred tail.jpg  \n","  inflating: data/fabi-data/test/17/Sirex with nematodes 2.jpg  \n","   creating: data/fabi-data/test/18/\n","  inflating: data/fabi-data/test/18/04EB584A-3B0E-45E4-8300-8FF93C2A455D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/2913384C-B94B-45C7-A87E-225B8873EEAD_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/3E9E2B2A-C617-4E08-B30D-E1C9DBC145D0_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/4DD2AC59-333B-4C2E-9C0B-E75D3765169A_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/6D67302D-F633-454A-AD60-A2B272BBB497_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/7D5BE38F-F614-4A70-97C8-1BAB81289AA1_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/7F257B7B-0B44-4A78-A4F2-2CB1E8799183_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/Adult Middelburg Spondyliaspis wing different angle.jpg  \n","  inflating: data/fabi-data/test/18/Adult Middelburg Spondyliaspis.jpg  \n","  inflating: data/fabi-data/test/18/AFFB4796-1560-4A3B-B72F-E204255C0A85_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/BB7DD3C2-0CB0-45E9-BB24-0DB2DEFCB119_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/Damage 2.jpg  \n","  inflating: data/fabi-data/test/18/E11EB6E7-2C6E-4900-AC81-5AB8E745324D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002212.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002214.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002217.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002218.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002223.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002229.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002231.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002234.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002236.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002237.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002239.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002241.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002245.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002254.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002265.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002271.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002282.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002283.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002286.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002289.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002293.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002295.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002300.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002302.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002307.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002317.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002319.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002335.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002345.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002346.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002348.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002358.tif  \n","  inflating: data/fabi-data/test/18/none-0000002384.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002391.jpg  \n","  inflating: data/fabi-data/test/18/Spondyliaspis Adult2.jpg  \n","  inflating: data/fabi-data/test/18/Stack3A.jpg  \n","  inflating: data/fabi-data/test/18/StackC.jpg  \n","   creating: data/fabi-data/test/19/\n","  inflating: data/fabi-data/test/19/DSCN0820.JPG  \n","  inflating: data/fabi-data/test/19/DSCN5459.JPG  \n","  inflating: data/fabi-data/test/19/eggs7.jpg  \n","  inflating: data/fabi-data/test/19/IMG_0582.JPG  \n","  inflating: data/fabi-data/test/19/IMG_0638.JPG  \n","  inflating: data/fabi-data/test/19/none-0000001397.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001398.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001399.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001401.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001405.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001425.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001433.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001434.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001436.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001438.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001440.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001452.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001459.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001461.jpeg  \n","  inflating: data/fabi-data/test/19/none-0000001468.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001469.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001472.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001483.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001487.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris female 1.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Male 2.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Male 4.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Samantha Bush.jpg  \n","   creating: data/fabi-data/test/2/\n","  inflating: data/fabi-data/test/2/SBush_Cleruchoides noackae_ovipositing into Thaumastocoris peregrinus eggs2.jpg  \n","  inflating: data/fabi-data/test/2/SBush_Cleruchoides noackae_with Thaumastocoris peregrinus eggs1.jpg  \n","   creating: data/fabi-data/test/20/\n","  inflating: data/fabi-data/test/20/DSCN1701.JPG  \n","  inflating: data/fabi-data/test/20/DSCN1705.JPG  \n","  inflating: data/fabi-data/test/20/IMG_2308.JPG  \n","   creating: data/fabi-data/test/21/\n","  inflating: data/fabi-data/test/21/311C3CAF-DAD3-4027-BF1E-5C96BCD5A648_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/21/CA4A546E-F2D4-4A2E-B530-A33605E87097_1_105_c.jpeg  \n","   creating: data/fabi-data/test/22/\n","  inflating: data/fabi-data/test/22/Armillaria GM Granados 03.JPG  \n","  inflating: data/fabi-data/test/22/Armillaria GM Granados 06.JPG  \n","  inflating: data/fabi-data/test/22/Armillaria mellea mushrooms Bavarian Germany 2009.jpg  \n","   creating: data/fabi-data/test/23/\n","  inflating: data/fabi-data/test/23/none-0000000728.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000730.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000732.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000736.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000742.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000747.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000760.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000769.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000770.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000776.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000777.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000779.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000781.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000784.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000792.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000796.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000803.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000805.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000809.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000810.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000812.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000813.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000817.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000834.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000845.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000851.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000858.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000860.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000862.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000876.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000893.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000895.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000916.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000917.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000929.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000932.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000939.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000940.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000944.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000947.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000953.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000956.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000957.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000967.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000975.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000984.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000991.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001001.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001002.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001012.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001015.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001017.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001024.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001025.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001029.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001040.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001050.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001051.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001054.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001065.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001073.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001076.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001078.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001080.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001087.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001091.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001093.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001097.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001104.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001106.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001117.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001120.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001127.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001131.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001135.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001136.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001145.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001146.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001150.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001152.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001154.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001155.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001158.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001159.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001160.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001162.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001163.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001170.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001179.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001183.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001185.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001195.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001203.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001209.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001210.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001213.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001214.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001218.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001224.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001238.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001241.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001246.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001248.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001259.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001266.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001267.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001268.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001269.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001270.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001276.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001281.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001282.jpg  \n","   creating: data/fabi-data/test/24/\n","  inflating: data/fabi-data/test/24/Bacterial blight cause by Pantoea ananatis.png  \n","  inflating: data/fabi-data/test/24/Bacterial blight Pantoea anantis_Izette.JPG  \n","   creating: data/fabi-data/test/25/\n","  inflating: data/fabi-data/test/25/Bacterial wilt caused by _i_Ralstonia solanacearum_i_.png  \n","   creating: data/fabi-data/test/26/\n","  inflating: data/fabi-data/test/26/Botryosphaeria canker internal Neofusicoccum spp_Izette.jpg  \n","   creating: data/fabi-data/test/27/\n","  inflating: data/fabi-data/test/27/Fig. 4. Fruiting structures with abundant mature conidia 3.jpg  \n","   creating: data/fabi-data/test/28/\n","  inflating: data/fabi-data/test/28/Calonectria symptoms on _i_Eucalyptus_i_ leaves.JPG  \n","   creating: data/fabi-data/test/29/\n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados01.JPG  \n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados14.JPG  \n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados17.JPG  \n","   creating: data/fabi-data/test/3/\n","  inflating: data/fabi-data/test/3/6A4F68D7-85B0-4F91-BA7B-E0D906B37970_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/6C6B7899-4F1D-472E-B48B-6BD804C37DA6_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/8531B8E1-3ACC-4EEB-99E1-21A44F1B7765_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/AEE41DD1-E10E-4E3E-B21F-91639A4F475C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/DSCN0361.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0363.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0377.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0803.JPG  \n","  inflating: data/fabi-data/test/3/DSCN1608.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4174.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4191.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4202.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5065.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5492.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5499.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5504.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5515.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5593.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5600.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5605.JPG  \n","  inflating: data/fabi-data/test/3/DSCN6967.JPG  \n","  inflating: data/fabi-data/test/3/DSCN6968.JPG  \n","  inflating: data/fabi-data/test/3/DSCN7320.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8134.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8137.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8146.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8149.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8150.JPG  \n","  inflating: data/fabi-data/test/3/FF36B45F-AC9F-44BD-BA00-4C670A18D626_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/IMG_0687.JPG  \n","  inflating: data/fabi-data/test/3/IMG_0699.JPG  \n","  inflating: data/fabi-data/test/3/IMG_0737.JPG  \n","   creating: data/fabi-data/test/30/\n","  inflating: data/fabi-data/test/30/T.zuluensis KZN ZG14 GM Granados 01.JPG  \n","  inflating: data/fabi-data/test/30/T.zuluensis KZN ZG14 GM Granados 06.JPG  \n","   creating: data/fabi-data/test/31/\n","  inflating: data/fabi-data/test/31/D.sapinea GM Granados 06.JPG  \n","  inflating: data/fabi-data/test/31/Diplodia5.JPG  \n","  inflating: data/fabi-data/test/31/Diplodia6.jpg  \n","   creating: data/fabi-data/test/32/\n","  inflating: data/fabi-data/test/32/Fusarium circinatum root rot.JPG  \n","   creating: data/fabi-data/test/33/\n","  inflating: data/fabi-data/test/33/Glycaspis 1.png  \n","   creating: data/fabi-data/test/34/\n","  inflating: data/fabi-data/test/34/_i_Eucalyptus smithii_i_ collar rot.JPG  \n","   creating: data/fabi-data/test/35/\n","  inflating: data/fabi-data/test/35/E.salmonicolor Podocarpus GM Granados 03.JPG  \n","   creating: data/fabi-data/test/36/\n","  inflating: data/fabi-data/test/36/F.circinatum GM Granados 03.JPG  \n","  inflating: data/fabi-data/test/36/F.circinatum GM Granados 05.JPG  \n","   creating: data/fabi-data/test/37/\n","  inflating: data/fabi-data/test/37/Powdery mildew GM Granados01.JPG  \n","   creating: data/fabi-data/test/38/\n","  inflating: data/fabi-data/test/38/Pseudophaeolus KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/test/38/Pseudophaeolus KZN GM Granados 05.JPG  \n","   creating: data/fabi-data/test/39/\n","  inflating: data/fabi-data/test/39/none-0000001288.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001302.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001304.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001312.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001316.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001319.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001320.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001324.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001328.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001330.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001331.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001339.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001347.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001351.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001358.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001363.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001366.jpg  \n","  inflating: data/fabi-data/test/39/Quambalaria GM Granados 01.JPG  \n","   creating: data/fabi-data/test/4/\n","  inflating: data/fabi-data/test/4/Ctenarytaina male 2 SBush.tif  \n","  inflating: data/fabi-data/test/4/Nymph 3 SBush.tif  \n","   creating: data/fabi-data/test/40/\n","  inflating: data/fabi-data/test/40/Fig.1c_FABInews.jpg  \n","   creating: data/fabi-data/test/41/\n","  inflating: data/fabi-data/test/41/Rhizina T Paap_IMG_5700.JPG  \n","   creating: data/fabi-data/test/42/\n","  inflating: data/fabi-data/test/42/Fig2_Conio.JPG  \n","   creating: data/fabi-data/test/43/\n","  inflating: data/fabi-data/test/43/none-0000000007.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000009.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000010.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000011.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000012.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000017.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000018.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000020.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000031.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000035.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000038.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000041.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000046.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000050.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000051.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000052.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000054.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000060.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000073.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000078.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000079.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000087.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000092.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000096.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000105.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000115.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000122.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000130.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000131.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000133.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000138.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000142.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000144.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000149.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000150.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000152.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000156.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000159.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000164.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000172.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000177.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000183.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000184.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000187.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000189.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000191.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000194.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000216.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000228.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000231.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000232.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000233.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000236.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000237.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000240.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000244.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000254.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000261.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000262.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000264.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000265.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000272.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000278.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000279.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000291.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000292.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000300.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000308.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000311.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000315.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000321.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000323.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000325.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000334.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000342.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000346.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000352.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000355.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000364.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000369.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000378.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000382.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000384.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000386.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000399.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000415.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000422.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000438.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000448.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000451.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000456.jpg  \n","  inflating: data/fabi-data/test/43/T.destructans KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/test/43/T.destructans KZN GM Granados 02.JPG  \n","   creating: data/fabi-data/test/44/\n","  inflating: data/fabi-data/test/44/none-0000000463.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000471.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000474.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000480.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000481.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000489.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000498.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000499.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000503.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000506.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000513.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000518.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000521.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000522.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000524.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000525.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000527.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000528.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000541.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000545.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000546.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000559.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000560.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000562.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000564.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000567.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000571.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000580.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000590.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000616.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000618.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000619.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000631.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000633.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000641.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000642.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000644.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000650.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000653.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000656.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000662.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000665.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000667.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000675.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000678.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000687.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000688.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000689.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000691.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000696.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000703.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000707.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000710.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000723.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000726.jpg  \n","   creating: data/fabi-data/test/45/\n","  inflating: data/fabi-data/test/45/Wattle rust KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/test/45/Wattle rust_Harding_KZN_GMGranados02.JPG  \n","  inflating: data/fabi-data/test/45/Wattle rust_Harding_KZN_GMGranados03.JPG  \n","   creating: data/fabi-data/test/5/\n","  inflating: data/fabi-data/test/5/Pupa and pupal casing.JPG  \n","   creating: data/fabi-data/test/6/\n","  inflating: data/fabi-data/test/6/16360465-C594-4220-9124-037C12A24735_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/2E3AECF5-9282-4CE9-80DC-D3F7064BB10B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/37324B65-6842-4318-9D81-15802953C399_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/45B43995-1E05-433E-B3B9-C7DACAA4C7CB_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/57834C65-77AB-47BD-A83D-FCC9AD90308B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/717C21AA-80AF-499E-8FEC-641DFD88CC01_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/934D8377-ACA2-42A6-8637-0691D4ED4869_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/Adult  male pic1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Adult Gb emerging 2 SBush.jpg  \n","  inflating: data/fabi-data/test/6/B8A1BEA8-679E-4DA3-AEF3-003D35786B9A_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/C7C2B96C-DD06-489C-B8A4-A3532F7C482B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/CD90717B-31F2-48E8-988C-8EB95C74CE13_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/CF9D1E33-7828-4022-A572-31F52447B7D2_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D6A57709-5D66-49B7-BA88-A5EB6D3E728B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D8BB34D7-7381-4320-98AA-EF74A4B83A1C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D93C028E-6E67-46F4-A64E-466A44652C28_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/DD84EA7E-C791-44B8-BCAF-2B90C75BB13D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/E7E14C4D-E772-4F18-A48E-E2913CDC1813_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/EB170C11-FF6D-4962-82E1-1157B22100BD_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/Gb Fifth instar nymph 4 SBush.tif  \n","  inflating: data/fabi-data/test/6/Gb Fifth Instar Nymph 6 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb instar 1 and 2 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb possibly second and third instar 1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb possibly third instar 1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002042.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002053.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002054.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002057.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002062.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002064.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002068.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002080.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002083.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002084.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002085.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002088.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002090.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002102.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002108.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002112.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002113.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002117.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002119.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002128.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002129.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002131.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002134.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002135.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002141.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002143.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002144.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002147.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002150.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002153.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002160.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002163.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002165.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002174.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002179.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002184.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002187.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002188.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002198.jpg  \n","   creating: data/fabi-data/test/7/\n","  inflating: data/fabi-data/test/7/80CFE9F1-7450-4B61-BE80-D93BCD03AE91_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/9A54E69A-0431-4817-BA25-0527E0B252D1_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/BE8504A8-0047-4E20-886A-4802BC642523_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/E5C68486-5E51-49CF-8007-77C05C81C09E_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/Gonipterus_adult4.jpg  \n","  inflating: data/fabi-data/test/7/Gonipterus_adult8.jpg  \n","  inflating: data/fabi-data/test/7/Gonipterus_larvae5.jpg  \n","  inflating: data/fabi-data/test/7/IMG_4254.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4255.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4461.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4467.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6114.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6168.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6170.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6199.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6202.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6213.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6217.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6218.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6219.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6227.JPG  \n","  inflating: data/fabi-data/test/7/none-0000001491.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001492.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001496.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001501.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001505.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001506.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001515.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001516.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001523.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001534.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001535.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001538.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001540.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001551.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001556.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001559.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001560.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001563.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001565.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001568.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001589.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001590.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001595.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001597.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001598.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001602.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001605.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001608.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001611.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001613.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001615.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001623.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001638.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001643.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001648.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001652.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001659.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001660.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001661.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001665.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001667.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001675.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001678.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001681.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001688.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001691.tif  \n","  inflating: data/fabi-data/test/7/none-0000001693.tif  \n","  inflating: data/fabi-data/test/7/none-0000001694.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001696.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001701.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001704.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001706.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001707.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001708.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001711.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001712.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001730.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001751.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001754.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001756.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001762.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001763.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001764.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001766.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001772.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001776.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001777.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001781.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001783.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001794.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001796.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001797.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001801.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001803.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001817.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001826.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001828.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001832.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001835.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001841.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001843.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001854.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001863.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001867.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001875.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001876.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001882.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001884.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001886.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001888.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001905.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001919.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001935.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001943.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001945.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001949.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001950.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001962.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001965.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001973.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001978.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001979.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001982.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001990.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001994.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001997.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002003.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002005.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002006.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002011.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002013.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002017.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002021.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002024.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002028.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002030.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002036.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002037.jpg  \n","   creating: data/fabi-data/test/8/\n","  inflating: data/fabi-data/test/8/Dissected_gall3.jpg  \n","  inflating: data/fabi-data/test/8/DSCN6609.JPG  \n","  inflating: data/fabi-data/test/8/DSCN6616.JPG  \n","  inflating: data/fabi-data/test/8/FB6A01A9-D6DD-4B2D-92B9-4D40FAF11427_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/8/IMG_0335.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0343.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0350.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0358.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1971.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1986.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1988.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2000.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2011.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2013.JPG  \n","  inflating: data/fabi-data/test/8/Leptocybe invasa pupae 1.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe invasa pupae 2.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe male 3.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe4.JPG_Female_wasp.jpg  \n","  inflating: data/fabi-data/test/8/Oviposition_scars_Linvasa4.jpg  \n","   creating: data/fabi-data/test/9/\n","  inflating: data/fabi-data/test/9/Megastigmus female brown 1b.jpg  \n","  inflating: data/fabi-data/test/9/Megastigmus Female cream 1a.jpg  \n","   creating: data/fabi-data/train/\n","   creating: data/fabi-data/train/0/\n","  inflating: data/fabi-data/train/0/Anitens_female_c.jpg  \n","   creating: data/fabi-data/train/1/\n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentails nymph 2.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis adult.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis adult4.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis nymph 1.jpg  \n","   creating: data/fabi-data/train/10/\n","  inflating: data/fabi-data/train/10/Galls of Ophelimus maskelli.jpeg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 1bresized.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 2a.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 2b.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 3a.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 11.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 3.tif  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 5.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 9.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 13.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes males 2.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes males 3.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 4.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 7.jpg  \n","   creating: data/fabi-data/train/11/\n","  inflating: data/fabi-data/train/11/DSC_0136 15 Edited.jpg  \n","  inflating: data/fabi-data/train/11/DSC_0142 21 Edited.jpg  \n","  inflating: data/fabi-data/train/11/DSC_0345.jpeg  \n","  inflating: data/fabi-data/train/11/DSC_9267.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9284.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9300.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9303.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9313.NEF  \n","  inflating: data/fabi-data/train/11/IMG_3374.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3422.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3423.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3424.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3427.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3428.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3431.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3466.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3467.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3468.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3471.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3474.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3477.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3480.JPG  \n","   creating: data/fabi-data/train/12/\n","  inflating: data/fabi-data/train/12/043C2B0E-C3E7-41B9-9BDB-C01CCA3C162D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/07026B37-232F-472A-BD0F-B017A7E5D5FA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0810112E-10C8-4ED0-9631-F61C83B146AD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0C7BA9F2-1A31-402A-A934-5CCE57889440_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0F76D033-8624-48C3-AA22-C33F924A3BFB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1506CA87-DCA3-47BD-881A-F968632308A0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1B6118D6-9FCE-4E11-9321-B0B7FD32570E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1C463DFF-26B7-4B58-BA3D-30E3AAFF74F3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/2A8EA66A-47FA-4675-8A0F-A176E733F78F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/2B21E100-F2D3-4E88-A0B8-2DD7CF73D3CB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/3776CD74-D775-4549-9FC4-DE0900DE8D20_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/39A60521-0B4D-4BBE-8FC5-AEFACC231A03_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/40FB2F99-92B1-4B88-B3C0-4B69C7C85FC7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/526A8779-51E4-4F73-BA4F-D91565CC716D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/54C59E62-E004-4BC6-8970-CE5BC29CF418_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/56D75DDE-CECE-4354-AC15-3D34AD513CA4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/5E308468-AE09-40CC-8D27-F09E25F0ACE1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/6BBF1749-8DAB-44F8-A278-4358F7A13615_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/6D4A3BE3-862A-4CE5-BF87-23FB9228E00B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/724605D2-B7D1-44D2-8124-E760342BFC17_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/76839D17-EAD6-4101-BA01-F63B919210AD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/782BB49D-9DBC-483C-9287-4CC6B53D273D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7C3DF9AE-3B0E-4897-AE00-BBD3F399201A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7D1AAA3E-56B4-45C6-B12F-CA100601C8C5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7EB5BBAF-7087-4C26-9437-57E968136593_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/809A0A24-F9B9-4431-980F-42302DEA701F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/81A74718-6965-4B9B-BC5D-7755B3C0CFD5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A18C6426-F85D-4E8C-8586-756DFC19D558_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A2545607-6019-4246-B72C-4468F0731855_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A83C99CE-D70E-42FE-9904-F5D4B38D428E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/B1DDF735-DFB9-4554-9B04-EEB62685A0CC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/B510B188-C0A3-4D3F-87EE-113B3184B211_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/BB66BC9C-7344-430C-95A2-FAA61725E76C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/CA051F33-8495-478B-9BA0-ED423A3887E6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/CA94B040-9C6D-4D2E-8DC6-D8CED087FA33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D0B97ABD-EEF2-462D-87F9-B9FB84F31D3E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D1C50C5E-5AA3-42A5-956A-076AB7980730_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D5FB90EA-0D5F-4846-B752-54F838173840_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/DC9F9526-78EC-4D61-BC13-B957FBAAA3E9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/DEC15466-3268-4182-9714-C3E4E5CE946C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/E12927A8-3986-4FCF-8AEE-89151DA14143_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/E74FD0F1-6A4C-42A4-8D5D-5BF11CBCF057_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/ECF5A653-FF4E-4F7E-860C-1B886D70BA76_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/EF507F58-1071-481D-AB04-F2EF3D2A532F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/F05DDFC7-4F98-4057-B6DC-2D0A75CF6D3D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/Larvae in growing tip of pine tree.jpeg  \n","   creating: data/fabi-data/train/13/\n","  inflating: data/fabi-data/train/13/Contents of dissected mummy.tif  \n","  inflating: data/fabi-data/train/13/Contents of dissected mummy2.tif  \n","  inflating: data/fabi-data/train/13/Dark.tif  \n","  inflating: data/fabi-data/train/13/Dark2.tif  \n","  inflating: data/fabi-data/train/13/Dark3.tif  \n","  inflating: data/fabi-data/train/13/Dark4.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 1a.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 1b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 2a.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 2b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 4.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 5a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 5b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 6a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 6b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 7a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 7b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 8a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 8b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 9 middle leg.jpg  \n","  inflating: data/fabi-data/train/13/From Gb mummies.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 2 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 3 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 4 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 5 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 6 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 7 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 1 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 10 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 11  Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 12 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 2 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 3 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 4 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 5 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 6 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 7 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Mummy April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus resized autocorrected.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus resized.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus2.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus3.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus4.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 1a.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 4.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 5.tif  \n","  inflating: data/fabi-data/train/13/P bliteus female Oct 2013 8a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus female Oct 2013 8b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 1a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 2b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 4.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 5.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 6.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 7.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 8.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus males feeding on honey paper Oct 2013 12.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Males feeding on honey paper Oct2013 13.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 2.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 5.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph 2b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph and lerp 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae 1a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Pupae 2.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae underneath 3b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus two parasitized nymphs 4b.jpg  \n","  inflating: data/fabi-data/train/13/Parasitised nymph Oct2013 1a.tif  \n","  inflating: data/fabi-data/train/13/Parasitised nymph Oct2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper2.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper3.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper5.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper6.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper7.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02a.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02c.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02d.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02f.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA04.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA04b.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05b edited.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05b.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05d.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA09 2.tif  \n","  inflating: data/fabi-data/train/13/SpObsSA10 1.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA10 2.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA10 3.jpg  \n","   creating: data/fabi-data/train/14/\n","  inflating: data/fabi-data/train/14/Possibly Quadristichus 2.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus 3 resized for email.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus autocorrected.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli2.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli5.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli6.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli8.jpg  \n","   creating: data/fabi-data/train/15/\n","  inflating: data/fabi-data/train/15/Copy of SA Ophelimus galls Selitrichodes Males 7.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 3.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 4.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5a.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5b.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 6.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 7a resized.jpg  \n","   creating: data/fabi-data/train/16/\n","  inflating: data/fabi-data/train/16/Image_120.jpg  \n","  inflating: data/fabi-data/train/16/Image_120b.jpg  \n","  inflating: data/fabi-data/train/16/Image_122.jpg  \n","  inflating: data/fabi-data/train/16/Image_125.jpg  \n","  inflating: data/fabi-data/train/16/Image_125b.jpg  \n","  inflating: data/fabi-data/train/16/Image_126.jpg  \n","  inflating: data/fabi-data/train/16/Image_127.jpg  \n","  inflating: data/fabi-data/train/16/Image_127b.jpg  \n","   creating: data/fabi-data/train/17/\n","  inflating: data/fabi-data/train/17/0CFB1157-776A-40EB-9023-8BFC1744B30A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/412F67DA-6F11-470D-BF35-4404E7BE6794_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/536385B9-96F1-4710-9D87-DA9AAE64D896_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/BDA98CCA-8E7D-417B-999D-5A5F0BAF1600_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/Browned larvae 1.jpg  \n","  inflating: data/fabi-data/train/17/DSCN0183.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0197.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0200.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0203.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0320.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1113.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1114.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1119.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1126.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1442.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1443.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1773.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1783.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1786.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1787.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1790.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1795.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1796.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1799.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1800.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1819.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1825.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1827.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1831.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2292.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2293.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2294.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2304.JPG  \n","  inflating: data/fabi-data/train/17/DSCN6938.JPG  \n","  inflating: data/fabi-data/train/17/DSCN6956.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7018.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7034.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7475.JPG  \n","  inflating: data/fabi-data/train/17/For scale.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no5.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no8.jpg  \n","  inflating: data/fabi-data/train/17/Larvae2 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae2 no2.jpg  \n","  inflating: data/fabi-data/train/17/Larvae3 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae4 stacked.jpg  \n","  inflating: data/fabi-data/train/17/Scale for Sirex larvae scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex larvae scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 1.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 3.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 4.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs 1.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs 2.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs with scale.jpg  \n","   creating: data/fabi-data/train/18/\n","  inflating: data/fabi-data/train/18/305175B6-1E20-49BE-AB7D-71FEEFEC553B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/406E93C6-E8FC-4B57-BFE7-F37D04B308C3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/4F1A6E39-3E0A-45F5-991D-556269EA3360_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/5C3B7AB7-6306-4F36-8E38-464431484BE0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/8E9BB382-24AB-4D23-A302-9701614AA090_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/90D2C906-6218-4A8D-886B-BFFA0D34ECC6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Adult Middelburg Spondyliaspis wing.jpg  \n","  inflating: data/fabi-data/train/18/Adult Middelburg Spondyliaspis2.jpg  \n","  inflating: data/fabi-data/train/18/Adult3.jpg  \n","  inflating: data/fabi-data/train/18/Adult4.jpg  \n","  inflating: data/fabi-data/train/18/AE3F0360-0A86-4032-A276-A92FCD48A89E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/B60AF6E2-271A-4CEE-A109-4AB2B67173E4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/B9D76401-6115-4CD8-8DF1-92DEEF573860_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/BDD5449D-7C42-48DB-AB3D-2444F43340AE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Damage 1.jpg  \n","  inflating: data/fabi-data/train/18/DC45DECE-6511-4F0B-8ECB-ADDC0265CC47_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/E2E5D0CA-FE62-4E35-9F00-D1EEF0620D71_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/ECA42586-03C5-4E02-A0ED-A57CF4A14C27_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/EF6AA3C8-9E98-4386-923D-A93977BF475A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Eggs.jpg  \n","  inflating: data/fabi-data/train/18/F1DCE26E-119E-4667-B246-85DBF0EC0FCC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/F45DF301-8453-4CAD-A8F7-47728C6DA032_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/F4829593-B31B-4AF4-98BE-3665CF01FF4E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/FCFBDFFE-4377-4114-AB98-49C6053B261F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Hatched eggs and lerps3.jpg  \n","  inflating: data/fabi-data/train/18/Hatched eggs and lerps5.jpg  \n","  inflating: data/fabi-data/train/18/Individual2 pic1.tif  \n","  inflating: data/fabi-data/train/18/Individual2 pic2.jpg  \n","  inflating: data/fabi-data/train/18/Individual2 pic4.tif  \n","  inflating: data/fabi-data/train/18/Largest lerp1.jpg  \n","  inflating: data/fabi-data/train/18/Medium Lerp.jpg  \n","  inflating: data/fabi-data/train/18/Medium Lerp3.jpg  \n","  inflating: data/fabi-data/train/18/Newly emerged adult.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002206.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002207.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002208.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002209.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002210.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002211.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002213.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002215.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002216.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002219.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002220.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002221.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002222.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002224.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002225.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002226.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002227.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002228.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002230.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002232.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002233.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002235.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002238.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002240.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002242.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002243.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002244.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002246.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002247.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002248.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002249.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002250.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002251.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002252.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002253.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002255.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002256.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002257.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002258.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002259.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002260.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002261.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002262.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002263.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002264.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002266.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002267.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002268.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002269.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002270.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002272.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002273.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002274.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002275.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002276.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002277.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002278.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002279.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002280.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002281.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002284.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002285.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002287.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002288.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002290.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002291.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002292.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002294.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002296.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002297.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002298.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002299.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002301.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002303.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002304.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002305.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002306.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002308.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002309.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002310.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002311.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002312.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002313.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002314.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002315.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002316.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002318.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002320.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002321.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002322.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002323.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002324.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002325.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002326.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002327.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002328.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002329.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002330.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002331.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002332.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002333.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002334.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002336.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002337.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002338.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002339.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002340.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002341.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002342.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002343.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002344.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002347.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002349.tif  \n","  inflating: data/fabi-data/train/18/none-0000002350.tif  \n","  inflating: data/fabi-data/train/18/none-0000002351.tif  \n","  inflating: data/fabi-data/train/18/none-0000002352.tif  \n","  inflating: data/fabi-data/train/18/none-0000002353.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002354.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002355.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002356.tif  \n","  inflating: data/fabi-data/train/18/none-0000002357.tif  \n","  inflating: data/fabi-data/train/18/none-0000002359.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002360.png  \n","  inflating: data/fabi-data/train/18/none-0000002361.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002362.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002363.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002364.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002365.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002366.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002367.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002368.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002369.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002370.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002371.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002372.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002373.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002374.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002375.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002376.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002377.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002378.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002379.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002380.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002381.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002382.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002383.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002385.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002386.tif  \n","  inflating: data/fabi-data/train/18/none-0000002387.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002388.tif  \n","  inflating: data/fabi-data/train/18/none-0000002389.tif  \n","  inflating: data/fabi-data/train/18/none-0000002390.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002392.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002393.tif  \n","  inflating: data/fabi-data/train/18/Nymph3.jpg  \n","  inflating: data/fabi-data/train/18/Nymph4.jpg  \n","  inflating: data/fabi-data/train/18/NymphLargeStackD.tif  \n","  inflating: data/fabi-data/train/18/Spondyliapsis adult 1a.jpg  \n","  inflating: data/fabi-data/train/18/Spondyliaspis wing different angle.jpg  \n","  inflating: data/fabi-data/train/18/Spondyliaspis wing.jpg  \n","  inflating: data/fabi-data/train/18/Stack with mesurements.jpg  \n","  inflating: data/fabi-data/train/18/Stack2C.jpg  \n","  inflating: data/fabi-data/train/18/Stack3B.jpg  \n","  inflating: data/fabi-data/train/18/Stack3C.jpg  \n","  inflating: data/fabi-data/train/18/StackB.jpg  \n","   creating: data/fabi-data/train/19/\n","  inflating: data/fabi-data/train/19/DSCN0817.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0819.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0821.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0827.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0828.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1595.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1596.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1597.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5446.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5456.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5457.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5458.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5463.JPG  \n","  inflating: data/fabi-data/train/19/eggs3.jpg  \n","  inflating: data/fabi-data/train/19/eggs4.jpg  \n","  inflating: data/fabi-data/train/19/Image14(2).jpg  \n","  inflating: data/fabi-data/train/19/Image21.jpg  \n","  inflating: data/fabi-data/train/19/Image26(2).jpg  \n","  inflating: data/fabi-data/train/19/Image43.jpg  \n","  inflating: data/fabi-data/train/19/Image46.jpg  \n","  inflating: data/fabi-data/train/19/IMG_0584.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0585.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0635.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0640.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0641.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0643.JPG  \n","  inflating: data/fabi-data/train/19/none-0000001395.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001396.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001400.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001402.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001403.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001404.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001406.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001407.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001408.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001409.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001410.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001411.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001412.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001413.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001414.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001415.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001416.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001417.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001418.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001419.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001420.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001421.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001422.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001423.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001424.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001426.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001427.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001428.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001429.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001430.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001431.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001432.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001435.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001437.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001439.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001441.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001442.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001443.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001444.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001445.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001446.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001447.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001448.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001449.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001450.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001451.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001453.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001454.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001455.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001456.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001457.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001458.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001460.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001462.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001463.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001464.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001465.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001466.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001467.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001470.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001471.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001473.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001474.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001475.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001476.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001477.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001478.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001479.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001480.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001481.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001482.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001484.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001485.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001486.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001488.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 2.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 3.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 4.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 7.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 1.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 3.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 5.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph2.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph3 adjusted.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph3.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph4.jpg  \n","   creating: data/fabi-data/train/2/\n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_male1.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_male2.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_ovipositing in Thaumastocoris peregrinus eggs1.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_with Thaumastocoris peregrinus eggs 2.jpg  \n","   creating: data/fabi-data/train/20/\n","  inflating: data/fabi-data/train/20/DSCN1695.JPG  \n","  inflating: data/fabi-data/train/20/DSCN1702.JPG  \n","  inflating: data/fabi-data/train/20/DSCN1708.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2289.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2290.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2314.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2324.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2329.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2341.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2342.JPG  \n","   creating: data/fabi-data/train/21/\n","  inflating: data/fabi-data/train/21/58BF974D-D89C-4CC6-AD15-9A702742AF8E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/A78D97FB-7738-4289-A3DD-9CD6B3F31A12_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/E8D65B8D-BDF5-4DA6-B8A4-3197AE190C51_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/FAA4C561-105A-4A9C-9ED5-9B945CD7B9F3_1_105_c.jpeg  \n","   creating: data/fabi-data/train/22/\n","  inflating: data/fabi-data/train/22/Armillaria fuscipes mushrooms Martin.bmp  \n","  inflating: data/fabi-data/train/22/Armillaria GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria internal_Izette.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria mellea clumps mushrooms Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria mellea mushroom clumps Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria mellea mushrooms on stump Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria montagnei mushrooms and spores Argentina 2012.jpg  \n","   creating: data/fabi-data/train/23/\n","  inflating: data/fabi-data/train/23/Austropuccinia1.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia2.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia3- ginna.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia3.JPG  \n","  inflating: data/fabi-data/train/23/none-0000000729.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000731.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000733.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000734.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000735.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000737.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000738.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000739.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000740.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000741.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000743.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000744.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000745.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000746.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000748.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000749.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000750.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000751.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000752.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000753.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000754.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000755.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000756.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000757.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000758.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000759.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000761.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000762.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000763.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000764.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000765.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000766.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000767.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000768.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000771.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000772.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000773.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000774.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000775.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000778.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000780.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000782.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000783.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000785.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000786.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000787.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000788.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000789.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000790.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000791.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000793.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000794.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000795.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000797.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000798.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000799.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000800.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000801.tif  \n","  inflating: data/fabi-data/train/23/none-0000000802.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000804.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000806.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000807.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000808.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000811.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000814.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000815.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000816.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000818.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000819.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000820.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000821.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000822.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000823.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000824.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000825.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000826.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000827.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000828.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000829.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000830.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000831.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000832.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000833.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000835.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000836.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000837.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000838.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000839.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000840.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000841.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000842.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000843.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000844.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000846.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000847.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000848.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000849.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000850.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000852.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000853.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000854.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000855.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000856.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000857.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000859.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000861.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000863.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000864.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000865.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000866.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000867.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000868.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000869.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000870.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000871.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000872.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000873.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000874.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000875.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000877.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000878.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000879.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000880.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000881.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000882.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000883.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000884.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000885.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000886.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000887.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000888.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000889.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000890.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000891.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000892.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000894.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000896.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000897.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000898.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000899.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000900.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000901.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000902.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000903.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000904.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000905.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000906.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000907.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000908.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000909.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000910.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000911.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000912.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000913.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000914.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000915.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000918.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000919.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000920.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000921.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000922.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000923.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000924.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000925.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000926.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000927.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000928.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000930.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000931.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000933.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000934.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000935.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000936.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000937.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000938.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000941.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000942.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000943.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000945.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000946.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000948.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000949.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000950.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000951.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000952.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000954.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000955.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000958.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000959.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000960.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000961.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000962.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000963.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000964.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000965.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000966.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000968.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000969.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000970.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000971.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000972.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000973.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000974.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000976.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000977.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000978.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000979.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000980.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000981.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000982.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000983.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000985.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000986.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000987.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000988.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000989.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000990.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000992.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000993.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000994.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000995.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000996.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000997.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000998.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000999.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001000.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001003.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001004.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001005.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001006.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001007.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001008.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001009.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001010.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001011.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001013.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001014.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001016.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001018.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001019.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001020.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001021.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001022.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001023.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001026.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001027.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001028.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001030.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001031.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001032.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001033.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001034.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001035.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001036.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001037.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001038.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001039.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001041.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001042.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001043.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001044.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001045.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001046.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001047.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001048.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001049.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001052.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001053.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001055.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001056.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001057.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001058.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001059.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001060.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001061.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001062.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001063.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001064.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001066.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001067.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001068.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001069.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001070.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001071.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001072.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001074.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001075.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001077.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001079.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001081.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001082.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001083.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001084.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001085.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001086.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001088.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001089.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001090.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001092.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001094.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001095.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001096.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001098.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001099.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001100.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001101.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001102.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001103.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001105.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001107.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001108.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001109.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001110.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001111.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001112.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001113.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001114.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001115.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001116.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001118.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001119.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001121.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001122.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001123.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001124.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001125.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001126.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001128.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001129.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001130.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001132.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001133.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001134.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001137.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001138.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001139.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001140.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001141.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001142.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001143.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001144.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001147.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001148.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001149.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001151.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001153.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001156.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001157.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001161.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001164.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001165.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001166.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001167.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001168.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001169.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001171.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001172.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001173.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001174.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001175.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001176.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001177.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001178.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001180.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001181.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001182.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001184.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001186.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001187.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001188.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001189.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001190.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001191.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001192.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001193.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001194.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001196.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001197.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001198.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001199.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001200.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001201.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001202.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001204.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001205.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001206.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001207.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001208.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001211.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001212.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001215.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001216.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001217.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001219.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001220.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001221.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001222.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001223.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001225.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001226.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001227.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001228.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001229.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001230.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001231.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001232.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001233.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001234.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001235.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001236.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001237.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001239.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001240.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001242.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001243.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001244.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001245.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001247.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001249.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001250.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001251.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001252.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001253.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001254.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001255.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001256.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001257.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001258.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001260.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001261.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001262.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001263.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001264.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001265.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001271.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001272.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001273.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001274.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001275.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001277.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001278.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001279.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001280.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001283.jpg  \n","   creating: data/fabi-data/train/24/\n","  inflating: data/fabi-data/train/24/Bacterial wilt cause by _i_Erwinia psidii_i_.png  \n","  inflating: data/fabi-data/train/24/Blisters on bark caused by _i_Erwinia psidii_i_.png  \n","  inflating: data/fabi-data/train/24/Dieback caused by _i_Erwinia psidii_i_ .png  \n","  inflating: data/fabi-data/train/24/Internal discolouration caused by _i_Erwinia psidii_i_.png  \n","   creating: data/fabi-data/train/25/\n","  inflating: data/fabi-data/train/25/Bacterial exudate of _i_Ralstonia solanacearum_i_.png  \n","  inflating: data/fabi-data/train/25/Bacterial Wilt external Ralstonia pseudosolanacearum_Izette.JPG  \n","  inflating: data/fabi-data/train/25/Bacterial wilt internal Ralstonia pseudosolanacearum_Izette.JPG  \n","  inflating: data/fabi-data/train/25/Discoloured wood caused by _i_Ralstonia solanacearum_i_.png  \n","   creating: data/fabi-data/train/26/\n","  inflating: data/fabi-data/train/26/Botryosphaeria canker external Neofusicoccum spp_Izette.jpg  \n","   creating: data/fabi-data/train/27/\n","  inflating: data/fabi-data/train/27/Fig. 3. A. Botryosphaeria canker on Eucalyptus sp. (F Jami).JPG  \n","  inflating: data/fabi-data/train/27/Fig. 4 Fruiting structures with abundant mature conidia.jpg  \n","  inflating: data/fabi-data/train/27/Fig. 4 Fruiting structures with abundant mature conidia2.jpg  \n","   creating: data/fabi-data/train/28/\n","  inflating: data/fabi-data/train/28/Calonectria leaf blight in _i_Eucalyptus plantation_i_.jpg  \n","   creating: data/fabi-data/train/29/\n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados02.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados03.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados04.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados05.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados07.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados08.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados09.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados10.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados12.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados13.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados15.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados16.JPG  \n","   creating: data/fabi-data/train/3/\n","  inflating: data/fabi-data/train/3/0C5FDA49-F7CC-461A-ACAB-8CEF936B2FE7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/1821C4A9-0060-49BE-875A-D2744FCDF300_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/26436902-1326-48AC-94C1-D954563010E8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/276FEA3C-D13E-485A-A77A-20E7C27AEEB7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/2FE42382-B37D-4589-B270-C18228444240_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/301FB1EE-42D3-4F1F-9557-AFEAC0D5ACED_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/3A380EEF-EDAA-4536-BAD3-53647701CA33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/40DF7AC2-02EB-431F-8187-C14CE5FDF3E5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/44CB9D90-2F90-45AC-BE5B-538DB9C1AF33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/5BC678E2-0A5E-400D-81D6-EFD53035639A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/684A08F6-7BC3-4457-A9DF-8CACFE2F1AB0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/7666AC27-C65B-4142-95E0-8FFF0A777124_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/8208F627-9878-4D90-B66C-5F86798F4212_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/9F3A3A9A-FB7A-4B9A-B40C-5525472CAF3A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/A0439CCD-AB23-4F24-A2A4-F87F227B439A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/BEBEEB9B-4406-40C0-BC1C-EC8464BA873F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/D7FD66DA-BC1C-451F-8AAA-D8A5FAF8F23A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/DE9DC524-5B08-4B39-BE83-C1B056B79B2C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/DSC000012.JPG  \n","  inflating: data/fabi-data/train/3/DSC00004.JPG  \n","  inflating: data/fabi-data/train/3/DSC00005.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0028.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0055.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0056.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0095.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0097.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0101.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0117.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0364.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0395.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0399.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0802.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0984.JPG  \n","  inflating: data/fabi-data/train/3/DSCN1332.JPG  \n","  inflating: data/fabi-data/train/3/DSCN1580.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4175.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4178.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4181.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4182.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4185.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4204.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5066.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5070.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5071.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5073.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5490.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5494.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5500.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5501.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5503.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5507.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5508.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5509.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5511.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5579.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5581.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5584.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5585.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5586.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5587.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5590.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5592.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5594.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5595.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5596.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5598.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5602.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5603.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5604.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6962.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6963.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6964.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6975.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7292.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7294.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7295.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7296.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7300.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7322.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8135.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8136.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8139.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8140.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8147.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8148.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8152.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8153.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8155.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8157.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8159.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8160.JPG  \n","  inflating: data/fabi-data/train/3/E02A74C2-39AE-4ADA-B062-448A5F21DD79_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/E6979849-8FE9-4D9B-86D4-BB6CE1C225C2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/FC0B348D-0788-4094-B2AA-68CD3DA014C2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/IMG_0674.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0675.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0677.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0678.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0679.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0680.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0681.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0685.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0686.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0696.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0697.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0698.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0700.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0701.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0702.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0711.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0712.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0713.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0714.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0715.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0716.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0717.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0718.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0721.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0723.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0725.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0726.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0727.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0728.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0735.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0740.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0742.JPG  \n","  inflating: data/fabi-data/train/3/larvae and tunnels.JPG  \n","   creating: data/fabi-data/train/30/\n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 07.JPG  \n","   creating: data/fabi-data/train/31/\n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 07.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 08.JPG  \n","  inflating: data/fabi-data/train/31/Diplodia1 Ginna.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia1.JPG  \n","  inflating: data/fabi-data/train/31/Diplodia2.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia3.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia4.JPG  \n","   creating: data/fabi-data/train/32/\n","  inflating: data/fabi-data/train/32/Fusarium circinatum collar rot.jpg  \n","   creating: data/fabi-data/train/33/\n","  inflating: data/fabi-data/train/33/Glycaspis 2.png  \n","  inflating: data/fabi-data/train/33/Glycaspis 3.png  \n","  inflating: data/fabi-data/train/33/P bliteus 1.png  \n","  inflating: data/fabi-data/train/33/P bliteus 2.png  \n","   creating: data/fabi-data/train/34/\n","  inflating: data/fabi-data/train/34/_i_Acacia mearnsii_i_ gummosis.JPG  \n","   creating: data/fabi-data/train/35/\n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 05.JPG  \n","   creating: data/fabi-data/train/36/\n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/36/Fuarium circinatum caramel coloured lessions and cankers.JPG  \n","  inflating: data/fabi-data/train/36/Fusarium circinatum resin exudation down main stem.JPG  \n","   creating: data/fabi-data/train/37/\n","  inflating: data/fabi-data/train/37/Powdery mildew GM Granados02.JPG  \n","  inflating: data/fabi-data/train/37/Powdery mildew GM Granados03.JPG  \n","   creating: data/fabi-data/train/38/\n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 06.JPG  \n","   creating: data/fabi-data/train/39/\n","  inflating: data/fabi-data/train/39/none-0000001284.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001285.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001286.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001287.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001289.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001290.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001291.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001292.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001293.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001294.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001295.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001296.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001297.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001298.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001299.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001300.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001301.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001303.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001305.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001306.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001307.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001308.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001309.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001310.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001311.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001313.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001314.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001315.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001317.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001318.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001321.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001322.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001323.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001325.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001326.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001327.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001329.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001332.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001333.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001334.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001335.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001336.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001337.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001338.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001340.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001341.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001342.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001343.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001344.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001345.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001346.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001348.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001349.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001350.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001352.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001353.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001354.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001355.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001356.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001357.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001359.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001360.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001361.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001362.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001364.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001365.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001367.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001368.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001369.jpg  \n","  inflating: data/fabi-data/train/39/Quambalaria GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/39/Quambalaria GM Granados 03.JPG  \n","   creating: data/fabi-data/train/4/\n","  inflating: data/fabi-data/train/4/Ctenarytaina male 1 SBush.tif  \n","  inflating: data/fabi-data/train/4/Ctenarytaina male 3 SBush.tif  \n","  inflating: data/fabi-data/train/4/Ctenarytaina male resized SBush.tif  \n","  inflating: data/fabi-data/train/4/Nymph 1 SBush.tif  \n","  inflating: data/fabi-data/train/4/Nymph 2 SBush.tif  \n","   creating: data/fabi-data/train/40/\n","  inflating: data/fabi-data/train/40/Fig.1b_FABInews.jpg  \n","  inflating: data/fabi-data/train/40/Fig.1e_Greyling_2016.png  \n","  inflating: data/fabi-data/train/40/Fig1.e_Greyling_2016.png  \n","  inflating: data/fabi-data/train/40/Fig1d_Dell_2008.png  \n","   creating: data/fabi-data/train/41/\n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5706.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5707.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5715.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5718.JPG  \n","   creating: data/fabi-data/train/42/\n","  inflating: data/fabi-data/train/42/Fig1_Conio.JPG  \n","   creating: data/fabi-data/train/43/\n","  inflating: data/fabi-data/train/43/none-0000000001.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000002.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000003.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000004.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000005.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000006.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000008.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000013.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000014.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000015.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000016.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000019.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000021.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000022.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000023.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000024.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000025.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000026.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000027.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000028.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000029.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000030.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000032.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000033.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000034.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000036.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000037.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000039.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000040.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000042.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000043.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000044.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000045.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000047.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000048.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000049.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000053.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000055.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000056.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000057.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000058.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000059.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000061.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000062.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000063.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000064.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000065.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000066.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000067.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000068.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000069.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000070.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000071.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000072.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000074.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000075.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000076.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000077.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000080.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000081.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000082.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000083.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000084.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000085.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000086.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000088.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000089.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000090.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000091.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000093.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000094.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000095.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000097.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000098.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000099.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000100.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000101.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000102.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000103.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000104.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000106.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000107.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000108.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000109.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000110.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000111.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000112.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000113.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000114.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000116.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000117.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000118.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000119.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000120.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000121.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000123.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000124.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000125.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000126.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000127.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000128.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000129.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000132.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000134.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000135.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000136.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000137.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000139.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000140.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000141.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000143.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000145.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000146.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000147.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000148.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000151.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000153.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000154.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000155.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000157.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000158.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000160.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000161.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000162.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000163.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000165.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000166.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000167.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000168.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000169.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000170.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000171.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000173.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000174.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000175.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000176.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000178.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000179.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000180.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000181.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000182.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000185.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000186.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000188.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000190.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000192.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000193.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000195.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000196.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000197.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000198.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000199.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000200.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000201.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000202.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000203.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000204.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000205.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000206.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000207.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000208.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000209.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000210.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000211.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000212.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000213.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000214.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000215.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000217.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000218.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000219.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000220.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000221.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000222.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000223.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000224.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000225.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000226.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000227.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000229.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000230.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000234.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000235.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000238.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000239.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000241.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000242.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000243.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000245.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000246.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000247.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000248.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000249.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000250.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000251.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000252.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000253.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000255.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000256.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000257.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000258.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000259.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000260.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000263.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000266.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000267.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000268.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000269.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000270.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000271.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000273.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000274.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000275.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000276.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000277.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000280.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000281.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000282.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000283.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000284.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000285.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000286.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000287.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000288.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000289.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000290.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000293.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000294.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000295.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000296.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000297.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000298.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000299.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000301.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000302.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000303.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000304.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000305.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000306.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000307.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000309.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000310.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000312.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000313.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000314.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000316.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000317.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000318.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000319.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000320.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000322.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000324.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000326.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000327.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000328.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000329.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000330.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000331.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000332.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000333.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000335.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000336.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000337.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000338.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000339.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000340.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000341.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000343.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000344.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000345.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000347.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000348.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000349.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000350.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000351.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000353.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000354.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000356.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000357.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000358.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000359.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000360.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000361.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000362.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000363.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000365.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000366.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000367.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000368.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000370.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000371.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000372.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000373.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000374.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000375.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000376.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000377.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000379.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000380.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000381.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000383.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000385.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000387.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000388.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000389.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000390.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000391.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000392.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000393.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000394.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000395.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000396.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000397.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000398.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000400.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000401.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000402.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000403.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000404.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000405.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000406.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000407.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000408.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000409.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000410.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000411.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000412.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000413.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000414.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000416.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000417.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000418.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000419.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000420.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000421.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000423.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000424.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000425.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000426.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000427.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000428.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000429.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000430.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000431.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000432.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000433.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000434.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000435.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000436.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000437.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000439.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000440.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000441.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000442.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000443.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000444.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000445.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000446.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000447.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000449.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000450.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000452.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000453.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000454.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000455.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000457.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000458.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000459.jpg  \n","  inflating: data/fabi-data/train/43/T.destructans KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/43/T.destructans KZN GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/43/Teratoshaeria destructans.jpg  \n","   creating: data/fabi-data/train/44/\n","  inflating: data/fabi-data/train/44/none-0000000460.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000461.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000462.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000464.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000465.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000466.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000467.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000468.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000469.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000470.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000472.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000473.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000475.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000476.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000477.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000478.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000479.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000482.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000483.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000484.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000485.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000486.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000487.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000488.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000490.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000491.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000492.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000493.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000494.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000495.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000496.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000497.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000500.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000501.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000502.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000504.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000505.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000507.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000508.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000509.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000510.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000511.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000512.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000514.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000515.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000516.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000517.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000519.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000520.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000523.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000526.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000529.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000530.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000531.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000532.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000533.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000534.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000535.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000536.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000537.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000538.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000539.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000540.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000542.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000543.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000544.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000547.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000548.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000549.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000550.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000551.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000552.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000553.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000554.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000555.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000556.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000557.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000558.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000561.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000563.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000565.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000566.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000568.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000569.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000570.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000572.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000573.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000574.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000575.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000576.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000577.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000578.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000579.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000581.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000582.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000583.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000584.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000585.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000586.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000587.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000588.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000589.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000591.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000592.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000593.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000594.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000595.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000596.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000597.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000598.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000599.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000600.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000601.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000602.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000603.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000604.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000605.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000606.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000607.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000608.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000609.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000610.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000611.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000612.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000613.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000614.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000615.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000617.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000620.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000621.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000622.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000623.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000624.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000625.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000626.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000627.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000628.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000629.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000630.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000632.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000634.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000635.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000636.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000637.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000638.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000639.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000640.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000643.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000645.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000646.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000647.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000648.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000649.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000651.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000652.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000654.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000655.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000657.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000658.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000659.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000660.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000661.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000663.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000664.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000666.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000668.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000669.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000670.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000671.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000672.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000673.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000674.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000676.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000677.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000679.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000680.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000681.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000682.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000683.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000684.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000685.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000686.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000690.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000692.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000693.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000694.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000695.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000697.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000698.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000699.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000700.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000701.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000702.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000704.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000705.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000706.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000708.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000709.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000711.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000712.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000713.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000714.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000715.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000716.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000717.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000718.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000719.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000720.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000721.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000722.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000724.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000725.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000727.jpg  \n","  inflating: data/fabi-data/train/44/T.nubilosa KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/44/T.nubilosa KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/44/T.nubilosa_Bulwer_KZN_GMGranados01.JPG  \n","   creating: data/fabi-data/train/45/\n","  inflating: data/fabi-data/train/45/1_Wattle rust.jpg  \n","  inflating: data/fabi-data/train/45/2_Wattle rust.jpg  \n","  inflating: data/fabi-data/train/45/3_Wattle rust.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados01.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados04.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados05.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados06.JPG  \n","   creating: data/fabi-data/train/5/\n","  inflating: data/fabi-data/train/5/DSCN1541.JPG  \n","  inflating: data/fabi-data/train/5/DSCN1549.JPG  \n","  inflating: data/fabi-data/train/5/Euproctis cocoon B.Hurley.JPG  \n","  inflating: data/fabi-data/train/5/Euproctis terminalis B. Hurley.JPG  \n","   creating: data/fabi-data/train/6/\n","  inflating: data/fabi-data/train/6/008BC3B2-28D6-45B9-A396-8822E5E3A58B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/067988C4-4AFC-4DA3-A5D5-687876C4B4A9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/0F8B2F51-6A54-4A64-BADC-CC0EFFE0934A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/10B0A8D6-8FD2-406B-9F34-9AD5A56C9420_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/17F73DE4-F546-458F-80F1-DCC7499D37BD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/1C2840B7-C814-4172-9B3C-5BB1B7BC4757_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/1DF0306F-7101-4631-A7CE-8DF7119E0FA1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/25F81054-30FB-4ABA-A07F-40605F97A934_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/2A87BA12-D302-40C3-8569-094A73128648_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/31C7381C-E429-48C3-A6F6-9E6103B86052_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/36CFC62C-3086-4F07-9C8E-714F2384BDE9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/37A1A534-49F9-4B08-B57C-9B5D536094B5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3A63507C-1ABB-4F55-B8BB-7F2BBC9B6400_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3A83DF2D-05D0-431F-ACEA-40555D61C20A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3FC127DE-2627-46ED-8C67-D906B08A6C86_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/406B6686-9138-49E1-9773-318D220E51C0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/410388D7-E42A-4913-809A-2B8BC0442EC6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/41C7A748-8639-4342-906B-98AC55BE0ED8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/44DF8EC3-6AFC-4946-8A9B-40EA3AC95B8B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/4592FF6A-E4DA-4C36-BAF8-62DAC82F88CA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/47F54696-67B1-4D5D-9933-6178C2488712_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/4900B533-0FA9-4FC5-AEE3-D832A9976AAC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/52A34225-1C60-4680-B04D-B1792C38816F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/55A7DDA7-36BE-4A98-8AAB-FD031C8085AA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5B4D5D9D-D3AE-43C6-81A3-82392A6E3248_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5B88AFAE-D521-4BF8-ADC4-51FC951A9F25_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5C639684-900F-4D05-B13F-2843921F3055_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5D49D5A6-D1E5-4019-9651-82B6AA8EA3A1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/653DE0A2-63F5-420A-B4D0-83E2618DF34A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6699EB04-A667-40C4-9534-F78893A6DC1F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6A28C7FA-3EAE-440C-B3CE-5B6116C6701A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6D1B7897-BB90-4641-925F-EE0C4F6A26C4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6D6AE712-2E5C-49D2-80E6-002D14BCAF3D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6ED38195-2E97-4A53-A527-51988D543EA7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/714D13EC-D9E5-4502-BFA6-80A20D8F06EB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/792CC3A1-F25E-407F-AA5B-0E3232483F07_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/7F123EB0-6C9F-48AF-A128-0A8A85B79D09_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/85A96CB6-A4EC-4A6A-B5C9-8C8094EF41A5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/861FFB9D-5531-4642-8952-A54A4BF2B663_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8E797FD5-764A-4106-8397-25B71F84731C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8F62CAF6-E978-4A22-93AC-27B1B280EF45_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8FAA8E86-0973-4580-A425-312E7AB2994C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9101F939-105B-40DA-BA93-2CFEB3FA403D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/924D457F-2FD6-4267-8556-1DE0511C80EE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9289C91F-6C77-4A79-83DE-497EF75195C4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/98E942F6-E83D-4FF1-ABFF-B252C6EA3FBB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/98F72AC5-B862-4C0F-A210-56EE67431AFE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9E0C2B5F-8283-471F-BBFF-9B28A568896D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9E79BC7E-504C-4EBD-9371-9512DB8D8CA7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A06EDB66-69B9-43B5-9122-E2D069F77381_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A1C66A43-6445-4541-A771-E908E8275172_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A2A5E574-BE8F-429C-891F-56EC825E6B6F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A475D807-1F06-45BF-9ABC-EDE7B2E68DEA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/ABCDBE90-942B-423B-A0D3-75FFFC366940_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/AD1C0931-F655-43B8-A6BE-006D76310911_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 1 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 6 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 7 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult male and female pic2SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult pic1SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult pic2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/B3272EC8-1856-4251-9BA2-4D382451CA1E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/B4DCC730-0B2F-41AB-A322-F7D9A8AC1E20_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C03B96D6-3E0D-4D79-B5A4-13DE12EB5314_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C04F90D5-BD60-40D8-AAB1-843738491FA0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C05CEC9E-4080-4DFE-9EC4-1F584E249B02_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C7F56B3D-6E73-4A02-B284-18CA7174AC8E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C8BE8542-DA18-4A7B-A04C-4D9C1799AF14_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/CF814A5F-819F-4848-B4C4-3B7DA3A1AAF8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/D0C4528B-7057-454D-831B-9566E339C8C7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/DCC84013-65D4-4D77-888E-B80221B916C3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/DCDAFC70-5C5E-4FCF-B2EF-6EA4FAD852B3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/E0DB2F7F-F9DD-49A3-A7ED-45818B8C1D4F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/E93FCE5F-27C0-4B95-8DA2-586C96728BCE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F11737D3-4F4A-4339-9310-1FC4ABD59BB9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F17502FC-4180-4B66-A99F-E57D19A8B593_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F24245A6-D94E-4867-8892-F47891568EE3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F302C29D-0144-4765-9C00-09AAD371B55D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F4CB8B2E-93B4-4F99-B6C9-E997F8B29B38_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F4E1557D-7413-473D-AD1E-137CCA4788E5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F5B4CFE7-E58E-4EF7-841D-8DC4CDAAEADB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F90135E3-52DE-4D0B-B062-E1EDCB1DFA14_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/FDC8CA73-FB4F-4F94-A0F5-471B85D54A4B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/Female 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 6 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 7 two females SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 9 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 1SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 1 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 3 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth instar nymph 2 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb Fifth instar nymph 3 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 7 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 1 to 4SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 1 to 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 2 to 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 4 to 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb male 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb nymph building lerp SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb nymph inside lerp SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly second instar SBush(1).jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly second instar SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly third Instar 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis  Male 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis Female 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis SBush.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002041.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002043.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002044.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002045.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002046.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002047.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002048.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002049.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002050.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002051.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002052.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002055.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002056.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002058.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002059.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002060.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002061.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002063.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002065.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002066.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002067.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002069.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002070.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002071.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002072.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002073.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002074.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002075.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002076.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002077.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002078.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002079.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002081.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002082.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002086.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002087.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002089.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002091.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002092.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002093.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002094.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002095.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002096.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002097.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002098.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002099.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002100.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002101.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002103.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002104.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002105.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002106.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002107.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002109.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002110.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002111.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002114.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002115.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002116.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002118.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002120.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002121.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002122.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002123.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002124.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002125.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002126.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002127.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002130.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002132.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002133.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002136.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002137.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002138.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002139.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002140.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002142.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002145.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002146.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002148.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002149.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002151.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002152.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002154.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002155.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002156.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002157.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002158.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002159.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002161.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002162.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002164.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002166.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002167.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002168.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002169.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002170.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002171.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002172.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002173.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002175.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002176.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002177.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002178.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002180.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002181.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002182.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002183.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002185.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002186.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002189.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002190.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002191.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002192.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002193.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002194.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002195.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002196.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002197.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002199.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002200.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002201.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002202.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002203.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002204.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002205.jpg  \n","   creating: data/fabi-data/train/7/\n","  inflating: data/fabi-data/train/7/1DD57CB0-BFEC-45B9-B2AA-41B8D73A118C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/28691AD0-0258-4D30-8170-394D4D8A7613_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2AA5A159-FB90-4634-ADDE-21FBBB1EAA59_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2F9D85EE-96F1-462B-A174-A26CB53CB378_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2FC02089-0F14-4F28-88CB-0FEBEFC426A1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/307E680B-5376-42FD-977D-068536FBCCBC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/457015D5-3B01-4980-91D3-D642A7D4AE32_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/4A0B9067-B991-4183-B1E7-AD732F081857_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/4BE8C325-C19A-4296-939D-C57E261734CC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/5295E23D-672A-4969-BFC8-BA6521C6924C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/5F340E00-6A07-4826-8A2E-434CBDCAC71F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/73B4ABE3-C24D-4B78-8E62-741793B9F018_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/A81B0FF7-5E8E-4E2C-835B-D7F65A4B5F2C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/A8633E3F-0D82-4BB6-96F9-3E09E4F1321C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/AD52FC80-9E88-4717-8751-AB485DE2B711_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/C9511A33-B2CB-40C4-B073-7B4E9E9A03B2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/CC90ECCD-079A-4F7D-B2FE-58261862CE90_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/DBE6BEF6-8AC8-4BE9-8630-47BB21C2EF8F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/DSC_4180.JPG  \n","  inflating: data/fabi-data/train/7/DSC_4204.JPG  \n","  inflating: data/fabi-data/train/7/E27726E7-101A-446A-A3EE-90DB010D6D85_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E286D740-DCBF-4245-AC9F-AE0DC27DF482_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E2D0BE4A-37BC-4722-BABF-B27914A2FB6F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E6C17234-303F-4516-864C-11D091D235B9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/Egg capsule.jpg  \n","  inflating: data/fabi-data/train/7/Egg packet (2) - Copy.jpg  \n","  inflating: data/fabi-data/train/7/Egg packet (2).jpg  \n","  inflating: data/fabi-data/train/7/Egg packet.jpg  \n","  inflating: data/fabi-data/train/7/Egg_packet_dissected.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult1.tif  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult2.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult3.tif  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult9.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg3.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg4.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg6.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_larvae.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_mating_pair.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_oviposit2.jpg  \n","  inflating: data/fabi-data/train/7/IMG_0338.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0364.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0411.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0415.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0418.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0419.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0420.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0421.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0434.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0439.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0444.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0447.JPG  \n","  inflating: data/fabi-data/train/7/IMG_1801.JPG  \n","  inflating: data/fabi-data/train/7/IMG_1807.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4239.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4245 (2).JPG  \n","  inflating: data/fabi-data/train/7/IMG_4245.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4249.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4462.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4464.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4465.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4466.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4468 (2).JPG  \n","  inflating: data/fabi-data/train/7/IMG_4468.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4469.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4470.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4471.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4472.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4473.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4474.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6090.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6091.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6092.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6112.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6113.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6115.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6116.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6117.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6118.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6119.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6131.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6132.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6133.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6134.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6135.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6136.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6159.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6160.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6162.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6163.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6164.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6165.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6166.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6167.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6169.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6171.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6172.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6173.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6193.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6194.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6196.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6197.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6198.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6200.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6201.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6204.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6205.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6206.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6207.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6208.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6209.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6210.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6211.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6212.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6214.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6215.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6216.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6225.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6226.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6228.JPG  \n","  inflating: data/fabi-data/train/7/none-0000001489.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001490.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001493.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001494.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001495.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001497.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001498.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001499.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001500.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001502.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001503.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001504.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001507.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001508.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001509.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001510.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001511.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001512.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001513.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001514.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001517.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001518.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001519.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001520.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001521.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001522.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001524.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001525.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001526.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001527.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001528.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001529.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001530.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001531.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001532.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001533.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001536.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001537.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001539.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001541.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001542.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001543.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001544.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001545.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001546.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001547.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001548.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001549.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001550.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001552.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001553.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001554.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001555.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001557.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001558.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001561.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001562.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001564.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001566.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001567.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001569.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001570.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001571.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001572.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001573.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001574.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001575.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001576.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001577.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001578.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001579.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001580.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001581.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001582.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001583.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001584.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001585.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001586.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001587.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001588.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001591.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001592.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001593.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001594.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001596.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001599.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001600.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001601.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001603.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001604.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001606.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001607.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001609.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001610.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001612.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001614.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001616.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001617.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001618.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001619.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001620.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001621.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001622.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001624.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001625.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001626.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001627.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001628.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001629.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001630.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001631.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001632.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001633.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001634.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001635.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001636.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001637.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001639.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001640.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001641.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001642.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001644.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001645.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001646.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001647.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001649.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001650.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001651.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001653.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001654.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001655.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001656.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001657.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001658.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001662.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001663.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001664.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001666.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001668.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001669.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001670.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001671.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001672.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001673.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001674.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001676.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001677.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001679.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001680.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001682.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001683.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001684.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001685.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001686.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001687.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001689.tif  \n","  inflating: data/fabi-data/train/7/none-0000001690.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001692.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001695.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001697.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001698.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001699.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001700.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001702.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001703.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001705.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001709.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001710.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001713.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001714.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001715.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001716.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001717.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001718.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001719.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001720.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001721.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001722.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001723.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001724.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001725.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001726.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001727.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001728.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001729.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001731.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001732.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001733.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001734.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001735.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001736.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001737.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001738.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001739.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001740.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001741.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001742.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001743.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001744.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001745.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001746.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001747.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001748.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001749.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001750.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001752.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001753.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001755.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001757.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001758.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001759.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001760.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001761.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001765.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001767.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001768.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001769.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001770.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001771.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001773.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001774.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001775.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001778.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001779.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001780.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001782.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001784.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001785.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001786.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001787.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001788.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001789.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001790.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001791.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001792.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001793.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001795.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001798.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001799.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001800.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001802.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001804.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001805.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001806.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001807.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001808.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001809.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001810.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001811.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001812.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001813.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001814.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001815.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001816.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001818.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001819.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001820.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001821.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001822.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001823.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001824.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001825.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001827.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001829.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001830.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001831.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001833.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001834.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001836.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001837.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001838.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001839.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001840.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001842.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001844.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001845.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001846.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001847.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001848.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001849.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001850.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001851.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001852.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001853.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001855.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001856.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001857.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001858.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001859.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001860.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001861.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001862.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001864.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001865.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001866.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001868.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001869.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001870.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001871.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001872.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001873.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001874.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001877.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001878.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001879.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001880.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001881.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001883.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001885.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001887.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001889.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001890.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001891.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001892.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001893.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001894.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001895.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001896.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001897.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001898.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001899.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001900.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001901.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001902.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001903.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001904.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001906.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001907.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001908.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001909.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001910.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001911.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001912.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001913.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001914.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001915.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001916.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001917.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001918.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001920.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001921.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001922.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001923.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001924.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001925.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001926.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001927.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001928.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001929.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001930.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001931.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001932.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001933.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001934.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001936.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001937.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001938.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001939.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001940.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001941.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001942.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001944.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001946.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001947.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001948.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001951.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001952.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001953.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001954.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001955.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001956.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001957.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001958.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001959.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001960.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001961.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001963.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001964.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001966.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001967.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001968.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001969.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001970.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001971.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001972.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001974.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001975.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001976.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001977.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001980.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001981.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001983.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001984.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001985.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001986.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001987.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001988.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001989.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001991.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001992.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001993.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001995.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001996.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001998.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001999.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002000.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002001.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002002.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002004.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002007.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002008.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002009.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002010.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002012.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002014.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002015.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002016.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002018.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002019.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002020.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002022.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002023.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002025.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002026.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002027.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002029.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002031.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002032.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002033.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002034.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002035.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002038.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002039.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002040.jpg  \n","   creating: data/fabi-data/train/8/\n","  inflating: data/fabi-data/train/8/1790F922-A855-4344-AD90-8B36530B71B9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/34210192-72DB-4F8C-971C-0BE8B2729412_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/36A5E91F-0B22-44B3-B65E-2F9D8B385502_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/394A3D8D-E242-4644-B0CA-6B2CB88F6561_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/404AD23D-88D0-4918-9CF6-A0C4A79361E9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/52C32865-F47F-4CA7-A49B-534F8F31B8CE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/572FBADC-28B6-4376-BFF3-FC29A6C87C0C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/80E39107-4E33-4DFD-B0EF-C51B242B1E09_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/D9C1DFA0-BC15-4218-A21E-CAF2B5A8953D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/Dissected_gall.jpg  \n","  inflating: data/fabi-data/train/8/Dissected_gall2.jpg  \n","  inflating: data/fabi-data/train/8/Dissected_gall4.jpg  \n","  inflating: data/fabi-data/train/8/DSCN5873.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6606.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6608.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6610.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6612.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6614.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6615.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6617.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6618.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6619.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6620.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6621.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6622.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6624.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6625.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6626.JPG  \n","  inflating: data/fabi-data/train/8/E8B8A6B0-208E-497C-89F1-BD949E702178_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/EA08DD96-0C39-4B8A-A10B-632E321C2DFE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/ECA50AD6-98EF-4400-A749-3B7D8E6F1C2B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/IMG_0336.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0337.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0338.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0340.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0346.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0349.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0354.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0355.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0370.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1867.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1870.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1874.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1969.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1970.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1987.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1990.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1992.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1999.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2001.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2002.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2012.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe female 1.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe female 1b.tif  \n","  inflating: data/fabi-data/train/8/Leptocybe female 2a.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe female 4a.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 1.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 2.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 4.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 5.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 6.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 7.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 8.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe Samantha Bush.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe1.JPG_Oviposition_scarring.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe2.JPG_Egg.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe3.JPG_Emergence_hole.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe5.JPG_Gall_formation_on_the_leaf_midrib.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa resized.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa2.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa3.jpg  \n","   creating: data/fabi-data/train/9/\n","  inflating: data/fabi-data/train/9/Megastigmus female brown 1a.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus female brown 2a.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus female brown 2b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus Female cream 1b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus male cream 1b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus pretorianensis.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus zebrinus.jpg  \n"]}],"source":["# Downlaod and extract fabi data\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF\" -O fabi-data.zip \u0026\u0026 rm -rf /tmp/cookies.txt\n","\n","!unzip fabi-data.zip -d data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3118,"status":"ok","timestamp":1696278268258,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"LkVR32jQqNjx","outputId":"bd16ace3-c7f0-4e17-dd14-ea01d6292585"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-10-02 20:24:25--  https://docs.google.com/uc?export=download\u0026confirm=t\u0026id=12e8MPvB3vi7SuUSmvl89wtSLyCskvk18\n","Resolving docs.google.com (docs.google.com)... 172.217.194.102, 172.217.194.138, 172.217.194.100, ...\n","Connecting to docs.google.com (docs.google.com)|172.217.194.102|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t7red0a0e8fsbc854t7a60du2u8j7iuv/1696278225000/11183032721846533402/*/12e8MPvB3vi7SuUSmvl89wtSLyCskvk18?e=download\u0026uuid=409efb44-3be2-48c4-86c6-0ebda86b34e7 [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-10-02 20:24:25--  https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t7red0a0e8fsbc854t7a60du2u8j7iuv/1696278225000/11183032721846533402/*/12e8MPvB3vi7SuUSmvl89wtSLyCskvk18?e=download\u0026uuid=409efb44-3be2-48c4-86c6-0ebda86b34e7\n","Resolving doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)... 74.125.130.132, 2404:6800:4003:c01::84\n","Connecting to doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)|74.125.130.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 41677934 (40M) [application/x-zip-compressed]\n","Saving to: ‘cnn-model.zip’\n","\n","cnn-model.zip       100%[===================\u003e]  39.75M  37.7MB/s    in 1.1s    \n","\n","2023-10-02 20:24:27 (37.7 MB/s) - ‘cnn-model.zip’ saved [41677934/41677934]\n","\n","Archive:  cnn-model.zip\n","  inflating: cnn-model/trainer_state.json  \n","  inflating: cnn-model/preprocessor_config.json  \n","  inflating: cnn-model/training_args.bin  \n","  inflating: cnn-model/config.json   \n","  inflating: cnn-model/pytorch_model.bin  \n"]}],"source":["# Downlaod ip102 cnn run 8 model\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=12e8MPvB3vi7SuUSmvl89wtSLyCskvk18' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=12e8MPvB3vi7SuUSmvl89wtSLyCskvk18\" -O cnn-model.zip \u0026\u0026 rm -rf /tmp/cookies.txt\n","\n","# https://drive.google.com/file/d/12e8MPvB3vi7SuUSmvl89wtSLyCskvk18/view?usp=sharing\n","# https://drive.google.com/file/d/12e8MPvB3vi7SuUSmvl89wtSLyCskvk18/view?usp=sharing\n","\n","!unzip cnn-model.zip -d cnn-model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej1I0zV6_yv8"},"outputs":[],"source":["\n","    # --dataset_name mnist \\\n","    # --output_dir output/cnn/mnist/mnistoutputs_1/ \\\n","    # --model_type resnet \\\n","\n","# !python cnn-ip102-fabi.py \\\n","#     --model_name cnn-model/ \\\n","#     --train_dir data/fabi-data/train \\\n","#     --validation_dir data/fabi-data/test \\\n","#     --output_dir drive/MyDrive/hons-research/output/cnn/ip102Fabi/ip102fabi_outputs_1/ \\\n","#     --remove_unused_columns False \\\n","#     --do_train \\\n","#     --do_eval \\\n","#     --push_to_hub False \\\n","#     --optim adamw_torch \\\n","#     --learning_rate 0.001 \\\n","#     --num_train_epochs 300 \\\n","#     --per_device_train_batch_size 64 \\\n","#     --per_device_eval_batch_size 64 \\\n","#     --logging_strategy steps \\\n","#     --logging_steps 10 \\\n","#     --evaluation_strategy epoch \\\n","#     --save_strategy epoch \\\n","#     --load_best_model_at_end True \\\n","#     --save_total_limit 3 \\\n","#     --save_steps 500 \\\n","#     --data_seed 1 \\\n","#     --seed 1 \\\n","#     --report_to wandb \\\n","#     --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"CQoIbuZzAz7E"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 20:25:24.354500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: W\u0026B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_202529-d0usn5er\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-frog-1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/d0usn5er\u001b[0m\n","10/02/2023 20:25:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 20:25:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=1,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/runs/Oct02_20-25-30_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=1,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 58378.41it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 191686.59it/s]\n","Downloading data files: 100% 2796/2796 [00:00\u003c00:00, 80150.87it/s]\n","Downloading data files: 100% 5/5 [00:00\u003c00:00, 18460.85it/s]\n","Extracting data files: 100% 5/5 [00:00\u003c00:00, 2136.25it/s]\n","Downloading data files: 100% 722/722 [00:00\u003c00:00, 79522.27it/s]\n","Downloading data files: 100% 1/1 [00:00\u003c00:00, 6909.89it/s]\n","Extracting data files: 100% 1/1 [00:00\u003c00:00, 1503.87it/s]\n","Generating train split: 2796 examples [00:00, 9652.82 examples/s]\n","Generating validation split: 722 examples [00:00, 11976.14 examples/s]\n","Casting the dataset: 100% 2796/2796 [00:00\u003c00:00, 69149.92 examples/s]\n","Casting the dataset: 100% 722/722 [00:00\u003c00:00, 18134.76 examples/s]\n","Downloading builder script: 100% 4.20k/4.20k [00:00\u003c00:00, 10.4MB/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 20:25:36,057 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 20:25:36,058 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 20:25:36,060 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 20:25:36,268 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 20:25:36,268 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 20:25:36,271 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 20:25:36,273 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 20:25:41,321 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 20:25:41,321 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 20:25:41,322 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 20:25:41,322 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 20:25:41,322 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 20:25:41,322 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 20:25:41,322 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 20:25:41,323 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 20:25:41,324 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:11\u003c3:33:48,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 20:26:52,771 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:26:52,772 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:26:52,772 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5125490427017212, 'eval_accuracy': 0.6024930747922438, 'eval_runtime': 20.1214, 'eval_samples_per_second': 35.882, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n","  0% 11/3300 [01:31\u003c3:33:48,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:27:12,898 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 20:27:12,904 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:27:13,014 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:27:13,018 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:31\u003c3:39:02,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 20:28:13,242 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:28:13,243 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:28:13,243 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2732208967208862, 'eval_accuracy': 0.6509695290858726, 'eval_runtime': 19.6587, 'eval_samples_per_second': 36.727, 'eval_steps_per_second': 0.153, 'epoch': 2.0}\n","  1% 22/3300 [02:51\u003c3:39:02,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:28:32,907 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 20:28:32,913 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:28:33,022 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:28:33,026 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:51\u003c3:34:17,  3.94s/it][INFO|trainer.py:3213] 2023-10-02 20:29:33,226 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:29:33,226 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:29:33,226 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9407262206077576, 'eval_accuracy': 0.7285318559556787, 'eval_runtime': 19.7283, 'eval_samples_per_second': 36.597, 'eval_steps_per_second': 0.152, 'epoch': 3.0}\n","  1% 33/3300 [04:11\u003c3:34:17,  3.94s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:29:52,961 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 20:29:52,968 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:29:53,090 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:29:53,095 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:12\u003c3:18:42,  3.66s/it][INFO|trainer.py:3213] 2023-10-02 20:30:53,773 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:30:53,773 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:30:53,773 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8511514663696289, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.5374, 'eval_samples_per_second': 36.955, 'eval_steps_per_second': 0.154, 'epoch': 4.0}\n","  1% 44/3300 [05:31\u003c3:18:42,  3.66s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:31:13,317 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 20:31:13,322 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:31:13,429 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:31:13,433 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:31:13,645 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:32\u003c3:34:26,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 20:32:13,849 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:32:13,850 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:32:13,850 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7812323570251465, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 19.7974, 'eval_samples_per_second': 36.469, 'eval_steps_per_second': 0.152, 'epoch': 5.0}\n","  2% 55/3300 [06:52\u003c3:34:26,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:32:33,654 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 20:32:33,660 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:32:33,771 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:32:33,775 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:32:33,981 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:52\u003c3:30:27,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 20:33:34,096 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:33:34,096 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:33:34,096 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7275392413139343, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.5214, 'eval_samples_per_second': 36.985, 'eval_steps_per_second': 0.154, 'epoch': 6.0}\n","  2% 66/3300 [08:12\u003c3:30:27,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:33:53,623 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 20:33:53,629 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:33:53,744 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:33:53,748 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:33:53,962 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:12\u003c3:40:22,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 20:34:54,102 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:34:54,103 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:34:54,103 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6815561056137085, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.7034, 'eval_samples_per_second': 36.643, 'eval_steps_per_second': 0.152, 'epoch': 7.0}\n","  2% 77/3300 [09:32\u003c3:40:22,  4.10s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:35:13,811 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 20:35:13,816 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:35:13,922 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:35:13,926 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:35:14,134 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:33\u003c3:27:15,  3.87s/it][INFO|trainer.py:3213] 2023-10-02 20:36:14,753 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:36:14,753 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:36:14,753 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7164969444274902, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.5955, 'eval_samples_per_second': 36.845, 'eval_steps_per_second': 0.153, 'epoch': 8.0}\n","  3% 88/3300 [10:53\u003c3:27:15,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:36:34,355 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 20:36:34,360 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:36:34,466 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:36:34,470 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:36:34,680 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:53\u003c3:25:29,  3.85s/it][INFO|trainer.py:3213] 2023-10-02 20:37:35,084 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:37:35,084 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:37:35,085 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7141689658164978, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.6639, 'eval_samples_per_second': 36.717, 'eval_steps_per_second': 0.153, 'epoch': 9.0}\n","  3% 99/3300 [12:13\u003c3:25:29,  3.85s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:37:54,754 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 20:37:54,771 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:37:54,878 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:37:54,882 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:37:55,094 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.6965, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:14\u003c3:32:02,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 20:38:55,451 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:38:55,451 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:38:55,451 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6280584931373596, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.6837, 'eval_samples_per_second': 36.68, 'eval_steps_per_second': 0.152, 'epoch': 10.0}\n","  3% 110/3300 [13:33\u003c3:32:02,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:39:15,140 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 20:39:15,146 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:39:15,253 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:39:15,257 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:39:15,485 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:34\u003c3:28:14,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 20:40:16,211 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:40:16,211 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:40:16,211 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8081530332565308, 'eval_accuracy': 0.778393351800554, 'eval_runtime': 19.7568, 'eval_samples_per_second': 36.544, 'eval_steps_per_second': 0.152, 'epoch': 11.0}\n","  4% 121/3300 [14:54\u003c3:28:14,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:40:35,974 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 20:40:35,979 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:40:36,088 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:40:36,092 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:40:36,298 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:54\u003c3:33:28,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 20:41:36,263 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:41:36,263 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:41:36,263 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6496309638023376, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.762, 'eval_samples_per_second': 36.535, 'eval_steps_per_second': 0.152, 'epoch': 12.0}\n","  4% 132/3300 [16:14\u003c3:33:28,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:41:56,030 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 20:41:56,035 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:41:56,144 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:41:56,149 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:41:56,376 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:15\u003c3:31:51,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 20:42:57,075 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:42:57,075 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:42:57,076 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.656539261341095, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.5333, 'eval_samples_per_second': 36.963, 'eval_steps_per_second': 0.154, 'epoch': 13.0}\n","  4% 143/3300 [17:35\u003c3:31:51,  4.03s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:43:16,614 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 20:43:16,619 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:43:16,726 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:43:16,730 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:43:16,965 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [18:35\u003c3:30:32,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 20:44:17,233 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:44:17,233 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:44:17,234 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6922107934951782, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.5901, 'eval_samples_per_second': 36.855, 'eval_steps_per_second': 0.153, 'epoch': 14.0}\n","  5% 154/3300 [18:55\u003c3:30:32,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:44:36,829 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 20:44:36,834 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:44:36,942 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:44:36,946 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:44:37,154 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:56\u003c3:27:45,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 20:45:37,445 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:45:37,445 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:45:37,445 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7360478639602661, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.6513, 'eval_samples_per_second': 36.741, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:15\u003c3:27:45,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:45:57,102 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 20:45:57,108 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:45:57,213 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:45:57,216 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:45:57,422 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:16\u003c3:26:16,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 20:46:58,013 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:46:58,013 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:46:58,013 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7455474734306335, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.705, 'eval_samples_per_second': 36.641, 'eval_steps_per_second': 0.152, 'epoch': 16.0}\n","  5% 176/3300 [21:36\u003c3:26:16,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:47:17,723 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 20:47:17,729 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:47:17,860 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:47:17,864 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:47:18,082 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:37\u003c3:27:26,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 20:48:18,342 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:48:18,342 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:48:18,342 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7331211566925049, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 19.5223, 'eval_samples_per_second': 36.983, 'eval_steps_per_second': 0.154, 'epoch': 17.0}\n","  6% 187/3300 [22:56\u003c3:27:26,  4.00s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:48:37,869 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 20:48:37,875 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:48:37,981 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:48:37,986 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:48:38,193 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:57\u003c3:36:41,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 20:49:38,546 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:49:38,547 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:49:38,547 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7565180659294128, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.6448, 'eval_samples_per_second': 36.753, 'eval_steps_per_second': 0.153, 'epoch': 18.0}\n","  6% 198/3300 [24:16\u003c3:36:41,  4.19s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:49:58,196 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 20:49:58,203 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:49:58,311 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:49:58,315 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:49:58,527 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2153, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:17\u003c3:28:37,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 20:50:58,614 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:50:58,615 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:50:58,615 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7089462280273438, 'eval_accuracy': 0.8254847645429363, 'eval_runtime': 19.5839, 'eval_samples_per_second': 36.867, 'eval_steps_per_second': 0.153, 'epoch': 19.0}\n","  6% 209/3300 [25:36\u003c3:28:37,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:51:18,205 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 20:51:18,211 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:51:18,334 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:51:18,338 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:51:18,561 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [26:37\u003c3:16:55,  3.84s/it][INFO|trainer.py:3213] 2023-10-02 20:52:18,538 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:52:18,539 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:52:18,539 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6793364882469177, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.556, 'eval_samples_per_second': 36.92, 'eval_steps_per_second': 0.153, 'epoch': 20.0}\n","  7% 220/3300 [26:56\u003c3:16:55,  3.84s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:52:38,102 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 20:52:38,108 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:52:38,227 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:52:38,232 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:52:38,472 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-198] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 20:52:38,497 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 20:52:38,497 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/checkpoint-110 (score: 0.6280584931373596).\n","{'train_runtime': 1617.2439, 'train_samples_per_second': 518.66, 'train_steps_per_second': 2.041, 'train_loss': 0.43068743619051847, 'epoch': 20.0}\n","  7% 220/3300 [26:57\u003c6:17:21,  7.35s/it]\n","[INFO|trainer.py:2939] 2023-10-02 20:52:38,571 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/\n","[INFO|configuration_utils.py:460] 2023-10-02 20:52:38,576 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:52:38,688 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:52:38,692 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       20.0\n","  train_loss               =     0.4307\n","  train_runtime            = 0:26:57.24\n","  train_samples_per_second =     518.66\n","  train_steps_per_second   =      2.041\n","[INFO|trainer.py:3213] 2023-10-02 20:52:38,706 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:52:38,707 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:52:38,707 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.74s/it]\n","***** eval metrics *****\n","  epoch                   =       20.0\n","  eval_accuracy           =     0.8116\n","  eval_loss               =     0.6281\n","  eval_runtime            = 0:00:19.67\n","  eval_samples_per_second =     36.701\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▅▆▇▇▇▇▇█▇██▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▃▃▂▂▁▂▂▁▂▁▁▂▂▂▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▃▁▄▁▃▂▃▃▄▄▁▂▃▃▁▂▂▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▆█▅█▆▇▆▆▅▅█▇▆▆█▇▇█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▇▅█▅█▅▇▇▅▅▅█▇▇▅█▇▇▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.81163\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.62806\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.6727\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.701\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 20.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 220\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2153\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.656632866390016e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.43069\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1617.2439\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 518.66\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.041\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfancy-frog-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/d0usn5er\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_202529-d0usn5er/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_1/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 1 \\\n","    --seed 1 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gAU8ZpJXhnFG"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 20:53:13.025978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_205317-yod76816\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdevout-wildflower-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/yod76816\u001b[0m\n","10/02/2023 20:53:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 20:53:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=2,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/runs/Oct02_20-53-18_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=2,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 42142.39it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 332072.03it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 20:53:21,861 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 20:53:21,862 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 20:53:21,864 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 20:53:22,010 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 20:53:22,011 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 20:53:22,014 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 20:53:22,016 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 20:53:23,875 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 20:53:23,875 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 20:53:23,875 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 20:53:23,875 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 20:53:23,875 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 20:53:23,875 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 20:53:23,876 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 20:53:23,876 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 20:53:23,877 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:04\u003c3:33:26,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 20:54:28,235 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:54:28,235 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:54:28,236 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.7352293729782104, 'eval_accuracy': 0.5443213296398892, 'eval_runtime': 20.0558, 'eval_samples_per_second': 36.0, 'eval_steps_per_second': 0.15, 'epoch': 1.0}\n","  0% 11/3300 [01:24\u003c3:33:26,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:54:48,297 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 20:54:48,303 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:54:48,413 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:54:48,418 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:36:14,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 20:55:49,083 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:55:49,083 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:55:49,084 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3332773447036743, 'eval_accuracy': 0.6274238227146814, 'eval_runtime': 19.6814, 'eval_samples_per_second': 36.684, 'eval_steps_per_second': 0.152, 'epoch': 2.0}\n","  1% 22/3300 [02:44\u003c3:36:14,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:56:08,771 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 20:56:08,778 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:56:08,891 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:56:08,896 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:45\u003c3:42:04,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 20:57:09,875 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:57:09,875 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:57:09,875 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9136693477630615, 'eval_accuracy': 0.7451523545706371, 'eval_runtime': 19.5348, 'eval_samples_per_second': 36.96, 'eval_steps_per_second': 0.154, 'epoch': 3.0}\n","  1% 33/3300 [04:05\u003c3:42:04,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:57:29,417 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 20:57:29,423 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:57:29,528 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:57:29,532 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:05\u003c3:31:20,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 20:58:29,851 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:58:29,851 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:58:29,851 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9137848615646362, 'eval_accuracy': 0.7506925207756233, 'eval_runtime': 19.5546, 'eval_samples_per_second': 36.922, 'eval_steps_per_second': 0.153, 'epoch': 4.0}\n","  1% 44/3300 [05:25\u003c3:31:20,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:58:49,411 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 20:58:49,416 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:58:49,522 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:58:49,526 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:58:49,748 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:26\u003c3:47:07,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 20:59:50,231 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:59:50,231 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:59:50,231 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9989383816719055, 'eval_accuracy': 0.7479224376731302, 'eval_runtime': 19.6617, 'eval_samples_per_second': 36.721, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:46\u003c3:47:07,  4.20s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:00:09,900 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 21:00:09,905 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:00:10,012 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:00:10,017 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:00:10,229 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:46\u003c3:29:26,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 21:01:10,418 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:01:10,419 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:01:10,419 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7707619071006775, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.6744, 'eval_samples_per_second': 36.698, 'eval_steps_per_second': 0.152, 'epoch': 6.0}\n","  2% 66/3300 [08:06\u003c3:29:26,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:01:30,099 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 21:01:30,105 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:01:30,212 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:01:30,216 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:01:30,424 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:07\u003c3:39:34,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 21:02:31,053 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:02:31,054 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:02:31,054 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7938690781593323, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.6446, 'eval_samples_per_second': 36.753, 'eval_steps_per_second': 0.153, 'epoch': 7.0}\n","  2% 77/3300 [09:26\u003c3:39:34,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:02:50,703 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 21:02:50,709 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:02:50,820 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:02:50,825 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:02:51,049 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:27\u003c3:25:39,  3.84s/it][INFO|trainer.py:3213] 2023-10-02 21:03:51,796 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:03:51,796 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:03:51,796 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6773293018341064, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.6073, 'eval_samples_per_second': 36.823, 'eval_steps_per_second': 0.153, 'epoch': 8.0}\n","  3% 88/3300 [10:47\u003c3:25:39,  3.84s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:04:11,410 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 21:04:11,416 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:04:11,531 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:04:11,535 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:04:11,745 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:48\u003c3:25:54,  3.86s/it][INFO|trainer.py:3213] 2023-10-02 21:05:12,136 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:05:12,136 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:05:12,136 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7343800663948059, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.5942, 'eval_samples_per_second': 36.848, 'eval_steps_per_second': 0.153, 'epoch': 9.0}\n","  3% 99/3300 [12:07\u003c3:25:54,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:05:31,737 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 21:05:31,753 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:05:31,858 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:05:31,862 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:05:32,071 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7098, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:08\u003c3:24:45,  3.85s/it][INFO|trainer.py:3213] 2023-10-02 21:06:32,202 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:06:32,202 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:06:32,202 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7459017038345337, 'eval_accuracy': 0.7797783933518005, 'eval_runtime': 19.6404, 'eval_samples_per_second': 36.761, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:27\u003c3:24:45,  3.85s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:06:51,848 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 21:06:51,854 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:06:51,962 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:06:51,966 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:06:52,192 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:28\u003c3:28:34,  3.94s/it][INFO|trainer.py:3213] 2023-10-02 21:07:52,157 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:07:52,157 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:07:52,157 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7546378374099731, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 19.6124, 'eval_samples_per_second': 36.813, 'eval_steps_per_second': 0.153, 'epoch': 11.0}\n","  4% 121/3300 [14:47\u003c3:28:34,  3.94s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:08:11,776 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 21:08:11,783 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:08:11,890 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:08:11,895 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:08:12,108 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [15:48\u003c3:44:06,  4.24s/it][INFO|trainer.py:3213] 2023-10-02 21:09:12,228 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:09:12,228 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:09:12,228 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7964629530906677, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.5822, 'eval_samples_per_second': 36.87, 'eval_steps_per_second': 0.153, 'epoch': 12.0}\n","  4% 132/3300 [16:07\u003c3:44:06,  4.24s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:09:31,817 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 21:09:31,832 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:09:31,940 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:09:31,944 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:09:32,157 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [17:08\u003c3:23:30,  3.87s/it][INFO|trainer.py:3213] 2023-10-02 21:10:32,288 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:10:32,288 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:10:32,288 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7391483187675476, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 19.6506, 'eval_samples_per_second': 36.742, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:28\u003c3:23:30,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:10:51,945 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 21:10:51,951 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:10:52,060 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:10:52,065 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:10:52,279 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [18:28\u003c3:28:59,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 21:11:52,733 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:11:52,733 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:11:52,734 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7075512409210205, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.7002, 'eval_samples_per_second': 36.649, 'eval_steps_per_second': 0.152, 'epoch': 14.0}\n","  5% 154/3300 [18:48\u003c3:28:59,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:12:12,440 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 21:12:12,445 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:12:12,552 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:12:12,556 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:12:12,765 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:49\u003c3:30:21,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 21:13:13,307 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:13:13,307 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:13:13,307 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.701051652431488, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.6258, 'eval_samples_per_second': 36.788, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:09\u003c3:30:21,  4.03s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:13:32,940 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 21:13:32,945 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:13:33,051 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:13:33,067 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:13:33,286 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:09\u003c3:29:23,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 21:14:33,738 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:14:33,739 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:14:33,739 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7535141110420227, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.5904, 'eval_samples_per_second': 36.855, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:29\u003c3:29:23,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:14:53,336 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 21:14:53,342 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:14:53,449 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:14:53,453 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:14:53,662 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:30\u003c3:24:00,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 21:15:54,115 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:15:54,116 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:15:54,116 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7883104681968689, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.6602, 'eval_samples_per_second': 36.724, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:49\u003c3:24:00,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:16:13,782 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 21:16:13,787 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:16:13,892 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:16:13,896 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:16:14,108 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:50\u003c3:22:17,  3.91s/it][INFO|trainer.py:3213] 2023-10-02 21:17:14,334 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:17:14,334 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:17:14,334 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7394670248031616, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.6711, 'eval_samples_per_second': 36.704, 'eval_steps_per_second': 0.153, 'epoch': 18.0}\n","  6% 198/3300 [24:10\u003c3:22:17,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:17:34,012 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 21:17:34,018 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:17:34,127 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:17:34,143 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:17:34,357 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-176] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 21:17:34,377 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 21:17:34,377 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/checkpoint-88 (score: 0.6773293018341064).\n","{'train_runtime': 1450.566, 'train_samples_per_second': 578.257, 'train_steps_per_second': 2.275, 'train_loss': 0.46578431370282414, 'epoch': 18.0}\n","  6% 198/3300 [24:10\u003c6:18:45,  7.33s/it]\n","[INFO|trainer.py:2939] 2023-10-02 21:17:34,447 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/\n","[INFO|configuration_utils.py:460] 2023-10-02 21:17:34,452 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:17:34,557 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:17:34,561 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       18.0\n","  train_loss               =     0.4658\n","  train_runtime            = 0:24:10.56\n","  train_samples_per_second =    578.257\n","  train_steps_per_second   =      2.275\n","[INFO|trainer.py:3213] 2023-10-02 21:17:34,575 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:17:34,575 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:17:34,575 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.74s/it]\n","***** eval metrics *****\n","  epoch                   =       18.0\n","  eval_accuracy           =      0.795\n","  eval_loss               =     0.6773\n","  eval_runtime            = 0:00:19.65\n","  eval_samples_per_second =     36.733\n","  eval_steps_per_second   =      0.153\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▆▇▆█▇██▇██▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▃▃▂▂▁▁▁▂▂▁▁▁▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▁▁▃▃▂▂▂▂▂▂▃▃▂▂▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆██▆▆▆▇▇▇▇▇▆▆▇▇▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▄█▆▆▄▆▆▆▆▆▆▆▄▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.79501\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.67733\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.6555\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.733\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.153\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 18.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 198\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00097\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7098\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.0909695797510144e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.46578\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1450.566\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 578.257\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.275\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdevout-wildflower-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/yod76816\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_205317-yod76816/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_2/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 2 \\\n","    --seed 2 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EuAxN-9TFIeO"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 21:18:06.686377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_211810-mln0cfic\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhelpful-wildflower-3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/mln0cfic\u001b[0m\n","10/02/2023 21:18:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 21:18:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=3,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/runs/Oct02_21-18-11_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=3,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 378706.90it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 325338.68it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 21:18:15,626 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 21:18:15,627 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 21:18:15,629 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 21:18:15,770 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 21:18:15,770 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 21:18:15,773 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 21:18:15,774 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 21:18:17,618 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 21:18:17,618 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 21:18:17,618 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 21:18:17,619 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 21:18:17,619 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 21:18:17,619 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 21:18:17,619 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 21:18:17,619 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 21:18:17,620 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:04\u003c3:40:06,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 21:19:22,596 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:19:22,596 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:19:22,596 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.591886281967163, 'eval_accuracy': 0.5623268698060941, 'eval_runtime': 20.1122, 'eval_samples_per_second': 35.899, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n","  0% 11/3300 [01:25\u003c3:40:06,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:19:42,715 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 21:19:42,720 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:19:42,827 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:19:42,831 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:29:17,  3.83s/it][INFO|trainer.py:3213] 2023-10-02 21:20:43,397 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:20:43,397 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:20:43,397 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2634447813034058, 'eval_accuracy': 0.6634349030470914, 'eval_runtime': 19.493, 'eval_samples_per_second': 37.039, 'eval_steps_per_second': 0.154, 'epoch': 2.0}\n","  1% 22/3300 [02:45\u003c3:29:17,  3.83s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:21:02,897 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 21:21:02,903 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:21:03,008 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:21:03,012 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:45\u003c3:40:41,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 21:22:03,269 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:22:03,270 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:22:03,270 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0503551959991455, 'eval_accuracy': 0.7119113573407202, 'eval_runtime': 19.5657, 'eval_samples_per_second': 36.901, 'eval_steps_per_second': 0.153, 'epoch': 3.0}\n","  1% 33/3300 [04:05\u003c3:40:41,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:22:22,841 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 21:22:22,847 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:22:22,960 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:22:22,965 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:05\u003c3:30:16,  3.87s/it][INFO|trainer.py:3213] 2023-10-02 21:23:23,329 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:23:23,330 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:23:23,330 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8416492938995361, 'eval_accuracy': 0.7728531855955678, 'eval_runtime': 19.5624, 'eval_samples_per_second': 36.908, 'eval_steps_per_second': 0.153, 'epoch': 4.0}\n","  1% 44/3300 [05:25\u003c3:30:16,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:23:42,898 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 21:23:42,904 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:23:43,020 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:23:43,026 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:23:43,251 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:25\u003c3:43:19,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 21:24:43,302 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:24:43,302 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:24:43,302 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8254572153091431, 'eval_accuracy': 0.7673130193905817, 'eval_runtime': 19.638, 'eval_samples_per_second': 36.765, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:45\u003c3:43:19,  4.13s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:25:02,947 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 21:25:02,953 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:25:03,060 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:25:03,064 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:25:03,275 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:46\u003c3:36:32,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 21:26:03,853 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:26:03,853 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:26:03,853 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7906554937362671, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.5533, 'eval_samples_per_second': 36.925, 'eval_steps_per_second': 0.153, 'epoch': 6.0}\n","  2% 66/3300 [08:05\u003c3:36:32,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:26:23,412 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 21:26:23,429 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:26:23,534 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:26:23,538 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:26:23,745 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:06\u003c3:52:31,  4.33s/it][INFO|trainer.py:3213] 2023-10-02 21:27:23,907 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:27:23,907 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:27:23,907 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8184888958930969, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.6413, 'eval_samples_per_second': 36.759, 'eval_steps_per_second': 0.153, 'epoch': 7.0}\n","  2% 77/3300 [09:25\u003c3:52:31,  4.33s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:27:43,554 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 21:27:43,560 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:27:43,678 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:27:43,682 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:27:43,908 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:26\u003c3:28:24,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 21:28:44,041 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:28:44,042 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:28:44,042 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7277288436889648, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.5075, 'eval_samples_per_second': 37.011, 'eval_steps_per_second': 0.154, 'epoch': 8.0}\n","  3% 88/3300 [10:45\u003c3:28:24,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:29:03,556 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 21:29:03,561 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:29:03,667 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:29:03,671 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:29:03,884 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:46\u003c3:28:42,  3.91s/it][INFO|trainer.py:3213] 2023-10-02 21:30:04,300 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:30:04,300 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:30:04,300 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7910296320915222, 'eval_accuracy': 0.7645429362880887, 'eval_runtime': 19.7524, 'eval_samples_per_second': 36.553, 'eval_steps_per_second': 0.152, 'epoch': 9.0}\n","  3% 99/3300 [12:06\u003c3:28:42,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:30:24,058 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 21:30:24,064 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:30:24,181 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:30:24,185 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:30:24,400 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7263, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:06\u003c3:27:36,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 21:31:24,345 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:31:24,345 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:31:24,345 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7646024227142334, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 19.5713, 'eval_samples_per_second': 36.891, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:26\u003c3:27:36,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:31:43,925 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 21:31:43,931 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:31:44,047 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:31:44,051 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:31:44,273 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:27\u003c3:24:24,  3.86s/it][INFO|trainer.py:3213] 2023-10-02 21:32:44,634 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:32:44,634 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:32:44,635 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6937806606292725, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 19.6296, 'eval_samples_per_second': 36.781, 'eval_steps_per_second': 0.153, 'epoch': 11.0}\n","  4% 121/3300 [14:46\u003c3:24:24,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:33:04,272 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 21:33:04,278 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:33:04,386 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:33:04,390 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:33:04,606 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:46\u003c3:23:39,  3.86s/it][INFO|trainer.py:3213] 2023-10-02 21:34:04,282 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:34:04,282 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:34:04,282 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.738738477230072, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.5196, 'eval_samples_per_second': 36.988, 'eval_steps_per_second': 0.154, 'epoch': 12.0}\n","  4% 132/3300 [16:06\u003c3:23:39,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:34:23,808 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 21:34:23,813 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:34:23,920 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:34:23,924 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:34:24,160 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:06\u003c3:24:12,  3.88s/it][INFO|trainer.py:3213] 2023-10-02 21:35:24,477 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:35:24,477 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:35:24,477 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6910752654075623, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.6446, 'eval_samples_per_second': 36.753, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:26\u003c3:24:12,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:35:44,127 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 21:35:44,133 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:35:44,240 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:35:44,244 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:35:44,456 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [18:26\u003c3:22:58,  3.87s/it][INFO|trainer.py:3213] 2023-10-02 21:36:44,531 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:36:44,531 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:36:44,531 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7414547204971313, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 19.6508, 'eval_samples_per_second': 36.741, 'eval_steps_per_second': 0.153, 'epoch': 14.0}\n","  5% 154/3300 [18:46\u003c3:22:58,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:37:04,189 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 21:37:04,194 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:37:04,301 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:37:04,306 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:37:04,517 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [19:47\u003c3:26:58,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 21:38:04,938 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:38:04,938 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:38:04,938 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6961999535560608, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 19.6105, 'eval_samples_per_second': 36.817, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:06\u003c3:26:58,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:38:24,555 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 21:38:24,560 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:38:24,668 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:38:24,672 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:38:24,911 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-132] due to args.save_total_limit\n","  5% 176/3300 [21:07\u003c3:30:06,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 21:39:25,009 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:39:25,010 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:39:25,010 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6758847832679749, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.5079, 'eval_samples_per_second': 37.011, 'eval_steps_per_second': 0.154, 'epoch': 16.0}\n","  5% 176/3300 [21:26\u003c3:30:06,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:39:44,523 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 21:39:44,529 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:39:44,636 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:39:44,641 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:39:44,851 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-143] due to args.save_total_limit\n","  6% 187/3300 [22:27\u003c3:31:04,  4.07s/it][INFO|trainer.py:3213] 2023-10-02 21:40:44,819 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:40:44,819 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:40:44,820 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7832902073860168, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.5684, 'eval_samples_per_second': 36.896, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:46\u003c3:31:04,  4.07s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:41:04,394 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 21:41:04,400 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:41:04,506 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:41:04,510 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:41:04,718 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-154] due to args.save_total_limit\n","  6% 198/3300 [23:47\u003c3:38:50,  4.23s/it][INFO|trainer.py:3213] 2023-10-02 21:42:05,086 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:42:05,086 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:42:05,086 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6445366740226746, 'eval_accuracy': 0.8213296398891967, 'eval_runtime': 19.7335, 'eval_samples_per_second': 36.587, 'eval_steps_per_second': 0.152, 'epoch': 18.0}\n","  6% 198/3300 [24:07\u003c3:38:50,  4.23s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:42:24,827 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 21:42:24,832 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:42:24,941 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:42:24,946 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:42:25,174 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-165] due to args.save_total_limit\n","{'loss': 0.2182, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:07\u003c3:23:29,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 21:43:25,289 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:43:25,290 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:43:25,290 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7939610481262207, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.7146, 'eval_samples_per_second': 36.623, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [25:27\u003c3:23:29,  3.95s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:43:45,011 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 21:43:45,017 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:43:45,125 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:43:45,129 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:43:45,344 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-176] due to args.save_total_limit\n","  7% 220/3300 [26:27\u003c3:21:28,  3.92s/it][INFO|trainer.py:3213] 2023-10-02 21:44:45,625 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:44:45,625 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:44:45,625 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7853844165802002, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.6554, 'eval_samples_per_second': 36.733, 'eval_steps_per_second': 0.153, 'epoch': 20.0}\n","  7% 220/3300 [26:47\u003c3:21:28,  3.92s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:45:05,288 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 21:45:05,294 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:45:05,401 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:45:05,405 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:45:05,615 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [27:48\u003c3:30:20,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 21:46:05,792 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:46:05,792 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:46:05,792 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7180808186531067, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.6441, 'eval_samples_per_second': 36.754, 'eval_steps_per_second': 0.153, 'epoch': 21.0}\n","  7% 231/3300 [28:07\u003c3:30:20,  4.11s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:46:25,443 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 21:46:25,449 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:46:25,558 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:46:25,562 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:46:25,790 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [29:08\u003c3:21:15,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 21:47:26,276 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:47:26,277 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:47:26,277 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6954718232154846, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.538, 'eval_samples_per_second': 36.954, 'eval_steps_per_second': 0.154, 'epoch': 22.0}\n","  7% 242/3300 [29:28\u003c3:21:15,  3.95s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:47:45,821 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 21:47:45,826 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:47:45,933 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:47:45,937 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:47:46,146 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [30:28\u003c3:13:55,  3.82s/it][INFO|trainer.py:3213] 2023-10-02 21:48:46,062 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:48:46,062 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:48:46,063 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6963913440704346, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.6574, 'eval_samples_per_second': 36.729, 'eval_steps_per_second': 0.153, 'epoch': 23.0}\n","  8% 253/3300 [30:48\u003c3:13:55,  3.82s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:49:05,738 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-02 21:49:05,746 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:49:05,852 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:49:05,856 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:49:06,071 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [31:48\u003c3:22:18,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 21:50:06,099 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:50:06,099 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:50:06,099 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7252514958381653, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.4533, 'eval_samples_per_second': 37.115, 'eval_steps_per_second': 0.154, 'epoch': 24.0}\n","  8% 264/3300 [32:07\u003c3:22:18,  4.00s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:50:25,559 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-02 21:50:25,565 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:50:25,671 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:50:25,675 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:50:25,897 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [33:08\u003c3:09:45,  3.76s/it][INFO|trainer.py:3213] 2023-10-02 21:51:26,003 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:51:26,003 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:51:26,003 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6649868488311768, 'eval_accuracy': 0.8157894736842105, 'eval_runtime': 19.6481, 'eval_samples_per_second': 36.747, 'eval_steps_per_second': 0.153, 'epoch': 25.0}\n","  8% 275/3300 [33:28\u003c3:09:45,  3.76s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:51:45,658 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-02 21:51:45,663 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:51:45,769 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:51:45,772 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:51:45,980 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [34:28\u003c3:32:08,  4.22s/it][INFO|trainer.py:3213] 2023-10-02 21:52:46,220 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:52:46,221 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:52:46,221 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7640067338943481, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.6412, 'eval_samples_per_second': 36.759, 'eval_steps_per_second': 0.153, 'epoch': 26.0}\n","  9% 286/3300 [34:48\u003c3:32:08,  4.22s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:53:05,868 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-02 21:53:05,874 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:53:05,979 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:53:05,983 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:53:06,192 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [35:49\u003c3:19:14,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 21:54:06,926 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:54:06,926 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:54:06,926 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.727474570274353, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.6702, 'eval_samples_per_second': 36.705, 'eval_steps_per_second': 0.153, 'epoch': 27.0}\n","  9% 297/3300 [36:08\u003c3:19:14,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:54:26,603 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-02 21:54:26,609 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:54:26,719 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:54:26,724 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:54:26,968 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1697, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [37:09\u003c3:11:54,  3.85s/it][INFO|trainer.py:3213] 2023-10-02 21:55:27,095 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:55:27,095 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:55:27,095 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.770520806312561, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.6001, 'eval_samples_per_second': 36.837, 'eval_steps_per_second': 0.153, 'epoch': 28.0}\n","  9% 308/3300 [37:29\u003c3:11:54,  3.85s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:55:46,701 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-02 21:55:46,707 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:55:46,815 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:55:46,819 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 21:55:47,026 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-286] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 21:55:47,038 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 21:55:47,039 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/checkpoint-198 (score: 0.6445366740226746).\n","{'train_runtime': 2249.4801, 'train_samples_per_second': 372.886, 'train_steps_per_second': 1.467, 'train_loss': 0.3662267172491396, 'epoch': 28.0}\n","  9% 308/3300 [37:29\u003c6:04:12,  7.30s/it]\n","[INFO|trainer.py:2939] 2023-10-02 21:55:47,104 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/\n","[INFO|configuration_utils.py:460] 2023-10-02 21:55:47,109 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:55:47,214 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:55:47,235 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       28.0\n","  train_loss               =     0.3662\n","  train_runtime            = 0:37:29.48\n","  train_samples_per_second =    372.886\n","  train_steps_per_second   =      1.467\n","[INFO|trainer.py:3213] 2023-10-02 21:55:47,250 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:55:47,250 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:55:47,250 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.79s/it]\n","***** eval metrics *****\n","  epoch                   =       28.0\n","  eval_accuracy           =     0.8213\n","  eval_loss               =     0.6445\n","  eval_runtime            = 0:00:19.78\n","  eval_samples_per_second =     36.484\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▅▇▇▇▇█▆▇▇▇█▇▇█▇█▇▇██▇██▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▄▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▂▂▁▁▂▁▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▁▂▂▃▂▃▂▄▂▃▂▃▃▃▂▂▄▄▃▃▂▃▁▃▃▃▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁█▇▇▆▇▆▇▅▇▆▇▆▆▆▇▇▅▅▆▆▇▆█▆▆▆▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁█▇▇▇▇▇█▅▇▇█▇▇▇█▇▅▅▇▇█▇█▇▇▇▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.82133\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.64454\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.7896\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.484\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 28.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 308\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1697\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 7.919286012946022e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.36623\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2249.4801\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 372.886\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.467\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhelpful-wildflower-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/mln0cfic\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_211810-mln0cfic/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_3/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 3 \\\n","    --seed 3 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hw_zqhJyFKun"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 21:56:21.413103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_215625-4t9svb5i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisty-rain-4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/4t9svb5i\u001b[0m\n","10/02/2023 21:56:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 21:56:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=4,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/runs/Oct02_21-56-26_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=4,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 55771.67it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 221397.52it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 21:56:30,256 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 21:56:30,257 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 21:56:30,259 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 21:56:30,403 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 21:56:30,403 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 21:56:30,406 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 21:56:30,408 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 21:56:32,238 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 21:56:32,238 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 21:56:32,238 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 21:56:32,238 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 21:56:32,238 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 21:56:32,239 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 21:56:32,239 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 21:56:32,239 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 21:56:32,240 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:05\u003c3:19:51,  3.65s/it][INFO|trainer.py:3213] 2023-10-02 21:57:37,354 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:57:37,355 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:57:37,355 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.7137205600738525, 'eval_accuracy': 0.5498614958448753, 'eval_runtime': 20.0653, 'eval_samples_per_second': 35.983, 'eval_steps_per_second': 0.15, 'epoch': 1.0}\n","  0% 11/3300 [01:25\u003c3:19:51,  3.65s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:57:57,425 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 21:57:57,431 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:57:57,539 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:57:57,544 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:36:30,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 21:58:57,747 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 21:58:57,747 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 21:58:57,747 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2560715675354004, 'eval_accuracy': 0.6398891966759003, 'eval_runtime': 19.4859, 'eval_samples_per_second': 37.053, 'eval_steps_per_second': 0.154, 'epoch': 2.0}\n","  1% 22/3300 [02:44\u003c3:36:30,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 21:59:17,238 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 21:59:17,244 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 21:59:17,355 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 21:59:17,359 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:45\u003c3:36:05,  3.97s/it][INFO|trainer.py:3213] 2023-10-02 22:00:17,650 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:00:17,650 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:00:17,650 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0692311525344849, 'eval_accuracy': 0.7271468144044322, 'eval_runtime': 19.5758, 'eval_samples_per_second': 36.882, 'eval_steps_per_second': 0.153, 'epoch': 3.0}\n","  1% 33/3300 [04:04\u003c3:36:05,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:00:37,233 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 22:00:37,239 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:00:37,352 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:00:37,357 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:06\u003c3:36:46,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 22:01:38,291 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:01:38,292 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:01:38,292 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9044975638389587, 'eval_accuracy': 0.7423822714681441, 'eval_runtime': 19.5849, 'eval_samples_per_second': 36.865, 'eval_steps_per_second': 0.153, 'epoch': 4.0}\n","  1% 44/3300 [05:25\u003c3:36:46,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:01:57,882 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 22:01:57,887 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:01:57,993 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:01:57,997 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:01:58,204 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:25\u003c3:42:03,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 22:02:58,237 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:02:58,238 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:02:58,238 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8538182377815247, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.6223, 'eval_samples_per_second': 36.795, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:45\u003c3:42:03,  4.11s/it]\n","100% 3/3 [00:05\u003c00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:03:17,866 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 22:03:17,873 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:03:17,982 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:03:17,986 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:03:18,212 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:45\u003c3:27:49,  3.86s/it][INFO|trainer.py:3213] 2023-10-02 22:04:18,103 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:04:18,103 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:04:18,103 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7858272194862366, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.6972, 'eval_samples_per_second': 36.655, 'eval_steps_per_second': 0.152, 'epoch': 6.0}\n","  2% 66/3300 [08:05\u003c3:27:49,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:04:37,807 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 22:04:37,812 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:04:37,925 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:04:37,929 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:04:38,150 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:05\u003c3:35:46,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 22:05:37,811 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:05:37,811 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:05:37,811 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8971755504608154, 'eval_accuracy': 0.7368421052631579, 'eval_runtime': 19.4698, 'eval_samples_per_second': 37.083, 'eval_steps_per_second': 0.154, 'epoch': 7.0}\n","  2% 77/3300 [09:25\u003c3:35:46,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:05:57,287 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 22:05:57,292 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:05:57,407 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:05:57,412 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:05:57,623 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:25\u003c3:52:10,  4.34s/it][INFO|trainer.py:3213] 2023-10-02 22:06:57,917 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:06:57,917 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:06:57,917 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7866522073745728, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 19.6638, 'eval_samples_per_second': 36.717, 'eval_steps_per_second': 0.153, 'epoch': 8.0}\n","  3% 88/3300 [10:45\u003c3:52:10,  4.34s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:07:17,586 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 22:07:17,592 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:07:17,699 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:07:17,703 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:07:17,928 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:45\u003c3:43:32,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 22:08:18,234 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:08:18,234 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:08:18,234 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6887191534042358, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.7816, 'eval_samples_per_second': 36.499, 'eval_steps_per_second': 0.152, 'epoch': 9.0}\n","  3% 99/3300 [12:05\u003c3:43:32,  4.19s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:08:38,022 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 22:08:38,028 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:08:38,145 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:08:38,150 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:08:38,380 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7211, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:06\u003c3:34:41,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 22:09:38,766 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:09:38,766 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:09:38,766 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7049255967140198, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.4723, 'eval_samples_per_second': 37.078, 'eval_steps_per_second': 0.154, 'epoch': 10.0}\n","  3% 110/3300 [13:25\u003c3:34:41,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:09:58,246 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 22:09:58,251 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:09:58,357 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:09:58,361 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:09:58,566 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:26\u003c3:28:28,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 22:10:58,936 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:10:58,936 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:10:58,936 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6406387090682983, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.5484, 'eval_samples_per_second': 36.934, 'eval_steps_per_second': 0.153, 'epoch': 11.0}\n","  4% 121/3300 [14:46\u003c3:28:28,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:11:18,490 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 22:11:18,496 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:11:18,601 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:11:18,605 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:11:18,842 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:47\u003c3:26:24,  3.91s/it][INFO|trainer.py:3213] 2023-10-02 22:12:19,399 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:12:19,399 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:12:19,399 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7926643490791321, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 19.5363, 'eval_samples_per_second': 36.957, 'eval_steps_per_second': 0.154, 'epoch': 12.0}\n","  4% 132/3300 [16:06\u003c3:26:24,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:12:38,942 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 22:12:38,947 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:12:39,056 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:12:39,060 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:12:39,270 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:07\u003c3:21:56,  3.84s/it][INFO|trainer.py:3213] 2023-10-02 22:13:39,499 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:13:39,499 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:13:39,499 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7756052017211914, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 19.5661, 'eval_samples_per_second': 36.901, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:26\u003c3:21:56,  3.84s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:13:59,071 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 22:13:59,077 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:13:59,188 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:13:59,193 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:13:59,410 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [18:27\u003c3:17:58,  3.78s/it][INFO|trainer.py:3213] 2023-10-02 22:14:59,686 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:14:59,686 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:14:59,686 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7336840629577637, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.457, 'eval_samples_per_second': 37.107, 'eval_steps_per_second': 0.154, 'epoch': 14.0}\n","  5% 154/3300 [18:46\u003c3:17:58,  3.78s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:15:19,150 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 22:15:19,155 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:15:19,265 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:15:19,270 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:15:19,500 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:47\u003c3:28:28,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 22:16:19,429 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:16:19,430 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:16:19,430 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7938776016235352, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.5929, 'eval_samples_per_second': 36.85, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:06\u003c3:28:28,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:16:39,029 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 22:16:39,035 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:16:39,141 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:16:39,145 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:16:39,355 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:07\u003c3:35:54,  4.15s/it][INFO|trainer.py:3213] 2023-10-02 22:17:39,954 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:17:39,954 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:17:39,954 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7422686815261841, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.5023, 'eval_samples_per_second': 37.021, 'eval_steps_per_second': 0.154, 'epoch': 16.0}\n","  5% 176/3300 [21:27\u003c3:35:54,  4.15s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:17:59,462 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 22:17:59,468 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:17:59,584 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:17:59,589 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:17:59,794 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:27\u003c3:29:27,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 22:19:00,058 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:19:00,059 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:19:00,059 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.696089506149292, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.7323, 'eval_samples_per_second': 36.59, 'eval_steps_per_second': 0.152, 'epoch': 17.0}\n","  6% 187/3300 [22:47\u003c3:29:27,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:19:19,797 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 22:19:19,803 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:19:19,914 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:19:19,918 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:19:20,138 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:48\u003c3:27:13,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 22:20:20,256 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:20:20,257 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:20:20,257 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8300472497940063, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.4832, 'eval_samples_per_second': 37.058, 'eval_steps_per_second': 0.154, 'epoch': 18.0}\n","  6% 198/3300 [24:07\u003c3:27:13,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:20:39,746 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 22:20:39,751 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:20:39,856 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:20:39,860 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:20:40,069 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.22, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:08\u003c3:38:30,  4.24s/it][INFO|trainer.py:3213] 2023-10-02 22:21:40,407 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:21:40,408 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:21:40,408 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7078130841255188, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.694, 'eval_samples_per_second': 36.661, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [25:27\u003c3:38:30,  4.24s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:22:00,109 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 22:22:00,116 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:22:00,232 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:22:00,236 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:22:00,477 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [26:28\u003c3:29:43,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 22:23:00,344 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:23:00,344 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:23:00,344 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6892114877700806, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.5431, 'eval_samples_per_second': 36.944, 'eval_steps_per_second': 0.154, 'epoch': 20.0}\n","  7% 220/3300 [26:47\u003c3:29:43,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:23:19,892 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 22:23:19,898 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:23:20,007 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:23:20,011 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:23:20,220 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [27:48\u003c3:36:30,  4.23s/it][INFO|trainer.py:3213] 2023-10-02 22:24:20,670 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:24:20,670 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:24:20,670 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.807765543460846, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.6798, 'eval_samples_per_second': 36.687, 'eval_steps_per_second': 0.152, 'epoch': 21.0}\n","  7% 231/3300 [28:08\u003c3:36:30,  4.23s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:24:40,357 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 22:24:40,363 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:24:40,480 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:24:40,485 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:24:40,710 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-209] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 22:24:40,732 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 22:24:40,732 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/checkpoint-121 (score: 0.6406387090682983).\n","{'train_runtime': 1688.566, 'train_samples_per_second': 496.753, 'train_steps_per_second': 1.954, 'train_loss': 0.4308773325635241, 'epoch': 21.0}\n","  7% 231/3300 [28:08\u003c6:13:53,  7.31s/it]\n","[INFO|trainer.py:2939] 2023-10-02 22:24:40,810 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/\n","[INFO|configuration_utils.py:460] 2023-10-02 22:24:40,816 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:24:40,932 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:24:40,937 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       21.0\n","  train_loss               =     0.4309\n","  train_runtime            = 0:28:08.56\n","  train_samples_per_second =    496.753\n","  train_steps_per_second   =      1.954\n","[INFO|trainer.py:3213] 2023-10-02 22:24:40,953 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:24:40,953 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:24:40,953 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.74s/it]\n","***** eval metrics *****\n","  epoch                   =       21.0\n","  eval_accuracy           =     0.8144\n","  eval_loss               =     0.6406\n","  eval_runtime            = 0:00:19.76\n","  eval_samples_per_second =     36.525\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▆▆▇▇▆▇▇██▇▇████▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▄▃▂▂▃▂▁▁▁▂▂▂▂▂▁▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▁▂▂▃▄▁▃▅▁▂▂▂▁▃▂▄▁▄▂▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁█▇▆▆▅█▆▄█▇▇▇█▆▇▅█▅▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁█▆▆▆▄█▆▄█▆█▆█▆█▄█▄█▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.8144\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.64064\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.7675\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.525\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 21.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 231\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.22\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.939464509709517e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.43088\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1688.566\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 496.753\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.954\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmisty-rain-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/4t9svb5i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_215625-4t9svb5i/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_4/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 4 \\\n","    --seed 4 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lGabtHrcFOTk"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 22:25:15.097630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_222519-t0uzcfeg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-sky-5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/t0uzcfeg\u001b[0m\n","10/02/2023 22:25:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 22:25:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=5,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/runs/Oct02_22-25-20_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=5,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 66278.41it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 337279.70it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 22:25:24,006 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 22:25:24,007 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 22:25:24,009 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 22:25:24,160 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 22:25:24,160 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 22:25:24,164 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 22:25:24,166 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 22:25:25,993 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 22:25:25,993 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 22:25:25,993 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 22:25:25,993 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 22:25:25,993 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 22:25:25,994 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 22:25:25,994 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 22:25:25,994 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 22:25:25,995 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:04\u003c3:41:40,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 22:26:30,642 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:26:30,643 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:26:30,643 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.8130241632461548, 'eval_accuracy': 0.4903047091412742, 'eval_runtime': 20.3484, 'eval_samples_per_second': 35.482, 'eval_steps_per_second': 0.147, 'epoch': 1.0}\n","  0% 11/3300 [01:24\u003c3:41:40,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:26:50,997 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 22:26:51,003 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:26:51,111 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:26:51,115 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:36:43,  3.97s/it][INFO|trainer.py:3213] 2023-10-02 22:27:51,500 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:27:51,501 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:27:51,501 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2508219480514526, 'eval_accuracy': 0.667590027700831, 'eval_runtime': 19.7396, 'eval_samples_per_second': 36.576, 'eval_steps_per_second': 0.152, 'epoch': 2.0}\n","  1% 22/3300 [02:45\u003c3:36:43,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:28:11,247 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 22:28:11,252 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:28:11,364 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:28:11,391 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:45\u003c3:43:47,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 22:29:11,976 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:29:11,976 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:29:11,976 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9483868479728699, 'eval_accuracy': 0.7160664819944599, 'eval_runtime': 19.6835, 'eval_samples_per_second': 36.681, 'eval_steps_per_second': 0.152, 'epoch': 3.0}\n","  1% 33/3300 [04:05\u003c3:43:47,  4.11s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:29:31,666 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 22:29:31,672 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:29:31,790 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:29:31,796 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:07\u003c3:42:02,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 22:30:33,153 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:30:33,153 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:30:33,154 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8670027256011963, 'eval_accuracy': 0.7451523545706371, 'eval_runtime': 19.7171, 'eval_samples_per_second': 36.618, 'eval_steps_per_second': 0.152, 'epoch': 4.0}\n","  1% 44/3300 [05:26\u003c3:42:02,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:30:52,877 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 22:30:52,882 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:30:52,989 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:30:52,993 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:30:53,212 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:27\u003c3:28:24,  3.85s/it][INFO|trainer.py:3213] 2023-10-02 22:31:53,603 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:31:53,604 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:31:53,604 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7548789381980896, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.6804, 'eval_samples_per_second': 36.686, 'eval_steps_per_second': 0.152, 'epoch': 5.0}\n","  2% 55/3300 [06:47\u003c3:28:24,  3.85s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:32:13,295 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 22:32:13,301 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:32:13,408 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:32:13,412 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:32:13,627 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:47\u003c3:37:48,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 22:33:13,614 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:33:13,615 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:33:13,615 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.772188663482666, 'eval_accuracy': 0.7825484764542936, 'eval_runtime': 19.7425, 'eval_samples_per_second': 36.571, 'eval_steps_per_second': 0.152, 'epoch': 6.0}\n","  2% 66/3300 [08:07\u003c3:37:48,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:33:33,364 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 22:33:33,369 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:33:33,475 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:33:33,479 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:33:33,687 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:07\u003c3:25:29,  3.83s/it][INFO|trainer.py:3213] 2023-10-02 22:34:33,399 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:34:33,399 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:34:33,399 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7196305990219116, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 19.7856, 'eval_samples_per_second': 36.491, 'eval_steps_per_second': 0.152, 'epoch': 7.0}\n","  2% 77/3300 [09:27\u003c3:25:29,  3.83s/it]\n","100% 3/3 [00:05\u003c00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:34:53,190 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 22:34:53,197 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:34:53,308 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:34:53,313 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:34:53,551 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:27\u003c3:37:17,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 22:35:53,574 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:35:53,574 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:35:53,574 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6835113763809204, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.7397, 'eval_samples_per_second': 36.576, 'eval_steps_per_second': 0.152, 'epoch': 8.0}\n","  3% 88/3300 [10:47\u003c3:37:17,  4.06s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:36:13,320 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 22:36:13,325 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:36:13,431 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:36:13,434 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:36:13,642 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:47\u003c3:31:00,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 22:37:13,645 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:37:13,645 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:37:13,646 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7482272982597351, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.7334, 'eval_samples_per_second': 36.588, 'eval_steps_per_second': 0.152, 'epoch': 9.0}\n","  3% 99/3300 [12:07\u003c3:31:00,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:37:33,385 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 22:37:33,391 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:37:33,497 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:37:33,500 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:37:33,710 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7119, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:07\u003c3:29:12,  3.94s/it][INFO|trainer.py:3213] 2023-10-02 22:38:33,939 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:38:33,939 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:38:33,940 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8014008402824402, 'eval_accuracy': 0.7728531855955678, 'eval_runtime': 19.8731, 'eval_samples_per_second': 36.331, 'eval_steps_per_second': 0.151, 'epoch': 10.0}\n","  3% 110/3300 [13:27\u003c3:29:12,  3.94s/it]\n","100% 3/3 [00:05\u003c00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:38:53,819 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 22:38:53,825 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:38:53,935 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:38:53,939 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:38:54,179 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:28\u003c3:38:08,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 22:39:54,457 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:39:54,457 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:39:54,457 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7510985732078552, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 19.7888, 'eval_samples_per_second': 36.485, 'eval_steps_per_second': 0.152, 'epoch': 11.0}\n","  4% 121/3300 [14:48\u003c3:38:08,  4.12s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:40:14,252 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 22:40:14,258 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:40:14,369 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:40:14,373 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:40:14,594 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [15:48\u003c3:22:50,  3.84s/it][INFO|trainer.py:3213] 2023-10-02 22:41:14,976 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:41:14,976 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:41:14,977 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7938525080680847, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.5577, 'eval_samples_per_second': 36.916, 'eval_steps_per_second': 0.153, 'epoch': 12.0}\n","  4% 132/3300 [16:08\u003c3:22:50,  3.84s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:41:34,540 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 22:41:34,558 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:41:34,665 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:41:34,669 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:41:34,882 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [17:09\u003c3:26:11,  3.92s/it][INFO|trainer.py:3213] 2023-10-02 22:42:35,527 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:42:35,527 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:42:35,528 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7383626699447632, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.6826, 'eval_samples_per_second': 36.682, 'eval_steps_per_second': 0.152, 'epoch': 13.0}\n","  4% 143/3300 [17:29\u003c3:26:11,  3.92s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:42:55,216 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 22:42:55,223 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:42:55,328 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:42:55,332 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:42:55,549 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [18:29\u003c3:27:37,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 22:43:55,843 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:43:55,843 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:43:55,843 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6942071914672852, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.6964, 'eval_samples_per_second': 36.656, 'eval_steps_per_second': 0.152, 'epoch': 14.0}\n","  5% 154/3300 [18:49\u003c3:27:37,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:44:15,544 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 22:44:15,550 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:44:15,655 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:44:15,659 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:44:15,867 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:49\u003c3:20:22,  3.83s/it][INFO|trainer.py:3213] 2023-10-02 22:45:15,781 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:45:15,782 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:45:15,782 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8113961219787598, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 19.7964, 'eval_samples_per_second': 36.471, 'eval_steps_per_second': 0.152, 'epoch': 15.0}\n","  5% 165/3300 [20:09\u003c3:20:22,  3.83s/it]\n","100% 3/3 [00:05\u003c00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:45:35,587 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 22:45:35,594 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:45:35,706 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:45:35,723 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:45:35,943 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:10\u003c3:22:17,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 22:46:36,342 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:46:36,342 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:46:36,342 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8558001518249512, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.5614, 'eval_samples_per_second': 36.909, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:29\u003c3:22:17,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:46:55,909 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 22:46:55,914 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:46:56,021 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:46:56,025 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:46:56,235 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:30\u003c3:26:27,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 22:47:56,429 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:47:56,430 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:47:56,430 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8003349900245667, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.6467, 'eval_samples_per_second': 36.749, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:50\u003c3:26:27,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:48:16,083 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 22:48:16,088 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:48:16,196 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:48:16,199 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:48:16,411 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:50\u003c3:29:58,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 22:49:16,953 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:49:16,953 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:49:16,953 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7412654757499695, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.6501, 'eval_samples_per_second': 36.743, 'eval_steps_per_second': 0.153, 'epoch': 18.0}\n","  6% 198/3300 [24:10\u003c3:29:58,  4.06s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:49:36,609 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 22:49:36,615 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:49:36,720 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:49:36,724 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:49:36,959 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-176] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 22:49:36,976 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 22:49:36,977 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/checkpoint-88 (score: 0.6835113763809204).\n","{'train_runtime': 1451.055, 'train_samples_per_second': 578.062, 'train_steps_per_second': 2.274, 'train_loss': 0.46643329389167554, 'epoch': 18.0}\n","  6% 198/3300 [24:11\u003c6:18:53,  7.33s/it]\n","[INFO|trainer.py:2939] 2023-10-02 22:49:37,053 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/\n","[INFO|configuration_utils.py:460] 2023-10-02 22:49:37,058 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:49:37,163 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:49:37,168 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       18.0\n","  train_loss               =     0.4664\n","  train_runtime            = 0:24:11.05\n","  train_samples_per_second =    578.062\n","  train_steps_per_second   =      2.274\n","[INFO|trainer.py:3213] 2023-10-02 22:49:37,182 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:49:37,182 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:49:37,182 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.74s/it]\n","***** eval metrics *****\n","  epoch                   =       18.0\n","  eval_accuracy           =     0.7936\n","  eval_loss               =     0.6835\n","  eval_runtime            = 0:00:19.77\n","  eval_samples_per_second =     36.515\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▆▇▇▇███▇██▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▂▁▂▁▁▁▂▁▂▁▁▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▂▂▂▃▃▃▃▄▃▁▂▂▃▁▂▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▇▇▇▆▆▆▆▅▆█▇▇▆█▇▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▇▇▇▇▇▇▇▇▆▇█▇▇▇███▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.79363\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.68351\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.7725\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.515\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 18.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 198\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00097\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7119\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.0909695797510144e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.46643\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1451.055\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 578.062\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.274\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfearless-sky-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/t0uzcfeg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_222519-t0uzcfeg/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_5/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 5 \\\n","    --seed 5 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uWs01trKFRid"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 22:50:09.646195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_225013-ufzrkcie\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-cloud-6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/ufzrkcie\u001b[0m\n","10/02/2023 22:50:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 22:50:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=6,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/runs/Oct02_22-50-14_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=6,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 51327.93it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 333643.06it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 22:50:18,519 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 22:50:18,520 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 22:50:18,522 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 22:50:18,663 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 22:50:18,664 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 22:50:18,667 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 22:50:18,669 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 22:50:20,497 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 22:50:20,497 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 22:50:20,498 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 22:50:20,498 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 22:50:20,498 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 22:50:20,498 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 22:50:20,498 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 22:50:20,499 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 22:50:20,500 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:05\u003c3:25:13,  3.74s/it][INFO|trainer.py:3213] 2023-10-02 22:51:25,611 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:51:25,611 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:51:25,611 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5856609344482422, 'eval_accuracy': 0.5637119113573407, 'eval_runtime': 19.9119, 'eval_samples_per_second': 36.26, 'eval_steps_per_second': 0.151, 'epoch': 1.0}\n","  0% 11/3300 [01:25\u003c3:25:13,  3.74s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:51:45,527 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 22:51:45,533 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:51:45,639 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:51:45,644 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:32:53,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 22:52:46,198 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:52:46,198 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:52:46,198 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1558016538619995, 'eval_accuracy': 0.6925207756232687, 'eval_runtime': 19.6979, 'eval_samples_per_second': 36.654, 'eval_steps_per_second': 0.152, 'epoch': 2.0}\n","  1% 22/3300 [02:45\u003c3:32:53,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:53:05,903 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 22:53:05,909 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:53:06,015 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:53:06,020 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:45\u003c3:23:23,  3.74s/it][INFO|trainer.py:3213] 2023-10-02 22:54:06,198 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:54:06,199 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:54:06,199 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.030949592590332, 'eval_accuracy': 0.7257617728531855, 'eval_runtime': 19.7157, 'eval_samples_per_second': 36.62, 'eval_steps_per_second': 0.152, 'epoch': 3.0}\n","  1% 33/3300 [04:05\u003c3:23:23,  3.74s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:54:25,921 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 22:54:25,927 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:54:26,039 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:54:26,044 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:05\u003c3:23:59,  3.76s/it][INFO|trainer.py:3213] 2023-10-02 22:55:26,336 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:55:26,336 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:55:26,337 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9164994955062866, 'eval_accuracy': 0.7520775623268698, 'eval_runtime': 19.5084, 'eval_samples_per_second': 37.01, 'eval_steps_per_second': 0.154, 'epoch': 4.0}\n","  1% 44/3300 [05:25\u003c3:23:59,  3.76s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:55:45,851 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 22:55:45,856 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:55:45,963 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:55:45,967 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:55:46,176 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:26\u003c3:24:36,  3.78s/it][INFO|trainer.py:3213] 2023-10-02 22:56:46,628 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:56:46,628 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:56:46,628 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7928858995437622, 'eval_accuracy': 0.7797783933518005, 'eval_runtime': 19.5624, 'eval_samples_per_second': 36.908, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:45\u003c3:24:36,  3.78s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:57:06,198 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 22:57:06,204 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:57:06,312 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:57:06,317 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:57:06,527 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:46\u003c3:35:44,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 22:58:06,740 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:58:06,740 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:58:06,740 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7506384253501892, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.6659, 'eval_samples_per_second': 36.713, 'eval_steps_per_second': 0.153, 'epoch': 6.0}\n","  2% 66/3300 [08:05\u003c3:35:44,  4.00s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:58:26,413 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 22:58:26,419 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:58:26,528 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:58:26,546 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:58:26,760 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:06\u003c3:39:05,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 22:59:26,916 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 22:59:26,917 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 22:59:26,917 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7256993055343628, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 19.6961, 'eval_samples_per_second': 36.657, 'eval_steps_per_second': 0.152, 'epoch': 7.0}\n","  2% 77/3300 [09:26\u003c3:39:05,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 22:59:46,618 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 22:59:46,624 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 22:59:46,735 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 22:59:46,739 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 22:59:46,961 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:26\u003c3:26:32,  3.86s/it][INFO|trainer.py:3213] 2023-10-02 23:00:46,970 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:00:46,970 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:00:46,970 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6958535313606262, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.5138, 'eval_samples_per_second': 36.999, 'eval_steps_per_second': 0.154, 'epoch': 8.0}\n","  3% 88/3300 [10:45\u003c3:26:32,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:01:06,492 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 23:01:06,497 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:01:06,601 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:01:06,606 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:01:06,816 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:46\u003c3:32:12,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 23:02:06,662 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:02:06,663 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:02:06,663 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.6394781470298767, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.6134, 'eval_samples_per_second': 36.812, 'eval_steps_per_second': 0.153, 'epoch': 9.0}\n","  3% 99/3300 [12:05\u003c3:32:12,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:02:26,282 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 23:02:26,288 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:02:26,409 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:02:26,413 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:02:26,624 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7187, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:06\u003c3:34:35,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 23:03:26,987 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:03:26,987 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:03:26,987 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6425327062606812, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.6256, 'eval_samples_per_second': 36.789, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:26\u003c3:34:35,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:03:46,619 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 23:03:46,624 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:03:46,731 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:03:46,735 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:03:46,971 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:26\u003c3:29:56,  3.96s/it][INFO|trainer.py:3213] 2023-10-02 23:04:47,094 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:04:47,095 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:04:47,095 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7521284818649292, 'eval_accuracy': 0.7797783933518005, 'eval_runtime': 19.8123, 'eval_samples_per_second': 36.442, 'eval_steps_per_second': 0.151, 'epoch': 11.0}\n","  4% 121/3300 [14:46\u003c3:29:56,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:05:06,915 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 23:05:06,921 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:05:07,035 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:05:07,039 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:05:07,257 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:47\u003c3:31:35,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 23:06:07,570 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:06:07,570 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:06:07,570 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6575528383255005, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.5745, 'eval_samples_per_second': 36.885, 'eval_steps_per_second': 0.153, 'epoch': 12.0}\n","  4% 132/3300 [16:06\u003c3:31:35,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:06:27,151 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 23:06:27,157 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:06:27,277 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:06:27,281 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:06:27,490 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [17:08\u003c3:19:06,  3.78s/it][INFO|trainer.py:3213] 2023-10-02 23:07:28,918 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:07:28,918 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:07:28,918 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6638205647468567, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.6271, 'eval_samples_per_second': 36.786, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:28\u003c3:19:06,  3.78s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:07:48,551 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 23:07:48,557 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:07:48,664 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:07:48,668 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:07:48,898 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [18:28\u003c3:17:47,  3.77s/it][INFO|trainer.py:3213] 2023-10-02 23:08:49,273 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:08:49,273 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:08:49,273 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6983149647712708, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 19.5679, 'eval_samples_per_second': 36.897, 'eval_steps_per_second': 0.153, 'epoch': 14.0}\n","  5% 154/3300 [18:48\u003c3:17:47,  3.77s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:09:08,848 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 23:09:08,854 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:09:08,965 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:09:08,969 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:09:09,182 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:48\u003c3:33:18,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 23:10:09,243 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:10:09,243 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:10:09,244 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7331042289733887, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.7413, 'eval_samples_per_second': 36.573, 'eval_steps_per_second': 0.152, 'epoch': 15.0}\n","  5% 165/3300 [20:08\u003c3:33:18,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:10:29,000 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 23:10:29,009 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:10:29,120 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:10:29,125 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:10:29,336 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:08\u003c3:27:17,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 23:11:29,206 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:11:29,206 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:11:29,206 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7469664216041565, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 19.5721, 'eval_samples_per_second': 36.889, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:28\u003c3:27:17,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:11:48,785 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 23:11:48,792 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:11:48,900 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:11:48,904 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:11:49,134 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:28\u003c3:24:03,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 23:12:49,359 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:12:49,359 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:12:49,359 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7909060716629028, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.5792, 'eval_samples_per_second': 36.876, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:48\u003c3:24:03,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:13:08,945 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 23:13:08,951 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:13:09,058 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:13:09,062 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:13:09,270 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:48\u003c3:22:30,  3.92s/it][INFO|trainer.py:3213] 2023-10-02 23:14:09,383 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:14:09,383 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:14:09,383 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7702085375785828, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 19.6397, 'eval_samples_per_second': 36.762, 'eval_steps_per_second': 0.153, 'epoch': 18.0}\n","  6% 198/3300 [24:08\u003c3:22:30,  3.92s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:14:29,029 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 23:14:29,035 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:14:29,156 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:14:29,160 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:14:29,375 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2152, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:09\u003c3:33:06,  4.14s/it][INFO|trainer.py:3213] 2023-10-02 23:15:29,873 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:15:29,873 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:15:29,873 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7377461194992065, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.7707, 'eval_samples_per_second': 36.519, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [25:29\u003c3:33:06,  4.14s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:15:49,651 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 23:15:49,658 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:15:49,769 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:15:49,774 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:15:50,003 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-187] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 23:15:50,019 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 23:15:50,019 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/checkpoint-99 (score: 0.6394781470298767).\n","{'train_runtime': 1529.5887, 'train_samples_per_second': 548.383, 'train_steps_per_second': 2.157, 'train_loss': 0.454492284350418, 'epoch': 19.0}\n","  6% 209/3300 [25:29\u003c6:17:01,  7.32s/it]\n","[INFO|trainer.py:2939] 2023-10-02 23:15:50,092 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/\n","[INFO|configuration_utils.py:460] 2023-10-02 23:15:50,097 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:15:50,202 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:15:50,206 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       19.0\n","  train_loss               =     0.4545\n","  train_runtime            = 0:25:29.58\n","  train_samples_per_second =    548.383\n","  train_steps_per_second   =      2.157\n","[INFO|trainer.py:3213] 2023-10-02 23:15:50,220 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:15:50,220 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:15:50,221 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.73s/it]\n","***** eval metrics *****\n","  epoch                   =       19.0\n","  eval_accuracy           =     0.8061\n","  eval_loss               =     0.6395\n","  eval_runtime            = 0:00:19.70\n","  eval_samples_per_second =     36.644\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▅▆▇▇▇▇██▇▇█▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▄▃▂▂▂▁▁▁▂▁▁▁▂▂▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▄▅▁▂▄▄▁▃▃▆▂▃▂▅▂▂▃▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▅▄█▇▅▅█▆▆▃▇▆▇▄▇▇▆▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▃▃█▆▆▃█▆▆▁▆▆▆▃▆▆▆▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.80609\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.63948\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.7033\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.644\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 19.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 209\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2152\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.373801223070515e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.45449\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1529.5887\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 548.383\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.157\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mglamorous-cloud-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/ufzrkcie\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_225013-ufzrkcie/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_6/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 6 \\\n","    --seed 6 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C5cP8uzzFVIB"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 23:16:23.253469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_231627-0onw9l54\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mazure-wave-7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/0onw9l54\u001b[0m\n","10/02/2023 23:16:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 23:16:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=7,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/runs/Oct02_23-16-28_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=7,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 71939.63it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 308555.33it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 23:16:32,208 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 23:16:32,209 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 23:16:32,211 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 23:16:32,354 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 23:16:32,355 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 23:16:32,358 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 23:16:32,360 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 23:16:34,188 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 23:16:34,188 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 23:16:34,188 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 23:16:34,188 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 23:16:34,188 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 23:16:34,188 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 23:16:34,188 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 23:16:34,189 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 23:16:34,190 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:04\u003c3:27:22,  3.78s/it][INFO|trainer.py:3213] 2023-10-02 23:17:38,647 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:17:38,647 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:17:38,648 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.6509217023849487, 'eval_accuracy': 0.5290858725761773, 'eval_runtime': 20.1292, 'eval_samples_per_second': 35.868, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n","  0% 11/3300 [01:24\u003c3:27:22,  3.78s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:17:58,783 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 23:17:58,790 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:17:58,905 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:17:58,909 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:25:20,  3.76s/it][INFO|trainer.py:3213] 2023-10-02 23:19:00,046 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:19:00,047 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:19:00,047 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1912046670913696, 'eval_accuracy': 0.6717451523545707, 'eval_runtime': 19.685, 'eval_samples_per_second': 36.678, 'eval_steps_per_second': 0.152, 'epoch': 2.0}\n","  1% 22/3300 [02:45\u003c3:25:20,  3.76s/it]\n","100% 3/3 [00:05\u003c00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:19:19,738 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 23:19:19,754 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:19:19,861 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:19:19,865 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:46\u003c3:31:14,  3.88s/it][INFO|trainer.py:3213] 2023-10-02 23:20:20,651 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:20:20,651 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:20:20,651 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9785252809524536, 'eval_accuracy': 0.7423822714681441, 'eval_runtime': 19.651, 'eval_samples_per_second': 36.741, 'eval_steps_per_second': 0.153, 'epoch': 3.0}\n","  1% 33/3300 [04:06\u003c3:31:14,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:20:40,311 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 23:20:40,317 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:20:40,433 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:20:40,437 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:06\u003c3:20:27,  3.69s/it][INFO|trainer.py:3213] 2023-10-02 23:21:40,904 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:21:40,904 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:21:40,905 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0687452554702759, 'eval_accuracy': 0.7049861495844876, 'eval_runtime': 19.704, 'eval_samples_per_second': 36.642, 'eval_steps_per_second': 0.152, 'epoch': 4.0}\n","  1% 44/3300 [05:26\u003c3:20:27,  3.69s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:22:00,614 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 23:22:00,621 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:22:00,729 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:22:00,732 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:22:00,941 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:26\u003c3:31:41,  3.91s/it][INFO|trainer.py:3213] 2023-10-02 23:23:00,940 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:23:00,940 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:23:00,941 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7476942539215088, 'eval_accuracy': 0.7853185595567868, 'eval_runtime': 19.6653, 'eval_samples_per_second': 36.714, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:46\u003c3:31:41,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:23:20,611 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 23:23:20,616 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:23:20,723 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:23:20,727 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:23:20,935 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:47\u003c3:38:12,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 23:24:21,587 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:24:21,587 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:24:21,587 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8691861033439636, 'eval_accuracy': 0.7631578947368421, 'eval_runtime': 19.6674, 'eval_samples_per_second': 36.711, 'eval_steps_per_second': 0.153, 'epoch': 6.0}\n","  2% 66/3300 [08:07\u003c3:38:12,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:24:41,261 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 23:24:41,267 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:24:41,387 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:24:41,391 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:24:41,609 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:07\u003c3:39:23,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 23:25:41,909 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:25:41,909 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:25:41,909 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7424572706222534, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 19.584, 'eval_samples_per_second': 36.867, 'eval_steps_per_second': 0.153, 'epoch': 7.0}\n","  2% 77/3300 [09:27\u003c3:39:23,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:26:01,500 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 23:26:01,506 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:26:01,614 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:26:01,619 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:26:01,840 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:27\u003c3:38:58,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 23:27:01,710 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:27:01,710 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:27:01,711 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7594853043556213, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.5916, 'eval_samples_per_second': 36.853, 'eval_steps_per_second': 0.153, 'epoch': 8.0}\n","  3% 88/3300 [10:47\u003c3:38:58,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:27:21,316 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 23:27:21,323 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:27:21,430 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:27:21,434 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:27:21,646 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:47\u003c3:28:25,  3.91s/it][INFO|trainer.py:3213] 2023-10-02 23:28:21,977 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:28:21,978 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:28:21,978 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7577560544013977, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.5803, 'eval_samples_per_second': 36.874, 'eval_steps_per_second': 0.153, 'epoch': 9.0}\n","  3% 99/3300 [12:07\u003c3:28:25,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:28:41,564 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 23:28:41,570 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:28:41,679 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:28:41,682 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:28:41,888 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7243, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:07\u003c3:27:00,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 23:29:41,986 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:29:41,986 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:29:41,986 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.723051905632019, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.5639, 'eval_samples_per_second': 36.905, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:27\u003c3:27:00,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:30:01,556 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 23:30:01,562 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:30:01,667 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:30:01,671 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:30:01,882 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:28\u003c3:24:09,  3.85s/it][INFO|trainer.py:3213] 2023-10-02 23:31:02,459 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:31:02,459 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:31:02,459 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7383555769920349, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 19.6949, 'eval_samples_per_second': 36.659, 'eval_steps_per_second': 0.152, 'epoch': 11.0}\n","  4% 121/3300 [14:47\u003c3:24:09,  3.85s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:31:22,161 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 23:31:22,167 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:31:22,283 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:31:22,298 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:31:22,517 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:49\u003c3:27:29,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 23:32:23,310 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:32:23,310 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:32:23,310 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6991182565689087, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.5799, 'eval_samples_per_second': 36.875, 'eval_steps_per_second': 0.153, 'epoch': 12.0}\n","  4% 132/3300 [16:08\u003c3:27:29,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:32:42,896 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 23:32:42,902 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:32:43,007 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:32:43,011 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:32:43,221 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:09\u003c3:24:06,  3.88s/it][INFO|trainer.py:3213] 2023-10-02 23:33:43,660 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:33:43,661 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:33:43,661 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.699390709400177, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.6087, 'eval_samples_per_second': 36.82, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:29\u003c3:24:06,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:34:03,276 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 23:34:03,283 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:34:03,398 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:34:03,403 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:34:03,632 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [18:30\u003c3:29:10,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 23:35:04,373 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:35:04,373 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:35:04,374 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6721455454826355, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.5918, 'eval_samples_per_second': 36.852, 'eval_steps_per_second': 0.153, 'epoch': 14.0}\n","  5% 154/3300 [18:49\u003c3:29:10,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:35:23,971 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 23:35:23,977 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:35:24,084 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:35:24,088 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:35:24,307 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [19:50\u003c3:28:58,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 23:36:25,158 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:36:25,158 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:36:25,159 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7433375120162964, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.6179, 'eval_samples_per_second': 36.803, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:10\u003c3:28:58,  4.00s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:36:44,782 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 23:36:44,789 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:36:44,908 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:36:44,913 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:36:45,140 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-132] due to args.save_total_limit\n","  5% 176/3300 [21:12\u003c3:31:26,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 23:37:46,460 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:37:46,460 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:37:46,461 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.740772545337677, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.6196, 'eval_samples_per_second': 36.8, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:31\u003c3:31:26,  4.06s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:38:06,087 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 23:38:06,093 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:38:06,199 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:38:06,203 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:38:06,413 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-143] due to args.save_total_limit\n","  6% 187/3300 [22:32\u003c3:29:33,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 23:39:06,426 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:39:06,426 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:39:06,426 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6956017017364502, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 19.5447, 'eval_samples_per_second': 36.941, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:51\u003c3:29:33,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:39:25,977 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 23:39:25,983 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:39:26,101 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:39:26,106 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:39:26,331 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:52\u003c3:25:57,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 23:40:26,952 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:40:26,952 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:40:26,952 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7138264179229736, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.6743, 'eval_samples_per_second': 36.698, 'eval_steps_per_second': 0.152, 'epoch': 18.0}\n","  6% 198/3300 [24:12\u003c3:25:57,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:40:46,632 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 23:40:46,638 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:40:46,744 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:40:46,748 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:40:46,971 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2166, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:13\u003c3:26:47,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 23:41:47,335 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:41:47,335 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:41:47,335 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.760982871055603, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.5596, 'eval_samples_per_second': 36.913, 'eval_steps_per_second': 0.153, 'epoch': 19.0}\n","  6% 209/3300 [25:32\u003c3:26:47,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:42:06,901 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 23:42:06,908 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:42:07,025 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:42:07,030 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:42:07,255 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [26:33\u003c3:22:48,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 23:43:07,776 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:43:07,777 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:43:07,777 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7563548684120178, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.6205, 'eval_samples_per_second': 36.798, 'eval_steps_per_second': 0.153, 'epoch': 20.0}\n","  7% 220/3300 [26:53\u003c3:22:48,  3.95s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:43:27,403 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 23:43:27,409 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:43:27,519 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:43:27,524 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:43:27,758 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [27:54\u003c3:30:13,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 23:44:28,569 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:44:28,569 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:44:28,570 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6733178496360779, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.5151, 'eval_samples_per_second': 36.997, 'eval_steps_per_second': 0.154, 'epoch': 21.0}\n","  7% 231/3300 [28:13\u003c3:30:13,  4.11s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:44:48,091 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 23:44:48,096 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:44:48,206 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:44:48,210 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:44:48,424 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [29:14\u003c3:20:22,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 23:45:48,689 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:45:48,690 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:45:48,690 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7699160575866699, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.6986, 'eval_samples_per_second': 36.652, 'eval_steps_per_second': 0.152, 'epoch': 22.0}\n","  7% 242/3300 [29:34\u003c3:20:22,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:46:08,403 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 23:46:08,411 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:46:08,516 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:46:08,520 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:46:08,731 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [30:35\u003c3:29:02,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 23:47:09,462 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:47:09,463 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:47:09,463 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7424999475479126, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.6595, 'eval_samples_per_second': 36.725, 'eval_steps_per_second': 0.153, 'epoch': 23.0}\n","  8% 253/3300 [30:54\u003c3:29:02,  4.12s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:47:29,129 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-02 23:47:29,135 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:47:29,244 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:47:29,249 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:47:29,518 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [31:55\u003c3:18:21,  3.92s/it][INFO|trainer.py:3213] 2023-10-02 23:48:30,062 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:48:30,063 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:48:30,063 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.729184627532959, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 19.6499, 'eval_samples_per_second': 36.743, 'eval_steps_per_second': 0.153, 'epoch': 24.0}\n","  8% 264/3300 [32:15\u003c3:18:21,  3.92s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:48:49,719 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-02 23:48:49,724 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:48:49,833 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:48:49,837 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:48:50,057 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-242] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 23:48:50,079 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 23:48:50,079 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/checkpoint-154 (score: 0.6721455454826355).\n","{'train_runtime': 1935.9494, 'train_samples_per_second': 433.276, 'train_steps_per_second': 1.705, 'train_loss': 0.39727998502326733, 'epoch': 24.0}\n","  8% 264/3300 [32:15\u003c6:11:03,  7.33s/it]\n","[INFO|trainer.py:2939] 2023-10-02 23:48:50,142 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/\n","[INFO|configuration_utils.py:460] 2023-10-02 23:48:50,148 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:48:50,264 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:48:50,268 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       24.0\n","  train_loss               =     0.3973\n","  train_runtime            = 0:32:15.94\n","  train_samples_per_second =    433.276\n","  train_steps_per_second   =      1.705\n","[INFO|trainer.py:3213] 2023-10-02 23:48:50,297 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:48:50,297 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:48:50,297 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.75s/it]\n","***** eval metrics *****\n","  epoch                   =       24.0\n","  eval_accuracy           =     0.8102\n","  eval_loss               =     0.6721\n","  eval_runtime            = 0:00:19.64\n","  eval_samples_per_second =     36.751\n","  eval_steps_per_second   =      0.153\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▆▅▇▇▇▇██▇███████▇██▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▄▂▂▂▂▂▁▁▁▁▁▂▁▁▁▂▂▁▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▁▃▂▂▁▃▃▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▆▆▆▆▇▇▇▇▆▇▇▇▇▇█▆▇▇█▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▅▇▅▇▇▇▇▇▇▅▇▇▇▇▇▇▅▇▇█▅▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.81025\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.67215\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.6457\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.751\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.153\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 24.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 264\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2166\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 6.787959439668019e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.39728\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1935.9494\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 433.276\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.705\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mazure-wave-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/0onw9l54\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_231627-0onw9l54/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_7/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 7 \\\n","    --seed 7 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-nDNgY-PFYRZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 23:49:23.994390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_234928-tvte08pm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-universe-8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/tvte08pm\u001b[0m\n","10/02/2023 23:49:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 23:49:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=8,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/runs/Oct02_23-49-29_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=8,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 48740.42it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 332764.38it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-02 23:49:32,774 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 23:49:32,775 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-02 23:49:32,777 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-02 23:49:32,918 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 23:49:32,918 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-02 23:49:32,922 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-02 23:49:32,923 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 23:49:34,769 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 23:49:34,769 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 23:49:34,769 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 23:49:34,769 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 23:49:34,769 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 23:49:34,770 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 23:49:34,770 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 23:49:34,770 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 23:49:34,771 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:04\u003c3:37:32,  3.97s/it][INFO|trainer.py:3213] 2023-10-02 23:50:39,234 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:50:39,234 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:50:39,234 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.7790454626083374, 'eval_accuracy': 0.4806094182825485, 'eval_runtime': 19.891, 'eval_samples_per_second': 36.298, 'eval_steps_per_second': 0.151, 'epoch': 1.0}\n","  0% 11/3300 [01:24\u003c3:37:32,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:50:59,132 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 23:50:59,137 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:50:59,243 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:50:59,247 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:25\u003c3:32:48,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 23:51:59,885 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:51:59,885 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:51:59,885 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2275726795196533, 'eval_accuracy': 0.6786703601108033, 'eval_runtime': 19.8626, 'eval_samples_per_second': 36.35, 'eval_steps_per_second': 0.151, 'epoch': 2.0}\n","  1% 22/3300 [02:44\u003c3:32:48,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:52:19,753 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 23:52:19,760 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:52:19,870 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:52:19,875 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:45\u003c3:42:49,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 23:53:20,234 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:53:20,234 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:53:20,234 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9882135987281799, 'eval_accuracy': 0.724376731301939, 'eval_runtime': 19.6, 'eval_samples_per_second': 36.837, 'eval_steps_per_second': 0.153, 'epoch': 3.0}\n","  1% 33/3300 [04:05\u003c3:42:49,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:53:39,839 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 23:53:39,846 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:53:39,962 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:53:39,967 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:05\u003c3:31:57,  3.91s/it][INFO|trainer.py:3213] 2023-10-02 23:54:40,654 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:54:40,654 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:54:40,654 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8598122000694275, 'eval_accuracy': 0.7493074792243767, 'eval_runtime': 19.6163, 'eval_samples_per_second': 36.806, 'eval_steps_per_second': 0.153, 'epoch': 4.0}\n","  1% 44/3300 [05:25\u003c3:31:57,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:55:00,276 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 23:55:00,281 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:55:00,386 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:55:00,390 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:55:00,595 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:26\u003c3:38:46,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 23:56:01,237 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:56:01,237 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:56:01,238 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.736294150352478, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 19.6731, 'eval_samples_per_second': 36.7, 'eval_steps_per_second': 0.152, 'epoch': 5.0}\n","  2% 55/3300 [06:46\u003c3:38:46,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:56:20,916 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 23:56:20,922 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:56:21,032 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:56:21,036 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:56:21,260 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:46\u003c3:38:29,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 23:57:21,594 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:57:21,594 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:57:21,595 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8243032693862915, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.7035, 'eval_samples_per_second': 36.643, 'eval_steps_per_second': 0.152, 'epoch': 6.0}\n","  2% 66/3300 [08:06\u003c3:38:29,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:57:41,305 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 23:57:41,310 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:57:41,419 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:57:41,423 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:57:41,632 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:07\u003c3:35:27,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 23:58:41,786 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 23:58:41,786 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 23:58:41,786 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7546952962875366, 'eval_accuracy': 0.7797783933518005, 'eval_runtime': 19.6049, 'eval_samples_per_second': 36.828, 'eval_steps_per_second': 0.153, 'epoch': 7.0}\n","  2% 77/3300 [09:26\u003c3:35:27,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 23:59:01,397 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 23:59:01,403 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 23:59:01,513 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 23:59:01,518 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 23:59:01,736 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:27\u003c3:34:01,  4.00s/it][INFO|trainer.py:3213] 2023-10-03 00:00:01,863 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:00:01,864 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:00:01,864 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7334113121032715, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.5329, 'eval_samples_per_second': 36.963, 'eval_steps_per_second': 0.154, 'epoch': 8.0}\n","  3% 88/3300 [10:46\u003c3:34:01,  4.00s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:00:21,403 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 00:00:21,409 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:00:21,516 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:00:21,520 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:00:21,740 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:47\u003c3:44:41,  4.21s/it][INFO|trainer.py:3213] 2023-10-03 00:01:22,522 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:01:22,522 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:01:22,522 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7200402021408081, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.6356, 'eval_samples_per_second': 36.77, 'eval_steps_per_second': 0.153, 'epoch': 9.0}\n","  3% 99/3300 [12:07\u003c3:44:41,  4.21s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:01:42,163 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 00:01:42,169 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:01:42,277 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:01:42,281 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:01:42,494 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7146, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:08\u003c3:30:29,  3.96s/it][INFO|trainer.py:3213] 2023-10-03 00:02:43,094 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:02:43,095 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:02:43,095 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6893209218978882, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.6335, 'eval_samples_per_second': 36.774, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:27\u003c3:30:29,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:03:02,734 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 00:03:02,739 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:03:02,846 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:03:02,850 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:03:03,058 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:28\u003c3:24:31,  3.86s/it][INFO|trainer.py:3213] 2023-10-03 00:04:03,465 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:04:03,466 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:04:03,466 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7064002156257629, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.6458, 'eval_samples_per_second': 36.751, 'eval_steps_per_second': 0.153, 'epoch': 11.0}\n","  4% 121/3300 [14:48\u003c3:24:31,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:04:23,118 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 00:04:23,124 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:04:23,239 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:04:23,244 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:04:23,476 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:49\u003c3:34:09,  4.06s/it][INFO|trainer.py:3213] 2023-10-03 00:05:24,269 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:05:24,269 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:05:24,269 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6973893642425537, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 19.6019, 'eval_samples_per_second': 36.833, 'eval_steps_per_second': 0.153, 'epoch': 12.0}\n","  4% 132/3300 [16:09\u003c3:34:09,  4.06s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:05:43,877 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 00:05:43,882 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:05:43,991 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:05:43,995 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:05:44,220 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:09\u003c3:23:33,  3.87s/it][INFO|trainer.py:3213] 2023-10-03 00:06:44,554 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:06:44,555 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:06:44,555 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8493256568908691, 'eval_accuracy': 0.7686980609418282, 'eval_runtime': 19.6093, 'eval_samples_per_second': 36.819, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:29\u003c3:23:33,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:07:04,170 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 00:07:04,176 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:07:04,283 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:07:04,287 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:07:04,497 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [18:30\u003c3:34:54,  4.10s/it][INFO|trainer.py:3213] 2023-10-03 00:08:05,036 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:08:05,036 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:08:05,036 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7421062588691711, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 19.6257, 'eval_samples_per_second': 36.789, 'eval_steps_per_second': 0.153, 'epoch': 14.0}\n","  5% 154/3300 [18:49\u003c3:34:54,  4.10s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:08:24,668 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 00:08:24,674 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:08:24,780 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:08:24,784 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:08:25,007 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:50\u003c3:29:46,  4.01s/it][INFO|trainer.py:3213] 2023-10-03 00:09:25,326 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:09:25,326 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:09:25,326 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7790101170539856, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.7169, 'eval_samples_per_second': 36.618, 'eval_steps_per_second': 0.152, 'epoch': 15.0}\n","  5% 165/3300 [20:10\u003c3:29:46,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:09:45,048 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 00:09:45,054 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:09:45,161 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:09:45,165 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:09:45,379 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:10\u003c3:38:35,  4.20s/it][INFO|trainer.py:3213] 2023-10-03 00:10:45,633 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:10:45,634 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:10:45,634 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8333005905151367, 'eval_accuracy': 0.7825484764542936, 'eval_runtime': 19.5564, 'eval_samples_per_second': 36.919, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:30\u003c3:38:35,  4.20s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:11:05,204 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 00:11:05,212 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:11:05,317 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:11:05,322 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:11:05,541 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:31\u003c3:28:29,  4.02s/it][INFO|trainer.py:3213] 2023-10-03 00:12:05,951 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:12:05,951 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:12:05,951 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.712345540523529, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.6325, 'eval_samples_per_second': 36.776, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:50\u003c3:28:29,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:12:25,591 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 00:12:25,597 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:12:25,704 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:12:25,708 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:12:25,915 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:51\u003c3:29:17,  4.05s/it][INFO|trainer.py:3213] 2023-10-03 00:13:26,038 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:13:26,039 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:13:26,039 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7455766201019287, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.6234, 'eval_samples_per_second': 36.793, 'eval_steps_per_second': 0.153, 'epoch': 18.0}\n","  6% 198/3300 [24:10\u003c3:29:17,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:13:45,667 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 00:13:45,673 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:13:45,780 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:13:45,784 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:13:45,993 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2141, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:11\u003c3:19:16,  3.87s/it][INFO|trainer.py:3213] 2023-10-03 00:14:46,502 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:14:46,502 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:14:46,502 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6829440593719482, 'eval_accuracy': 0.8379501385041551, 'eval_runtime': 19.7013, 'eval_samples_per_second': 36.647, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [25:31\u003c3:19:16,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:15:06,211 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 00:15:06,217 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:15:06,338 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:15:06,343 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:15:06,557 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-110] due to args.save_total_limit\n","  7% 220/3300 [26:32\u003c3:16:47,  3.83s/it][INFO|trainer.py:3213] 2023-10-03 00:16:06,865 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:16:06,865 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:16:06,866 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6729613542556763, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 19.5921, 'eval_samples_per_second': 36.852, 'eval_steps_per_second': 0.153, 'epoch': 20.0}\n","  7% 220/3300 [26:51\u003c3:16:47,  3.83s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:16:26,464 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 00:16:26,471 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:16:26,578 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:16:26,582 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:16:26,797 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [27:52\u003c3:16:28,  3.84s/it][INFO|trainer.py:3213] 2023-10-03 00:17:26,945 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:17:26,945 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:17:26,945 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7778957486152649, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.6341, 'eval_samples_per_second': 36.773, 'eval_steps_per_second': 0.153, 'epoch': 21.0}\n","  7% 231/3300 [28:11\u003c3:16:28,  3.84s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:17:46,585 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 00:17:46,590 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:17:46,697 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:17:46,700 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:17:46,914 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-198] due to args.save_total_limit\n","  7% 242/3300 [29:12\u003c3:19:36,  3.92s/it][INFO|trainer.py:3213] 2023-10-03 00:18:47,578 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:18:47,578 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:18:47,579 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7770201563835144, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.7514, 'eval_samples_per_second': 36.554, 'eval_steps_per_second': 0.152, 'epoch': 22.0}\n","  7% 242/3300 [29:32\u003c3:19:36,  3.92s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:19:07,337 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 00:19:07,354 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:19:07,462 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:19:07,466 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:19:07,677 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-209] due to args.save_total_limit\n","  8% 253/3300 [30:33\u003c3:15:54,  3.86s/it][INFO|trainer.py:3213] 2023-10-03 00:20:08,010 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:20:08,011 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:20:08,011 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.745461106300354, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.8569, 'eval_samples_per_second': 36.36, 'eval_steps_per_second': 0.151, 'epoch': 23.0}\n","  8% 253/3300 [30:53\u003c3:15:54,  3.86s/it]\n","100% 3/3 [00:05\u003c00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:20:27,875 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 00:20:27,882 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:20:27,988 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:20:27,992 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:20:28,217 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [31:54\u003c3:18:14,  3.92s/it][INFO|trainer.py:3213] 2023-10-03 00:21:28,804 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:21:28,804 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:21:28,804 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.742904007434845, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.6555, 'eval_samples_per_second': 36.733, 'eval_steps_per_second': 0.153, 'epoch': 24.0}\n","  8% 264/3300 [32:13\u003c3:18:14,  3.92s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:21:48,465 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-03 00:21:48,472 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:21:48,580 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:21:48,585 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:21:48,806 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [33:13\u003c3:24:11,  4.05s/it][INFO|trainer.py:3213] 2023-10-03 00:22:48,625 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:22:48,626 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:22:48,626 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7369587421417236, 'eval_accuracy': 0.8130193905817175, 'eval_runtime': 19.6959, 'eval_samples_per_second': 36.657, 'eval_steps_per_second': 0.152, 'epoch': 25.0}\n","  8% 275/3300 [33:33\u003c3:24:11,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:23:08,329 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-03 00:23:08,335 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:23:08,464 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:23:08,469 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:23:08,693 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [34:34\u003c3:16:18,  3.91s/it][INFO|trainer.py:3213] 2023-10-03 00:24:09,206 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:24:09,206 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:24:09,206 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7051546573638916, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.5823, 'eval_samples_per_second': 36.87, 'eval_steps_per_second': 0.153, 'epoch': 26.0}\n","  9% 286/3300 [34:54\u003c3:16:18,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:24:28,805 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-03 00:24:28,811 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:24:28,923 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:24:28,927 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:24:29,148 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [35:54\u003c3:21:48,  4.03s/it][INFO|trainer.py:3213] 2023-10-03 00:25:29,325 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:25:29,326 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:25:29,326 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7573915719985962, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.6138, 'eval_samples_per_second': 36.811, 'eval_steps_per_second': 0.153, 'epoch': 27.0}\n","  9% 297/3300 [36:14\u003c3:21:48,  4.03s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:25:48,946 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-03 00:25:48,952 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:25:49,061 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:25:49,065 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:25:49,295 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1653, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [37:14\u003c3:22:36,  4.06s/it][INFO|trainer.py:3213] 2023-10-03 00:26:49,710 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:26:49,710 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:26:49,710 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8158057332038879, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.6719, 'eval_samples_per_second': 36.702, 'eval_steps_per_second': 0.153, 'epoch': 28.0}\n","  9% 308/3300 [37:34\u003c3:22:36,  4.06s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:27:09,389 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-03 00:27:09,395 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:27:09,506 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:27:09,511 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:27:09,732 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [38:35\u003c3:15:49,  3.94s/it][INFO|trainer.py:3213] 2023-10-03 00:28:09,856 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:28:09,856 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:28:09,856 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7808286547660828, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.5151, 'eval_samples_per_second': 36.997, 'eval_steps_per_second': 0.154, 'epoch': 29.0}\n"," 10% 319/3300 [38:54\u003c3:15:49,  3.94s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:28:29,378 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-03 00:28:29,383 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:28:29,493 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:28:29,496 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:28:29,712 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-297] due to args.save_total_limit\n"," 10% 330/3300 [39:55\u003c3:18:28,  4.01s/it][INFO|trainer.py:3213] 2023-10-03 00:29:29,875 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:29:29,875 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:29:29,875 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7583415508270264, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.6331, 'eval_samples_per_second': 36.775, 'eval_steps_per_second': 0.153, 'epoch': 30.0}\n"," 10% 330/3300 [40:14\u003c3:18:28,  4.01s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:29:49,514 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-330\n","[INFO|configuration_utils.py:460] 2023-10-03 00:29:49,519 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-330/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:29:49,629 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-330/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:29:49,633 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-330/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:29:49,858 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-308] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 00:29:49,877 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 00:29:49,877 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/checkpoint-220 (score: 0.6729613542556763).\n","{'train_runtime': 2415.1599, 'train_samples_per_second': 347.306, 'train_steps_per_second': 1.366, 'train_loss': 0.34633542263146605, 'epoch': 30.0}\n"," 10% 330/3300 [40:15\u003c6:02:16,  7.32s/it]\n","[INFO|trainer.py:2939] 2023-10-03 00:29:49,934 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/\n","[INFO|configuration_utils.py:460] 2023-10-03 00:29:49,939 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:29:50,044 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:29:50,048 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       30.0\n","  train_loss               =     0.3463\n","  train_runtime            = 0:40:15.15\n","  train_samples_per_second =    347.306\n","  train_steps_per_second   =      1.366\n","[INFO|trainer.py:3213] 2023-10-03 00:29:50,061 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:29:50,061 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:29:50,062 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.75s/it]\n","***** eval metrics *****\n","  epoch                   =       30.0\n","  eval_accuracy           =     0.8283\n","  eval_loss               =      0.673\n","  eval_runtime            = 0:00:19.79\n","  eval_samples_per_second =     36.475\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▆▆▇▇▇▇▇▇▇█▇▇▇▇▇▇██▇█▇▇█▇█▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▂▁▂▂▁▁▁▁▁▂▁▂▂▁▁▁▁▂▂▁▁▁▁▂▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▇▃▃▄▅▃▁▃▃▃▃▃▃▅▂▃▃▄▂▃▅▇▄▄▂▃▄▁▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▂▆▆▅▄▆█▆▆▆▆▆▆▄▇▆▆▄▇▆▄▂▅▅▇▆▅█▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▁▆▆▃▃▆█▆▆▆▆▆▆▃▆▆▆▃▆▆▃▁▆▃▆▆▆█▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.82825\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.67296\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.7944\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.475\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 30.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 330\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1653\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 8.484949299585024e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.34634\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2415.1599\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 347.306\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.366\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlemon-universe-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/tvte08pm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_234928-tvte08pm/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_8/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 8 \\\n","    --seed 8 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GJ6zuVm0Fa-J"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 00:30:21.677432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_003025-gbmi1a5r\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisunderstood-oath-9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/gbmi1a5r\u001b[0m\n","10/03/2023 00:30:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 00:30:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=9,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/runs/Oct03_00-30-26_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=9,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 73747.34it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 337880.98it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 00:30:30,538 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 00:30:30,539 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 00:30:30,541 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 00:30:30,682 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 00:30:30,682 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 00:30:30,686 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 00:30:30,688 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 00:30:32,528 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 00:30:32,529 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 00:30:32,529 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 00:30:32,529 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 00:30:32,529 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 00:30:32,529 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 00:30:32,529 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 00:30:32,530 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-03 00:30:32,531 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:04\u003c3:32:38,  3.88s/it][INFO|trainer.py:3213] 2023-10-03 00:31:36,896 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:31:36,897 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:31:36,897 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5901778936386108, 'eval_accuracy': 0.554016620498615, 'eval_runtime': 20.0654, 'eval_samples_per_second': 35.982, 'eval_steps_per_second': 0.15, 'epoch': 1.0}\n","  0% 11/3300 [01:24\u003c3:32:38,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:31:56,968 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 00:31:56,973 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:31:57,082 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:31:57,086 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:24\u003c3:40:10,  4.03s/it][INFO|trainer.py:3213] 2023-10-03 00:32:57,442 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:32:57,442 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:32:57,442 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2116656303405762, 'eval_accuracy': 0.6745152354570637, 'eval_runtime': 19.6517, 'eval_samples_per_second': 36.74, 'eval_steps_per_second': 0.153, 'epoch': 2.0}\n","  1% 22/3300 [02:44\u003c3:40:10,  4.03s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:33:17,101 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 00:33:17,108 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:33:17,217 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:33:17,220 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:44\u003c3:32:27,  3.90s/it][INFO|trainer.py:3213] 2023-10-03 00:34:17,427 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:34:17,427 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:34:17,428 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.977083146572113, 'eval_accuracy': 0.7409972299168975, 'eval_runtime': 19.8061, 'eval_samples_per_second': 36.454, 'eval_steps_per_second': 0.151, 'epoch': 3.0}\n","  1% 33/3300 [04:04\u003c3:32:27,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:34:37,240 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 00:34:37,246 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:34:37,357 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:34:37,361 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:05\u003c3:43:36,  4.12s/it][INFO|trainer.py:3213] 2023-10-03 00:35:38,008 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:35:38,008 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:35:38,009 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.916896402835846, 'eval_accuracy': 0.7354570637119113, 'eval_runtime': 19.492, 'eval_samples_per_second': 37.041, 'eval_steps_per_second': 0.154, 'epoch': 4.0}\n","  1% 44/3300 [05:24\u003c3:43:36,  4.12s/it]\n","100% 3/3 [00:05\u003c00:00,  2.56s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:35:57,506 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 00:35:57,511 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:35:57,616 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:35:57,620 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:35:57,852 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:25\u003c3:26:22,  3.82s/it][INFO|trainer.py:3213] 2023-10-03 00:36:57,818 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:36:57,818 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:36:57,818 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7731997966766357, 'eval_accuracy': 0.7811634349030471, 'eval_runtime': 19.5535, 'eval_samples_per_second': 36.924, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:44\u003c3:26:22,  3.82s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:37:17,379 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 00:37:17,384 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:37:17,490 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:37:17,494 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:37:17,702 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:45\u003c3:38:08,  4.05s/it][INFO|trainer.py:3213] 2023-10-03 00:38:18,153 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:38:18,153 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:38:18,153 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8655021786689758, 'eval_accuracy': 0.760387811634349, 'eval_runtime': 19.5856, 'eval_samples_per_second': 36.864, 'eval_steps_per_second': 0.153, 'epoch': 6.0}\n","  2% 66/3300 [08:05\u003c3:38:08,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:38:37,755 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 00:38:37,763 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:38:37,871 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:38:37,875 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:38:38,085 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:05\u003c3:34:19,  3.99s/it][INFO|trainer.py:3213] 2023-10-03 00:39:38,478 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:39:38,478 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:39:38,478 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7599145770072937, 'eval_accuracy': 0.7728531855955678, 'eval_runtime': 19.6825, 'eval_samples_per_second': 36.682, 'eval_steps_per_second': 0.152, 'epoch': 7.0}\n","  2% 77/3300 [09:25\u003c3:34:19,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:39:58,166 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 00:39:58,172 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:39:58,284 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:39:58,288 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:39:58,516 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:26\u003c3:29:17,  3.91s/it][INFO|trainer.py:3213] 2023-10-03 00:40:58,746 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:40:58,746 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:40:58,746 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7130407691001892, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.6549, 'eval_samples_per_second': 36.734, 'eval_steps_per_second': 0.153, 'epoch': 8.0}\n","  3% 88/3300 [10:45\u003c3:29:17,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:41:18,409 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 00:41:18,414 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:41:18,520 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:41:18,524 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:41:18,736 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:46\u003c3:23:23,  3.81s/it][INFO|trainer.py:3213] 2023-10-03 00:42:19,015 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:42:19,015 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:42:19,016 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8126251101493835, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.539, 'eval_samples_per_second': 36.952, 'eval_steps_per_second': 0.154, 'epoch': 9.0}\n","  3% 99/3300 [12:06\u003c3:23:23,  3.81s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:42:38,560 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 00:42:38,577 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:42:38,682 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:42:38,686 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:42:38,894 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7125, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:06\u003c3:36:55,  4.08s/it][INFO|trainer.py:3213] 2023-10-03 00:43:38,867 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:43:38,867 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:43:38,867 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.726798415184021, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.5928, 'eval_samples_per_second': 36.85, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:25\u003c3:36:55,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:43:58,465 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 00:43:58,470 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:43:58,578 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:43:58,581 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:43:58,792 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:26\u003c3:25:31,  3.88s/it][INFO|trainer.py:3213] 2023-10-03 00:44:59,277 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:44:59,278 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:44:59,278 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6933242678642273, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.6278, 'eval_samples_per_second': 36.785, 'eval_steps_per_second': 0.153, 'epoch': 11.0}\n","  4% 121/3300 [14:46\u003c3:25:31,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:45:18,911 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 00:45:18,917 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:45:19,029 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:45:19,033 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:45:19,251 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:46\u003c3:26:19,  3.91s/it][INFO|trainer.py:3213] 2023-10-03 00:46:19,365 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:46:19,366 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:46:19,366 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7591747045516968, 'eval_accuracy': 0.7728531855955678, 'eval_runtime': 19.8381, 'eval_samples_per_second': 36.395, 'eval_steps_per_second': 0.151, 'epoch': 12.0}\n","  4% 132/3300 [16:06\u003c3:26:19,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:46:39,209 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 00:46:39,214 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:46:39,334 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:46:39,338 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:46:39,545 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:07\u003c3:28:16,  3.96s/it][INFO|trainer.py:3213] 2023-10-03 00:47:40,011 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:47:40,011 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:47:40,011 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7156893610954285, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.6468, 'eval_samples_per_second': 36.749, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:27\u003c3:28:16,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:47:59,663 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 00:47:59,669 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:47:59,777 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:47:59,781 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:47:59,995 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [18:27\u003c3:28:21,  3.97s/it][INFO|trainer.py:3213] 2023-10-03 00:49:00,014 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:49:00,014 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:49:00,014 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7295992374420166, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.607, 'eval_samples_per_second': 36.824, 'eval_steps_per_second': 0.153, 'epoch': 14.0}\n","  5% 154/3300 [18:47\u003c3:28:21,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:49:19,626 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 00:49:19,632 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:49:19,738 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:49:19,741 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:49:19,950 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:47\u003c3:23:24,  3.89s/it][INFO|trainer.py:3213] 2023-10-03 00:50:20,141 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:50:20,142 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:50:20,142 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6972450613975525, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 19.6609, 'eval_samples_per_second': 36.723, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:07\u003c3:23:24,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:50:39,808 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 00:50:39,814 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:50:39,925 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:50:39,941 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:50:40,168 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:07\u003c3:31:04,  4.05s/it][INFO|trainer.py:3213] 2023-10-03 00:51:40,398 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:51:40,398 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:51:40,398 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7348624467849731, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.5748, 'eval_samples_per_second': 36.884, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:27\u003c3:31:04,  4.05s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:51:59,979 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 00:51:59,985 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:52:00,091 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:52:00,095 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:52:00,304 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [22:28\u003c3:20:59,  3.87s/it][INFO|trainer.py:3213] 2023-10-03 00:53:00,819 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:53:00,819 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:53:00,819 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7349065542221069, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.6413, 'eval_samples_per_second': 36.759, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:47\u003c3:20:59,  3.87s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:53:20,466 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 00:53:20,472 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:53:20,578 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:53:20,582 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:53:20,789 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [23:48\u003c3:21:03,  3.89s/it][INFO|trainer.py:3213] 2023-10-03 00:54:20,886 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:54:20,887 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:54:20,887 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7174408435821533, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.6844, 'eval_samples_per_second': 36.679, 'eval_steps_per_second': 0.152, 'epoch': 18.0}\n","  6% 198/3300 [24:08\u003c3:21:03,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:54:40,577 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 00:54:40,582 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:54:40,687 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:54:40,691 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:54:40,912 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2114, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:08\u003c3:29:46,  4.07s/it][INFO|trainer.py:3213] 2023-10-03 00:55:41,039 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:55:41,039 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:55:41,039 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6845481395721436, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.7107, 'eval_samples_per_second': 36.63, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [25:28\u003c3:29:46,  4.07s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:56:00,756 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 00:56:00,762 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:56:00,872 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:56:00,877 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:56:01,102 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-121] due to args.save_total_limit\n","  7% 220/3300 [26:29\u003c3:23:30,  3.96s/it][INFO|trainer.py:3213] 2023-10-03 00:57:01,708 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:57:01,708 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:57:01,709 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7090651392936707, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.5139, 'eval_samples_per_second': 36.999, 'eval_steps_per_second': 0.154, 'epoch': 20.0}\n","  7% 220/3300 [26:48\u003c3:23:30,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:57:21,227 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 00:57:21,233 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:57:21,339 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:57:21,343 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:57:21,554 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [27:49\u003c3:23:11,  3.97s/it][INFO|trainer.py:3213] 2023-10-03 00:58:22,050 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:58:22,050 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:58:22,050 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6873688697814941, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.5239, 'eval_samples_per_second': 36.98, 'eval_steps_per_second': 0.154, 'epoch': 21.0}\n","  7% 231/3300 [28:09\u003c3:23:11,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 00:58:41,579 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 00:58:41,584 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 00:58:41,691 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 00:58:41,695 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 00:58:41,920 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-198] due to args.save_total_limit\n","  7% 242/3300 [29:09\u003c3:18:45,  3.90s/it][INFO|trainer.py:3213] 2023-10-03 00:59:42,452 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 00:59:42,453 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 00:59:42,453 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7384017705917358, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.6238, 'eval_samples_per_second': 36.792, 'eval_steps_per_second': 0.153, 'epoch': 22.0}\n","  7% 242/3300 [29:29\u003c3:18:45,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:00:02,082 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 01:00:02,088 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:00:02,195 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:00:02,199 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:00:02,410 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [30:29\u003c3:18:10,  3.90s/it][INFO|trainer.py:3213] 2023-10-03 01:01:02,503 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:01:02,503 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:01:02,504 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8756483793258667, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.6845, 'eval_samples_per_second': 36.679, 'eval_steps_per_second': 0.152, 'epoch': 23.0}\n","  8% 253/3300 [30:49\u003c3:18:10,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:01:22,195 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 01:01:22,201 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:01:22,311 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:01:22,316 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:01:22,532 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [31:50\u003c3:27:44,  4.11s/it][INFO|trainer.py:3213] 2023-10-03 01:02:22,856 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:02:22,856 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:02:22,856 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8211181163787842, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.6014, 'eval_samples_per_second': 36.834, 'eval_steps_per_second': 0.153, 'epoch': 24.0}\n","  8% 264/3300 [32:09\u003c3:27:44,  4.11s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:02:42,465 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-03 01:02:42,470 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:02:42,577 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:02:42,581 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:02:42,813 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [33:10\u003c3:31:03,  4.19s/it][INFO|trainer.py:3213] 2023-10-03 01:03:43,426 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:03:43,426 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:03:43,426 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7367834448814392, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.608, 'eval_samples_per_second': 36.822, 'eval_steps_per_second': 0.153, 'epoch': 25.0}\n","  8% 275/3300 [33:30\u003c3:31:03,  4.19s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:04:03,040 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-03 01:04:03,045 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:04:03,151 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:04:03,155 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:04:03,362 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [34:31\u003c3:22:41,  4.04s/it][INFO|trainer.py:3213] 2023-10-03 01:05:03,990 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:05:03,991 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:05:03,991 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7726143002510071, 'eval_accuracy': 0.796398891966759, 'eval_runtime': 19.7569, 'eval_samples_per_second': 36.544, 'eval_steps_per_second': 0.152, 'epoch': 26.0}\n","  9% 286/3300 [34:51\u003c3:22:41,  4.04s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:05:23,754 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-03 01:05:23,759 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:05:23,867 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:05:23,871 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:05:24,084 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [35:52\u003c3:24:23,  4.08s/it][INFO|trainer.py:3213] 2023-10-03 01:06:24,791 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:06:24,791 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:06:24,791 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7560610175132751, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.7503, 'eval_samples_per_second': 36.556, 'eval_steps_per_second': 0.152, 'epoch': 27.0}\n","  9% 297/3300 [36:12\u003c3:24:23,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:06:44,548 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-03 01:06:44,553 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:06:44,661 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:06:44,665 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:06:44,889 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1687, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [37:12\u003c3:20:40,  4.02s/it][INFO|trainer.py:3213] 2023-10-03 01:07:45,166 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:07:45,166 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:07:45,166 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8070854544639587, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.72, 'eval_samples_per_second': 36.613, 'eval_steps_per_second': 0.152, 'epoch': 28.0}\n","  9% 308/3300 [37:32\u003c3:20:40,  4.02s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:08:04,892 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-03 01:08:04,898 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:08:05,006 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:08:05,010 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:08:05,227 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [38:32\u003c3:20:21,  4.03s/it][INFO|trainer.py:3213] 2023-10-03 01:09:05,307 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:09:05,307 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:09:05,308 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6779911518096924, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.509, 'eval_samples_per_second': 37.009, 'eval_steps_per_second': 0.154, 'epoch': 29.0}\n"," 10% 319/3300 [38:52\u003c3:20:21,  4.03s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:09:24,823 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-03 01:09:24,829 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:09:24,936 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:09:24,940 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:09:25,150 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-209] due to args.save_total_limit\n"," 10% 330/3300 [39:53\u003c3:22:30,  4.09s/it][INFO|trainer.py:3213] 2023-10-03 01:10:25,842 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:10:25,842 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:10:25,843 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9752967953681946, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.7104, 'eval_samples_per_second': 36.63, 'eval_steps_per_second': 0.152, 'epoch': 30.0}\n"," 10% 330/3300 [40:13\u003c3:22:30,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:10:45,559 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-330\n","[INFO|configuration_utils.py:460] 2023-10-03 01:10:45,564 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-330/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:10:45,674 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-330/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:10:45,678 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-330/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:10:45,916 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-297] due to args.save_total_limit\n"," 10% 341/3300 [41:13\u003c3:09:03,  3.83s/it][INFO|trainer.py:3213] 2023-10-03 01:11:46,094 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:11:46,095 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:11:46,095 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7934457659721375, 'eval_accuracy': 0.7991689750692521, 'eval_runtime': 19.7013, 'eval_samples_per_second': 36.647, 'eval_steps_per_second': 0.152, 'epoch': 31.0}\n"," 10% 341/3300 [41:33\u003c3:09:03,  3.83s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:12:05,803 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-341\n","[INFO|configuration_utils.py:460] 2023-10-03 01:12:05,809 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-341/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:12:05,915 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-341/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:12:05,919 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-341/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:12:06,129 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-308] due to args.save_total_limit\n"," 11% 352/3300 [42:34\u003c3:19:56,  4.07s/it][INFO|trainer.py:3213] 2023-10-03 01:13:06,549 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:13:06,549 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:13:06,549 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8165206909179688, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.7193, 'eval_samples_per_second': 36.614, 'eval_steps_per_second': 0.152, 'epoch': 32.0}\n"," 11% 352/3300 [42:53\u003c3:19:56,  4.07s/it]\n","100% 3/3 [00:05\u003c00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:13:26,275 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-352\n","[INFO|configuration_utils.py:460] 2023-10-03 01:13:26,282 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-352/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:13:26,398 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-352/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:13:26,403 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-352/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:13:26,630 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-330] due to args.save_total_limit\n"," 11% 363/3300 [43:54\u003c3:11:09,  3.91s/it][INFO|trainer.py:3213] 2023-10-03 01:14:26,590 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:14:26,590 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:14:26,590 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7924833297729492, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.5753, 'eval_samples_per_second': 36.883, 'eval_steps_per_second': 0.153, 'epoch': 33.0}\n"," 11% 363/3300 [44:13\u003c3:11:09,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:14:46,172 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-363\n","[INFO|configuration_utils.py:460] 2023-10-03 01:14:46,178 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-363/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:14:46,285 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-363/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:14:46,289 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-363/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:14:46,514 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-341] due to args.save_total_limit\n"," 11% 374/3300 [45:14\u003c3:03:52,  3.77s/it][INFO|trainer.py:3213] 2023-10-03 01:15:47,360 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:15:47,360 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:15:47,361 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8201479911804199, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.5619, 'eval_samples_per_second': 36.908, 'eval_steps_per_second': 0.153, 'epoch': 34.0}\n"," 11% 374/3300 [45:34\u003c3:03:52,  3.77s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:16:06,930 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-374\n","[INFO|configuration_utils.py:460] 2023-10-03 01:16:06,936 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-374/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:16:07,048 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-374/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:16:07,052 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-374/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:16:07,268 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-352] due to args.save_total_limit\n"," 12% 385/3300 [46:35\u003c3:12:17,  3.96s/it][INFO|trainer.py:3213] 2023-10-03 01:17:07,790 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:17:07,790 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:17:07,790 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7286847233772278, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 19.5959, 'eval_samples_per_second': 36.845, 'eval_steps_per_second': 0.153, 'epoch': 35.0}\n"," 12% 385/3300 [46:54\u003c3:12:17,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:17:27,392 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-385\n","[INFO|configuration_utils.py:460] 2023-10-03 01:17:27,397 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-385/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:17:27,504 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-385/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:17:27,508 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-385/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:17:27,721 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-363] due to args.save_total_limit\n"," 12% 396/3300 [47:55\u003c3:19:20,  4.12s/it][INFO|trainer.py:3213] 2023-10-03 01:18:27,760 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:18:27,760 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:18:27,760 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7629695534706116, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 19.7525, 'eval_samples_per_second': 36.552, 'eval_steps_per_second': 0.152, 'epoch': 36.0}\n"," 12% 396/3300 [48:14\u003c3:19:20,  4.12s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:18:47,519 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-396\n","[INFO|configuration_utils.py:460] 2023-10-03 01:18:47,524 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-396/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:18:47,636 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-396/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:18:47,640 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-396/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:18:47,879 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-374] due to args.save_total_limit\n","{'loss': 0.1491, 'learning_rate': 0.0008787878787878789, 'epoch': 36.36}\n"," 12% 407/3300 [49:15\u003c3:12:45,  4.00s/it][INFO|trainer.py:3213] 2023-10-03 01:19:48,172 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:19:48,172 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:19:48,173 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8344209790229797, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.5402, 'eval_samples_per_second': 36.949, 'eval_steps_per_second': 0.154, 'epoch': 37.0}\n"," 12% 407/3300 [49:35\u003c3:12:45,  4.00s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:20:07,720 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-407\n","[INFO|configuration_utils.py:460] 2023-10-03 01:20:07,725 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-407/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:20:07,831 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-407/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:20:07,835 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-407/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:20:08,043 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-385] due to args.save_total_limit\n"," 13% 418/3300 [50:35\u003c3:07:31,  3.90s/it][INFO|trainer.py:3213] 2023-10-03 01:21:08,344 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:21:08,345 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:21:08,345 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8888373970985413, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.5181, 'eval_samples_per_second': 36.991, 'eval_steps_per_second': 0.154, 'epoch': 38.0}\n"," 13% 418/3300 [50:55\u003c3:07:31,  3.90s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:21:27,868 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-418\n","[INFO|configuration_utils.py:460] 2023-10-03 01:21:27,874 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-418/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:21:27,982 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-418/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:21:27,986 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-418/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:21:28,192 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-396] due to args.save_total_limit\n"," 13% 429/3300 [51:55\u003c3:05:53,  3.88s/it][INFO|trainer.py:3213] 2023-10-03 01:22:28,055 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:22:28,055 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:22:28,055 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.800212562084198, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 19.6374, 'eval_samples_per_second': 36.767, 'eval_steps_per_second': 0.153, 'epoch': 39.0}\n"," 13% 429/3300 [52:15\u003c3:05:53,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:22:47,699 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-429\n","[INFO|configuration_utils.py:460] 2023-10-03 01:22:47,705 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-429/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:22:47,813 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-429/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:22:47,819 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-429/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:22:48,045 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-407] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 01:22:48,059 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 01:22:48,059 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/checkpoint-319 (score: 0.6779911518096924).\n","{'train_runtime': 3135.6014, 'train_samples_per_second': 267.508, 'train_steps_per_second': 1.052, 'train_loss': 0.2993287951240451, 'epoch': 39.0}\n"," 13% 429/3300 [52:15\u003c5:49:44,  7.31s/it]\n","[INFO|trainer.py:2939] 2023-10-03 01:22:48,136 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/\n","[INFO|configuration_utils.py:460] 2023-10-03 01:22:48,142 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:22:48,248 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:22:48,252 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       39.0\n","  train_loss               =     0.2993\n","  train_runtime            = 0:52:15.60\n","  train_samples_per_second =    267.508\n","  train_steps_per_second   =      1.052\n","[INFO|trainer.py:3213] 2023-10-03 01:22:48,266 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:22:48,266 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:22:48,267 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.74s/it]\n","***** eval metrics *****\n","  epoch                   =       39.0\n","  eval_accuracy           =     0.8144\n","  eval_loss               =      0.678\n","  eval_runtime            = 0:00:19.67\n","  eval_samples_per_second =      36.69\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▆▇▆▇▇▇██▇█▇▇███████▇▇█▇█▇█▇▇▇▇███▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▃▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▃▂▁▂▂▂▁▃▂▂▂▂▁▂▂▃▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▅▁▂▂▃▃▂▂▃▅▃▂▃▂▃▃▄▁▁▃▃▂▂▄▄▄▁▄▄▄▂▂▂▄▂▁▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▄█▇▇▆▆▇▇▆▄▆▇▆▇▆▆▅██▆▆▇▇▅▅▅█▅▅▅▇▇▇▅▇█▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆▃█▆▆▄▆█▆▆▃▆▆▆▆▆▄▄██▆▄▆▆▄▄▄█▄▄▄▆▆▆▄██▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.8144\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.67799\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.6782\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.69\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 39.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 429\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00088\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1491\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.1030434089460531e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.29933\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 3135.6014\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 267.508\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.052\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmisunderstood-oath-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/gbmi1a5r\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_003025-gbmi1a5r/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_9/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 9 \\\n","    --seed 9 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jLPgVSaRFd--"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-03 01:23:20.431894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231003_012324-68yzy8sq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstoic-totem-10\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/68yzy8sq\u001b[0m\n","10/03/2023 01:23:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/03/2023 01:23:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=10,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/runs/Oct03_01-23-25_546a1b591bd2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=10,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00\u003c00:00, 47980.22it/s]\n","Resolving data files: 100% 723/723 [00:00\u003c00:00, 184896.15it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:713] 2023-10-03 01:23:29,312 \u003e\u003e loading configuration file cnn-model/config.json\n","[INFO|configuration_utils.py:775] 2023-10-03 01:23:29,313 \u003e\u003e Model config ResNetConfig {\n","  \"_name_or_path\": \"cnn-model/\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"problem_type\": \"single_label_classification\",\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2986] 2023-10-03 01:23:29,315 \u003e\u003e loading weights file cnn-model/pytorch_model.bin\n","[INFO|modeling_utils.py:3771] 2023-10-03 01:23:29,456 \u003e\u003e All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-03 01:23:29,456 \u003e\u003e Some weights of ResNetForImageClassification were not initialized from the model checkpoint at cnn-model/ and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([102, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([102]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:367] 2023-10-03 01:23:29,459 \u003e\u003e loading configuration file cnn-model/preprocessor_config.json\n","[INFO|image_processing_utils.py:419] 2023-10-03 01:23:29,461 \u003e\u003e Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-03 01:23:31,321 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-03 01:23:31,321 \u003e\u003e   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-03 01:23:31,322 \u003e\u003e   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-03 01:23:31,322 \u003e\u003e   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-03 01:23:31,322 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-03 01:23:31,322 \u003e\u003e   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-03 01:23:31,322 \u003e\u003e   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-03 01:23:31,323 \u003e\u003e   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-03 01:23:31,323 \u003e\u003e Automatic Weights \u0026 Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:05\u003c3:38:47,  3.99s/it][INFO|trainer.py:3213] 2023-10-03 01:24:37,043 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:24:37,043 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:24:37,043 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5827454328536987, 'eval_accuracy': 0.5734072022160664, 'eval_runtime': 20.0737, 'eval_samples_per_second': 35.967, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n","  0% 11/3300 [01:25\u003c3:38:47,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:24:57,123 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-03 01:24:57,129 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:24:57,234 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:24:57,238 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:26\u003c3:35:42,  3.95s/it][INFO|trainer.py:3213] 2023-10-03 01:25:57,596 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:25:57,596 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:25:57,597 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3011494874954224, 'eval_accuracy': 0.6703601108033241, 'eval_runtime': 19.6763, 'eval_samples_per_second': 36.694, 'eval_steps_per_second': 0.152, 'epoch': 2.0}\n","  1% 22/3300 [02:45\u003c3:35:42,  3.95s/it]\n","100% 3/3 [00:05\u003c00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:26:17,285 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-03 01:26:17,291 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:26:17,398 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:26:17,402 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:46\u003c3:36:52,  3.98s/it][INFO|trainer.py:3213] 2023-10-03 01:27:17,791 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:27:17,791 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:27:17,791 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9816274046897888, 'eval_accuracy': 0.739612188365651, 'eval_runtime': 19.719, 'eval_samples_per_second': 36.614, 'eval_steps_per_second': 0.152, 'epoch': 3.0}\n","  1% 33/3300 [04:06\u003c3:36:52,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:27:37,516 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-03 01:27:37,522 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:27:37,634 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:27:37,639 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:06\u003c3:36:00,  3.98s/it][INFO|trainer.py:3213] 2023-10-03 01:28:38,146 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:28:38,147 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:28:38,147 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8667404651641846, 'eval_accuracy': 0.7506925207756233, 'eval_runtime': 19.6401, 'eval_samples_per_second': 36.762, 'eval_steps_per_second': 0.153, 'epoch': 4.0}\n","  1% 44/3300 [05:26\u003c3:36:00,  3.98s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:28:57,793 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-03 01:28:57,799 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:28:57,908 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:28:57,912 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:28:58,132 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:26\u003c3:32:48,  3.93s/it][INFO|trainer.py:3213] 2023-10-03 01:29:58,315 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:29:58,315 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:29:58,315 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8076854348182678, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.553, 'eval_samples_per_second': 36.925, 'eval_steps_per_second': 0.153, 'epoch': 5.0}\n","  2% 55/3300 [06:46\u003c3:32:48,  3.93s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:30:17,876 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-03 01:30:17,881 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:30:17,989 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:30:17,993 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:30:18,203 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:47\u003c3:45:25,  4.18s/it][INFO|trainer.py:3213] 2023-10-03 01:31:18,428 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:31:18,428 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:31:18,428 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7898344993591309, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.6756, 'eval_samples_per_second': 36.695, 'eval_steps_per_second': 0.152, 'epoch': 6.0}\n","  2% 66/3300 [08:06\u003c3:45:25,  4.18s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:31:38,109 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-03 01:31:38,115 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:31:38,234 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:31:38,238 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:31:38,447 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:07\u003c3:33:08,  3.97s/it][INFO|trainer.py:3213] 2023-10-03 01:32:38,974 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:32:38,974 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:32:38,974 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.815820574760437, 'eval_accuracy': 0.7825484764542936, 'eval_runtime': 19.5428, 'eval_samples_per_second': 36.945, 'eval_steps_per_second': 0.154, 'epoch': 7.0}\n","  2% 77/3300 [09:27\u003c3:33:08,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:32:58,522 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-03 01:32:58,528 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:32:58,638 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:32:58,642 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:32:58,862 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:27\u003c3:33:49,  3.99s/it][INFO|trainer.py:3213] 2023-10-03 01:33:59,185 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:33:59,185 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:33:59,185 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7826990485191345, 'eval_accuracy': 0.778393351800554, 'eval_runtime': 19.617, 'eval_samples_per_second': 36.805, 'eval_steps_per_second': 0.153, 'epoch': 8.0}\n","  3% 88/3300 [10:47\u003c3:33:49,  3.99s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:34:18,807 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-03 01:34:18,813 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:34:18,921 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:34:18,925 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:34:19,135 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [11:48\u003c3:35:07,  4.03s/it][INFO|trainer.py:3213] 2023-10-03 01:35:19,587 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:35:19,588 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:35:19,588 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7497690320014954, 'eval_accuracy': 0.7936288088642659, 'eval_runtime': 19.6388, 'eval_samples_per_second': 36.764, 'eval_steps_per_second': 0.153, 'epoch': 9.0}\n","  3% 99/3300 [12:07\u003c3:35:07,  4.03s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:35:39,233 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-03 01:35:39,238 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:35:39,345 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:35:39,359 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:35:39,583 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7242, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:08\u003c3:37:16,  4.09s/it][INFO|trainer.py:3213] 2023-10-03 01:36:39,857 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:36:39,857 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:36:39,857 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.801967978477478, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.6098, 'eval_samples_per_second': 36.818, 'eval_steps_per_second': 0.153, 'epoch': 10.0}\n","  3% 110/3300 [13:28\u003c3:37:16,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:36:59,472 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-03 01:36:59,478 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:36:59,584 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:36:59,589 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:36:59,798 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:28\u003c3:29:55,  3.96s/it][INFO|trainer.py:3213] 2023-10-03 01:37:59,990 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:37:59,990 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:37:59,990 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9029848575592041, 'eval_accuracy': 0.7465373961218836, 'eval_runtime': 19.8513, 'eval_samples_per_second': 36.37, 'eval_steps_per_second': 0.151, 'epoch': 11.0}\n","  4% 121/3300 [14:48\u003c3:29:55,  3.96s/it]\n","100% 3/3 [00:05\u003c00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:38:19,848 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-03 01:38:19,854 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:38:19,969 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:38:19,974 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:38:20,199 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [15:49\u003c3:26:36,  3.91s/it][INFO|trainer.py:3213] 2023-10-03 01:39:20,455 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:39:20,456 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:39:20,456 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8101044297218323, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.4934, 'eval_samples_per_second': 37.038, 'eval_steps_per_second': 0.154, 'epoch': 12.0}\n","  4% 132/3300 [16:08\u003c3:26:36,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:39:39,955 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-03 01:39:39,961 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:39:40,068 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:39:40,084 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:39:40,300 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [17:09\u003c3:24:01,  3.88s/it][INFO|trainer.py:3213] 2023-10-03 01:40:40,459 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:40:40,459 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:40:40,460 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7948456406593323, 'eval_accuracy': 0.7867036011080333, 'eval_runtime': 19.6563, 'eval_samples_per_second': 36.731, 'eval_steps_per_second': 0.153, 'epoch': 13.0}\n","  4% 143/3300 [17:28\u003c3:24:01,  3.88s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:41:00,122 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-03 01:41:00,127 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:41:00,237 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:41:00,241 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:41:00,450 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [18:29\u003c3:34:12,  4.09s/it][INFO|trainer.py:3213] 2023-10-03 01:42:01,046 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:42:01,047 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:42:01,047 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8016431927680969, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 19.6824, 'eval_samples_per_second': 36.683, 'eval_steps_per_second': 0.152, 'epoch': 14.0}\n","  5% 154/3300 [18:49\u003c3:34:12,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:42:20,736 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-03 01:42:20,741 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:42:20,849 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:42:20,853 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:42:21,070 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [19:50\u003c3:35:29,  4.12s/it][INFO|trainer.py:3213] 2023-10-03 01:43:21,567 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:43:21,567 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:43:21,567 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6794036626815796, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.6473, 'eval_samples_per_second': 36.748, 'eval_steps_per_second': 0.153, 'epoch': 15.0}\n","  5% 165/3300 [20:09\u003c3:35:29,  4.12s/it]\n","100% 3/3 [00:05\u003c00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:43:41,220 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-03 01:43:41,226 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:43:41,339 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:43:41,344 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:43:41,581 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-99] due to args.save_total_limit\n","  5% 176/3300 [21:10\u003c3:32:16,  4.08s/it][INFO|trainer.py:3213] 2023-10-03 01:44:41,893 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:44:41,894 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:44:41,894 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.820325493812561, 'eval_accuracy': 0.7880886426592798, 'eval_runtime': 19.5592, 'eval_samples_per_second': 36.914, 'eval_steps_per_second': 0.153, 'epoch': 16.0}\n","  5% 176/3300 [21:30\u003c3:32:16,  4.08s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:45:01,459 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-03 01:45:01,464 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:45:01,571 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:45:01,575 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:45:01,793 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-143] due to args.save_total_limit\n","  6% 187/3300 [22:30\u003c3:35:03,  4.14s/it][INFO|trainer.py:3213] 2023-10-03 01:46:02,303 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:46:02,303 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:46:02,303 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.807241678237915, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.5781, 'eval_samples_per_second': 36.878, 'eval_steps_per_second': 0.153, 'epoch': 17.0}\n","  6% 187/3300 [22:50\u003c3:35:03,  4.14s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:46:21,889 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-03 01:46:21,895 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:46:22,001 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:46:22,005 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:46:22,211 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-154] due to args.save_total_limit\n","  6% 198/3300 [23:51\u003c3:31:33,  4.09s/it][INFO|trainer.py:3213] 2023-10-03 01:47:22,617 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:47:22,617 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:47:22,617 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7568027973175049, 'eval_accuracy': 0.8047091412742382, 'eval_runtime': 19.5604, 'eval_samples_per_second': 36.911, 'eval_steps_per_second': 0.153, 'epoch': 18.0}\n","  6% 198/3300 [24:10\u003c3:31:33,  4.09s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:47:42,184 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-03 01:47:42,190 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:47:42,306 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:47:42,310 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:47:42,547 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2159, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:11\u003c3:20:11,  3.89s/it][INFO|trainer.py:3213] 2023-10-03 01:48:42,807 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:48:42,807 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:48:42,807 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7595847249031067, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 19.7016, 'eval_samples_per_second': 36.647, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [25:31\u003c3:20:11,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:49:02,514 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-03 01:49:02,521 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:49:02,635 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:49:02,639 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:49:02,855 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [26:31\u003c3:33:35,  4.16s/it][INFO|trainer.py:3213] 2023-10-03 01:50:02,963 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:50:02,963 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:50:02,963 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8248226642608643, 'eval_accuracy': 0.7853185595567868, 'eval_runtime': 19.5661, 'eval_samples_per_second': 36.9, 'eval_steps_per_second': 0.153, 'epoch': 20.0}\n","  7% 220/3300 [26:51\u003c3:33:35,  4.16s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:50:22,537 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-03 01:50:22,542 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:50:22,652 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:50:22,655 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:50:22,870 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [27:52\u003c3:23:05,  3.97s/it][INFO|trainer.py:3213] 2023-10-03 01:51:23,551 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:51:23,552 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:51:23,552 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8136354684829712, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 19.5286, 'eval_samples_per_second': 36.972, 'eval_steps_per_second': 0.154, 'epoch': 21.0}\n","  7% 231/3300 [28:11\u003c3:23:05,  3.97s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:51:43,087 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-03 01:51:43,092 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:51:43,199 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:51:43,203 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:51:43,429 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [29:12\u003c3:27:40,  4.07s/it][INFO|trainer.py:3213] 2023-10-03 01:52:43,753 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:52:43,754 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:52:43,754 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8770642280578613, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.6117, 'eval_samples_per_second': 36.815, 'eval_steps_per_second': 0.153, 'epoch': 22.0}\n","  7% 242/3300 [29:32\u003c3:27:40,  4.07s/it]\n","100% 3/3 [00:05\u003c00:00,  2.58s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:53:03,372 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-03 01:53:03,378 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:53:03,486 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:53:03,490 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:53:03,702 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [30:32\u003c3:18:36,  3.91s/it][INFO|trainer.py:3213] 2023-10-03 01:54:04,301 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:54:04,301 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:54:04,302 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7849543690681458, 'eval_accuracy': 0.8033240997229917, 'eval_runtime': 19.7615, 'eval_samples_per_second': 36.536, 'eval_steps_per_second': 0.152, 'epoch': 23.0}\n","  8% 253/3300 [30:52\u003c3:18:36,  3.91s/it]\n","100% 3/3 [00:05\u003c00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:54:24,068 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-03 01:54:24,075 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:54:24,191 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:54:24,196 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:54:24,423 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [31:52\u003c3:16:41,  3.89s/it][INFO|trainer.py:3213] 2023-10-03 01:55:24,285 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:55:24,285 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:55:24,285 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7681167721748352, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.5158, 'eval_samples_per_second': 36.996, 'eval_steps_per_second': 0.154, 'epoch': 24.0}\n","  8% 264/3300 [32:12\u003c3:16:41,  3.89s/it]\n","100% 3/3 [00:05\u003c00:00,  2.57s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:55:43,807 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-03 01:55:43,813 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:55:43,922 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:55:43,926 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:55:44,156 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [33:13\u003c3:29:49,  4.16s/it][INFO|trainer.py:3213] 2023-10-03 01:56:44,583 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:56:44,583 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:56:44,583 \u003e\u003e   Batch size = 256\n","\n","  0% 0/3 [00:00\u003c?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.768416702747345, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.8395, 'eval_samples_per_second': 36.392, 'eval_steps_per_second': 0.151, 'epoch': 25.0}\n","  8% 275/3300 [33:33\u003c3:29:49,  4.16s/it]\n","100% 3/3 [00:05\u003c00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-03 01:57:04,430 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-03 01:57:04,436 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:57:04,544 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:57:04,549 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-03 01:57:04,757 \u003e\u003e Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-253] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-03 01:57:04,776 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-03 01:57:04,776 \u003e\u003e Loading best model from drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/checkpoint-165 (score: 0.6794036626815796).\n","{'train_runtime': 2013.5113, 'train_samples_per_second': 416.586, 'train_steps_per_second': 1.639, 'train_loss': 0.38946288022128017, 'epoch': 25.0}\n","  8% 275/3300 [33:33\u003c6:09:08,  7.32s/it]\n","[INFO|trainer.py:2939] 2023-10-03 01:57:04,838 \u003e\u003e Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/\n","[INFO|configuration_utils.py:460] 2023-10-03 01:57:04,844 \u003e\u003e Configuration saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-03 01:57:04,965 \u003e\u003e Model weights saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-03 01:57:04,969 \u003e\u003e Image processor saved in drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       25.0\n","  train_loss               =     0.3895\n","  train_runtime            = 0:33:33.51\n","  train_samples_per_second =    416.586\n","  train_steps_per_second   =      1.639\n","[INFO|trainer.py:3213] 2023-10-03 01:57:04,983 \u003e\u003e ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-03 01:57:04,983 \u003e\u003e   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-03 01:57:04,983 \u003e\u003e   Batch size = 256\n","100% 3/3 [00:05\u003c00:00,  1.77s/it]\n","***** eval metrics *****\n","  epoch                   =       25.0\n","  eval_accuracy           =     0.7978\n","  eval_loss               =     0.6794\n","  eval_runtime            = 0:00:19.78\n","  eval_samples_per_second =     36.499\n","  eval_steps_per_second   =      0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W\u0026B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▆▇▇▇▇▇█▆▇▇▇█▇███▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▃▂▂▂▂▂▂▂▃▂▂▂▁▂▂▂▂▂▂▃▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▄▃▂▃▂▂▃▂▅▁▃▃▃▂▂▂▄▂▁▂▄▁▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆▅▆▇▆▇▆▆▇▄█▆▆▆▇▇▇▅▇█▇▅█▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▅▅▇▇▅█▇▇▇▄█▇▅▇▇▇▇▅▇█▇▅█▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.79778\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.6794\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 19.7814\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.499\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.152\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 25.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 275\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2159\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 7.07079108298752e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.38946\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2013.5113\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 416.586\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.639\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstoic-totem-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnIp102Fabi/runs/68yzy8sq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231003_012324-68yzy8sq/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name cnn-model/ \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/ip102fabi/fabiPandD_outputs_10/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 10 \\\n","    --seed 10 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL1ERu-ZcQUS"},"outputs":[],"source":["# End run\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","provenance":[{"file_id":"17Xhtplfpdqdz1TwipHI60aqky1CvSd1x","timestamp":1693214691473}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}