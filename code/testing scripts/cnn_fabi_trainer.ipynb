{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54620,"status":"ok","timestamp":1698130145645,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"},"user_tz":-120},"id":"sMTacwHyZP-0","outputId":"6e94ef7a-d192-4df0-94b7-fcd5c2092163"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=837a93fa5f071e593c0a8c753491afe4ecf96c525255852c06aa97fb4099a717\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built pathtools\n","Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, dill, responses, multiprocess, huggingface-hub, gitdb, GitPython, wandb, datasets, evaluate\n","Successfully installed GitPython-3.1.40 datasets-2.14.6 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.1 gitdb-4.0.11 huggingface-hub-0.18.0 multiprocess-0.70.15 pathtools-0.1.2 responses-0.18.0 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-mnotvrm5\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-mnotvrm5\n","  Resolved https://github.com/huggingface/transformers to commit 32f799db0d625ec5cf82624ff2604c5a891ebf61\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (0.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0.dev0)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.35.0.dev0)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0.dev0)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (2023.7.22)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.35.0.dev0-py3-none-any.whl size=7883258 sha256=7f3844dfcd14234d5a6fed03c3127677b4b13e723611043fd804e984e0e938be\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3_jr36h6/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n","Successfully built transformers\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.18.0\n","    Uninstalling huggingface-hub-0.18.0:\n","      Successfully uninstalled huggingface-hub-0.18.0\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0.dev0\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.12)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.32.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","Collecting accelerate\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.23.0\n"]}],"source":["!pip install torch torchvision\n","!pip install datasets evaluate wandb\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install wandb\n","!pip install accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xu5wBqMkkf62","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698130173751,"user_tz":-120,"elapsed":28129,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"}},"outputId":"4df84e24-9938-419b-f923-7b511925fb68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"SDb4tIi39i1u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698130263532,"user_tz":-120,"elapsed":89793,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"}},"outputId":"290027b5-c92e-4f56-afab-3f9faf5a5e14"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-24 06:49:33--  https://docs.google.com/uc?export=download&confirm=t&id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF\n","Resolving docs.google.com (docs.google.com)... 142.251.12.100, 142.251.12.113, 142.251.12.139, ...\n","Connecting to docs.google.com (docs.google.com)|142.251.12.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-04-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0np434757mk4tacb53b25hd1qk4s6ad9/1698130125000/11183032721846533402/*/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF?e=download&uuid=e468804a-f9a3-4485-9dbf-4667059cf08c [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-10-24 06:49:33--  https://doc-04-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0np434757mk4tacb53b25hd1qk4s6ad9/1698130125000/11183032721846533402/*/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF?e=download&uuid=e468804a-f9a3-4485-9dbf-4667059cf08c\n","Resolving doc-04-bk-docs.googleusercontent.com (doc-04-bk-docs.googleusercontent.com)... 74.125.200.132, 2404:6800:4003:c00::84\n","Connecting to doc-04-bk-docs.googleusercontent.com (doc-04-bk-docs.googleusercontent.com)|74.125.200.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2246169376 (2.1G) [application/x-zip-compressed]\n","Saving to: ‘fabi-data.zip’\n","\n","fabi-data.zip       100%[===================>]   2.09G  35.2MB/s    in 69s     \n","\n","2023-10-24 06:50:43 (30.9 MB/s) - ‘fabi-data.zip’ saved [2246169376/2246169376]\n","\n","Archive:  fabi-data.zip\n","   creating: data/fabi-data/test/\n","   creating: data/fabi-data/test/0/\n","  inflating: data/fabi-data/test/0/Anaphes nitens 1 Female on Gonipterus egg capsule.jpg  \n","   creating: data/fabi-data/test/1/\n","  inflating: data/fabi-data/test/1/SBush_Blastopsylla occidentalis adult 3.jpg  \n","  inflating: data/fabi-data/test/1/SBush_Blastopsylla occidentalis adult2.jpg  \n","   creating: data/fabi-data/test/10/\n","  inflating: data/fabi-data/test/10/Heavily infested lower branches.jpeg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus 1a.jpg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus 1b.jpg  \n","  inflating: data/fabi-data/test/10/SA Ophelimus galls Selitrichodes female 1a.jpg  \n","   creating: data/fabi-data/test/11/\n","  inflating: data/fabi-data/test/11/DSC_9298.NEF  \n","  inflating: data/fabi-data/test/11/DSC_9300.jpg  \n","  inflating: data/fabi-data/test/11/IMG_3426.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3429.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3469.JPG  \n","  inflating: data/fabi-data/test/11/IMG_3481.JPG  \n","   creating: data/fabi-data/test/12/\n","  inflating: data/fabi-data/test/12/145F5006-CF53-4BFC-81DC-F1EC372DB56D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/1714C5B0-4599-4DC9-BB9A-FB6C2FD8A7CF_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/2DFA84C6-5FB3-431B-B6B2-56A4FC8F4C52_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/322C9D7E-E06B-44B9-B7AE-E36993B32062_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/4C6FB127-A670-40B6-BEF5-A4451AFA5521_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/593AF0C5-A2FA-4062-A271-B5A168B1D497_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/6866D250-37C9-4478-845F-B32662F4028E_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/7C4CA639-5682-49AA-8E90-47BE10C8621C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/C8B432D2-317E-4B5A-A020-F0182775ABEB_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/F05178E4-EAE9-4C5D-96AC-D72451666010_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/F233A982-D15C-4E02-B366-23D6CC03F3BA_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/12/FFCB3191-F78F-489C-B098-6E356E695FAE_1_105_c.jpeg  \n","   creating: data/fabi-data/test/13/\n","  inflating: data/fabi-data/test/13/Male F1 3 April 2015 resized.jpg  \n","  inflating: data/fabi-data/test/13/Male F1 April 2015.jpg  \n","  inflating: data/fabi-data/test/13/Male mystery wasp 8 Oct 2013.jpg  \n","  inflating: data/fabi-data/test/13/Male mystery wasp 9 Oct2013.jpg  \n","  inflating: data/fabi-data/test/13/Mummy April 2015 resized.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Female Oct 2013 5b.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 10.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 11.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Male Oct 2013 9.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 1.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 4.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Newly emerged male 6.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus Parasitised nymph 1.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus pupae underneath 3a.jpg  \n","  inflating: data/fabi-data/test/13/P bliteus two parasitised nymphs 4a.jpg  \n","  inflating: data/fabi-data/test/13/Pb possibly hyper.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02b.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02c adjusted.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr02e.jpg  \n","  inflating: data/fabi-data/test/13/SpObsBr03.jpg  \n","  inflating: data/fabi-data/test/13/SpObsSA05c.jpg  \n","  inflating: data/fabi-data/test/13/SpObsSA09 1.tif  \n","  inflating: data/fabi-data/test/13/SpObsSA09 3.tif  \n","  inflating: data/fabi-data/test/13/SpObsSA09 4.tif  \n","   creating: data/fabi-data/test/14/\n","  inflating: data/fabi-data/test/14/Possibly Quadristichus resized for email.jpg  \n","  inflating: data/fabi-data/test/14/Qmendeli.jpg  \n","  inflating: data/fabi-data/test/14/Qmendeli7.jpg  \n","   creating: data/fabi-data/test/15/\n","  inflating: data/fabi-data/test/15/Selitrichodes female 2.tif  \n","  inflating: data/fabi-data/test/15/Selitrichodes female 7a.jpg  \n","   creating: data/fabi-data/test/16/\n","  inflating: data/fabi-data/test/16/Image_122b.jpg  \n","  inflating: data/fabi-data/test/16/Image_126b.jpg  \n","   creating: data/fabi-data/test/17/\n","  inflating: data/fabi-data/test/17/Close up of browned larvae.jpg  \n","  inflating: data/fabi-data/test/17/Close up of strange eggs.jpg  \n","  inflating: data/fabi-data/test/17/DSCN0198.JPG  \n","  inflating: data/fabi-data/test/17/DSCN1806.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7030.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7031.JPG  \n","  inflating: data/fabi-data/test/17/DSCN7476.JPG  \n","  inflating: data/fabi-data/test/17/Larvae1 no4.jpg  \n","  inflating: data/fabi-data/test/17/Larvae3 no2.jpg  \n","  inflating: data/fabi-data/test/17/Scarred Sirex Larvae 1.jpg  \n","  inflating: data/fabi-data/test/17/Sirex larvae scarred head.jpg  \n","  inflating: data/fabi-data/test/17/Sirex scarred tail 2.jpg  \n","  inflating: data/fabi-data/test/17/Sirex scarred tail.jpg  \n","  inflating: data/fabi-data/test/17/Sirex with nematodes 2.jpg  \n","   creating: data/fabi-data/test/18/\n","  inflating: data/fabi-data/test/18/04EB584A-3B0E-45E4-8300-8FF93C2A455D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/2913384C-B94B-45C7-A87E-225B8873EEAD_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/3E9E2B2A-C617-4E08-B30D-E1C9DBC145D0_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/4DD2AC59-333B-4C2E-9C0B-E75D3765169A_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/6D67302D-F633-454A-AD60-A2B272BBB497_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/7D5BE38F-F614-4A70-97C8-1BAB81289AA1_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/7F257B7B-0B44-4A78-A4F2-2CB1E8799183_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/Adult Middelburg Spondyliaspis wing different angle.jpg  \n","  inflating: data/fabi-data/test/18/Adult Middelburg Spondyliaspis.jpg  \n","  inflating: data/fabi-data/test/18/AFFB4796-1560-4A3B-B72F-E204255C0A85_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/BB7DD3C2-0CB0-45E9-BB24-0DB2DEFCB119_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/Damage 2.jpg  \n","  inflating: data/fabi-data/test/18/E11EB6E7-2C6E-4900-AC81-5AB8E745324D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002212.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002214.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002217.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002218.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002223.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002229.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002231.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002234.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002236.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002237.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002239.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002241.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002245.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002254.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002265.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002271.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002282.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002283.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002286.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002289.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002293.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002295.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002300.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002302.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002307.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002317.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002319.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002335.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002345.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002346.jpeg  \n","  inflating: data/fabi-data/test/18/none-0000002348.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002358.tif  \n","  inflating: data/fabi-data/test/18/none-0000002384.jpg  \n","  inflating: data/fabi-data/test/18/none-0000002391.jpg  \n","  inflating: data/fabi-data/test/18/Spondyliaspis Adult2.jpg  \n","  inflating: data/fabi-data/test/18/Stack3A.jpg  \n","  inflating: data/fabi-data/test/18/StackC.jpg  \n","   creating: data/fabi-data/test/19/\n","  inflating: data/fabi-data/test/19/DSCN0820.JPG  \n","  inflating: data/fabi-data/test/19/DSCN5459.JPG  \n","  inflating: data/fabi-data/test/19/eggs7.jpg  \n","  inflating: data/fabi-data/test/19/IMG_0582.JPG  \n","  inflating: data/fabi-data/test/19/IMG_0638.JPG  \n","  inflating: data/fabi-data/test/19/none-0000001397.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001398.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001399.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001401.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001405.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001425.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001433.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001434.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001436.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001438.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001440.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001452.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001459.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001461.jpeg  \n","  inflating: data/fabi-data/test/19/none-0000001468.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001469.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001472.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001483.jpg  \n","  inflating: data/fabi-data/test/19/none-0000001487.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris female 1.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Male 2.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Male 4.jpg  \n","  inflating: data/fabi-data/test/19/Thaumastocoris Samantha Bush.jpg  \n","   creating: data/fabi-data/test/2/\n","  inflating: data/fabi-data/test/2/SBush_Cleruchoides noackae_ovipositing into Thaumastocoris peregrinus eggs2.jpg  \n","  inflating: data/fabi-data/test/2/SBush_Cleruchoides noackae_with Thaumastocoris peregrinus eggs1.jpg  \n","   creating: data/fabi-data/test/20/\n","  inflating: data/fabi-data/test/20/DSCN1701.JPG  \n","  inflating: data/fabi-data/test/20/DSCN1705.JPG  \n","  inflating: data/fabi-data/test/20/IMG_2308.JPG  \n","   creating: data/fabi-data/test/21/\n","  inflating: data/fabi-data/test/21/311C3CAF-DAD3-4027-BF1E-5C96BCD5A648_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/21/CA4A546E-F2D4-4A2E-B530-A33605E87097_1_105_c.jpeg  \n","   creating: data/fabi-data/test/22/\n","  inflating: data/fabi-data/test/22/Armillaria GM Granados 03.JPG  \n","  inflating: data/fabi-data/test/22/Armillaria GM Granados 06.JPG  \n","  inflating: data/fabi-data/test/22/Armillaria mellea mushrooms Bavarian Germany 2009.jpg  \n","   creating: data/fabi-data/test/23/\n","  inflating: data/fabi-data/test/23/none-0000000728.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000730.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000732.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000736.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000742.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000747.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000760.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000769.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000770.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000776.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000777.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000779.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000781.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000784.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000792.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000796.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000803.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000805.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000809.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000810.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000812.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000813.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000817.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000834.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000845.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000851.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000858.jpeg  \n","  inflating: data/fabi-data/test/23/none-0000000860.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000862.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000876.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000893.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000895.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000916.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000917.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000929.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000932.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000939.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000940.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000944.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000947.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000953.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000956.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000957.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000967.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000975.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000984.jpg  \n","  inflating: data/fabi-data/test/23/none-0000000991.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001001.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001002.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001012.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001015.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001017.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001024.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001025.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001029.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001040.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001050.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001051.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001054.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001065.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001073.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001076.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001078.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001080.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001087.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001091.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001093.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001097.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001104.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001106.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001117.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001120.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001127.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001131.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001135.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001136.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001145.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001146.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001150.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001152.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001154.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001155.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001158.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001159.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001160.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001162.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001163.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001170.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001179.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001183.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001185.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001195.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001203.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001209.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001210.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001213.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001214.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001218.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001224.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001238.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001241.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001246.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001248.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001259.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001266.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001267.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001268.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001269.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001270.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001276.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001281.jpg  \n","  inflating: data/fabi-data/test/23/none-0000001282.jpg  \n","   creating: data/fabi-data/test/24/\n","  inflating: data/fabi-data/test/24/Bacterial blight cause by Pantoea ananatis.png  \n","  inflating: data/fabi-data/test/24/Bacterial blight Pantoea anantis_Izette.JPG  \n","   creating: data/fabi-data/test/25/\n","  inflating: data/fabi-data/test/25/Bacterial wilt caused by _i_Ralstonia solanacearum_i_.png  \n","   creating: data/fabi-data/test/26/\n","  inflating: data/fabi-data/test/26/Botryosphaeria canker internal Neofusicoccum spp_Izette.jpg  \n","   creating: data/fabi-data/test/27/\n","  inflating: data/fabi-data/test/27/Fig. 4. Fruiting structures with abundant mature conidia 3.jpg  \n","   creating: data/fabi-data/test/28/\n","  inflating: data/fabi-data/test/28/Calonectria symptoms on _i_Eucalyptus_i_ leaves.JPG  \n","   creating: data/fabi-data/test/29/\n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados01.JPG  \n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados14.JPG  \n","  inflating: data/fabi-data/test/29/Ceratocystis_Nseleni_KZN_GU_GMGranados17.JPG  \n","   creating: data/fabi-data/test/3/\n","  inflating: data/fabi-data/test/3/6A4F68D7-85B0-4F91-BA7B-E0D906B37970_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/6C6B7899-4F1D-472E-B48B-6BD804C37DA6_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/8531B8E1-3ACC-4EEB-99E1-21A44F1B7765_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/AEE41DD1-E10E-4E3E-B21F-91639A4F475C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/DSCN0361.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0363.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0377.JPG  \n","  inflating: data/fabi-data/test/3/DSCN0803.JPG  \n","  inflating: data/fabi-data/test/3/DSCN1608.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4174.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4191.JPG  \n","  inflating: data/fabi-data/test/3/DSCN4202.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5065.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5492.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5499.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5504.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5515.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5593.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5600.JPG  \n","  inflating: data/fabi-data/test/3/DSCN5605.JPG  \n","  inflating: data/fabi-data/test/3/DSCN6967.JPG  \n","  inflating: data/fabi-data/test/3/DSCN6968.JPG  \n","  inflating: data/fabi-data/test/3/DSCN7320.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8134.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8137.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8146.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8149.JPG  \n","  inflating: data/fabi-data/test/3/DSCN8150.JPG  \n","  inflating: data/fabi-data/test/3/FF36B45F-AC9F-44BD-BA00-4C670A18D626_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/3/IMG_0687.JPG  \n","  inflating: data/fabi-data/test/3/IMG_0699.JPG  \n","  inflating: data/fabi-data/test/3/IMG_0737.JPG  \n","   creating: data/fabi-data/test/30/\n","  inflating: data/fabi-data/test/30/T.zuluensis KZN ZG14 GM Granados 01.JPG  \n","  inflating: data/fabi-data/test/30/T.zuluensis KZN ZG14 GM Granados 06.JPG  \n","   creating: data/fabi-data/test/31/\n","  inflating: data/fabi-data/test/31/D.sapinea GM Granados 06.JPG  \n","  inflating: data/fabi-data/test/31/Diplodia5.JPG  \n","  inflating: data/fabi-data/test/31/Diplodia6.jpg  \n","   creating: data/fabi-data/test/32/\n","  inflating: data/fabi-data/test/32/Fusarium circinatum root rot.JPG  \n","   creating: data/fabi-data/test/33/\n","  inflating: data/fabi-data/test/33/Glycaspis 1.png  \n","   creating: data/fabi-data/test/34/\n","  inflating: data/fabi-data/test/34/_i_Eucalyptus smithii_i_ collar rot.JPG  \n","   creating: data/fabi-data/test/35/\n","  inflating: data/fabi-data/test/35/E.salmonicolor Podocarpus GM Granados 03.JPG  \n","   creating: data/fabi-data/test/36/\n","  inflating: data/fabi-data/test/36/F.circinatum GM Granados 03.JPG  \n","  inflating: data/fabi-data/test/36/F.circinatum GM Granados 05.JPG  \n","   creating: data/fabi-data/test/37/\n","  inflating: data/fabi-data/test/37/Powdery mildew GM Granados01.JPG  \n","   creating: data/fabi-data/test/38/\n","  inflating: data/fabi-data/test/38/Pseudophaeolus KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/test/38/Pseudophaeolus KZN GM Granados 05.JPG  \n","   creating: data/fabi-data/test/39/\n","  inflating: data/fabi-data/test/39/none-0000001288.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001302.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001304.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001312.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001316.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001319.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001320.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001324.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001328.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001330.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001331.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001339.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001347.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001351.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001358.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001363.jpg  \n","  inflating: data/fabi-data/test/39/none-0000001366.jpg  \n","  inflating: data/fabi-data/test/39/Quambalaria GM Granados 01.JPG  \n","   creating: data/fabi-data/test/4/\n","  inflating: data/fabi-data/test/4/Ctenarytaina male 2 SBush.tif  \n","  inflating: data/fabi-data/test/4/Nymph 3 SBush.tif  \n","   creating: data/fabi-data/test/40/\n","  inflating: data/fabi-data/test/40/Fig.1c_FABInews.jpg  \n","   creating: data/fabi-data/test/41/\n","  inflating: data/fabi-data/test/41/Rhizina T Paap_IMG_5700.JPG  \n","   creating: data/fabi-data/test/42/\n","  inflating: data/fabi-data/test/42/Fig2_Conio.JPG  \n","   creating: data/fabi-data/test/43/\n","  inflating: data/fabi-data/test/43/none-0000000007.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000009.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000010.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000011.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000012.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000017.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000018.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000020.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000031.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000035.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000038.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000041.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000046.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000050.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000051.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000052.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000054.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000060.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000073.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000078.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000079.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000087.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000092.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000096.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000105.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000115.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000122.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000130.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000131.jpeg  \n","  inflating: data/fabi-data/test/43/none-0000000133.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000138.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000142.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000144.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000149.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000150.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000152.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000156.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000159.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000164.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000172.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000177.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000183.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000184.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000187.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000189.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000191.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000194.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000216.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000228.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000231.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000232.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000233.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000236.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000237.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000240.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000244.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000254.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000261.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000262.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000264.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000265.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000272.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000278.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000279.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000291.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000292.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000300.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000308.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000311.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000315.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000321.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000323.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000325.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000334.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000342.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000346.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000352.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000355.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000364.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000369.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000378.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000382.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000384.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000386.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000399.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000415.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000422.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000438.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000448.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000451.jpg  \n","  inflating: data/fabi-data/test/43/none-0000000456.jpg  \n","  inflating: data/fabi-data/test/43/T.destructans KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/test/43/T.destructans KZN GM Granados 02.JPG  \n","   creating: data/fabi-data/test/44/\n","  inflating: data/fabi-data/test/44/none-0000000463.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000471.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000474.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000480.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000481.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000489.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000498.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000499.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000503.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000506.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000513.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000518.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000521.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000522.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000524.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000525.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000527.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000528.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000541.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000545.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000546.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000559.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000560.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000562.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000564.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000567.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000571.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000580.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000590.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000616.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000618.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000619.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000631.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000633.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000641.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000642.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000644.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000650.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000653.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000656.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000662.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000665.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000667.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000675.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000678.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000687.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000688.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000689.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000691.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000696.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000703.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000707.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000710.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000723.jpg  \n","  inflating: data/fabi-data/test/44/none-0000000726.jpg  \n","   creating: data/fabi-data/test/45/\n","  inflating: data/fabi-data/test/45/Wattle rust KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/test/45/Wattle rust_Harding_KZN_GMGranados02.JPG  \n","  inflating: data/fabi-data/test/45/Wattle rust_Harding_KZN_GMGranados03.JPG  \n","   creating: data/fabi-data/test/5/\n","  inflating: data/fabi-data/test/5/Pupa and pupal casing.JPG  \n","   creating: data/fabi-data/test/6/\n","  inflating: data/fabi-data/test/6/16360465-C594-4220-9124-037C12A24735_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/2E3AECF5-9282-4CE9-80DC-D3F7064BB10B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/37324B65-6842-4318-9D81-15802953C399_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/45B43995-1E05-433E-B3B9-C7DACAA4C7CB_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/57834C65-77AB-47BD-A83D-FCC9AD90308B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/717C21AA-80AF-499E-8FEC-641DFD88CC01_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/934D8377-ACA2-42A6-8637-0691D4ED4869_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/Adult  male pic1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Adult Gb emerging 2 SBush.jpg  \n","  inflating: data/fabi-data/test/6/B8A1BEA8-679E-4DA3-AEF3-003D35786B9A_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/C7C2B96C-DD06-489C-B8A4-A3532F7C482B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/CD90717B-31F2-48E8-988C-8EB95C74CE13_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/CF9D1E33-7828-4022-A572-31F52447B7D2_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D6A57709-5D66-49B7-BA88-A5EB6D3E728B_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D8BB34D7-7381-4320-98AA-EF74A4B83A1C_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/D93C028E-6E67-46F4-A64E-466A44652C28_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/DD84EA7E-C791-44B8-BCAF-2B90C75BB13D_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/E7E14C4D-E772-4F18-A48E-E2913CDC1813_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/EB170C11-FF6D-4962-82E1-1157B22100BD_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/6/Gb Fifth instar nymph 4 SBush.tif  \n","  inflating: data/fabi-data/test/6/Gb Fifth Instar Nymph 6 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb instar 1 and 2 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb possibly second and third instar 1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/Gb possibly third instar 1 SBush.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002042.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002053.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002054.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002057.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002062.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002064.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002068.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002080.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002083.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002084.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002085.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002088.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002090.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002102.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002108.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002112.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002113.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002117.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002119.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002128.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002129.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002131.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002134.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002135.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002141.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002143.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002144.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002147.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002150.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002153.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002160.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002163.jpeg  \n","  inflating: data/fabi-data/test/6/none-0000002165.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002174.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002179.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002184.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002187.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002188.jpg  \n","  inflating: data/fabi-data/test/6/none-0000002198.jpg  \n","   creating: data/fabi-data/test/7/\n","  inflating: data/fabi-data/test/7/80CFE9F1-7450-4B61-BE80-D93BCD03AE91_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/9A54E69A-0431-4817-BA25-0527E0B252D1_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/BE8504A8-0047-4E20-886A-4802BC642523_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/E5C68486-5E51-49CF-8007-77C05C81C09E_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/7/Gonipterus_adult4.jpg  \n","  inflating: data/fabi-data/test/7/Gonipterus_adult8.jpg  \n","  inflating: data/fabi-data/test/7/Gonipterus_larvae5.jpg  \n","  inflating: data/fabi-data/test/7/IMG_4254.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4255.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4461.JPG  \n","  inflating: data/fabi-data/test/7/IMG_4467.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6114.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6168.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6170.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6199.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6202.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6213.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6217.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6218.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6219.JPG  \n","  inflating: data/fabi-data/test/7/IMG_6227.JPG  \n","  inflating: data/fabi-data/test/7/none-0000001491.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001492.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001496.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001501.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001505.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001506.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001515.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001516.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001523.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001534.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001535.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001538.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001540.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001551.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001556.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001559.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001560.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001563.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001565.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001568.jpeg  \n","  inflating: data/fabi-data/test/7/none-0000001589.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001590.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001595.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001597.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001598.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001602.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001605.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001608.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001611.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001613.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001615.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001623.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001638.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001643.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001648.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001652.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001659.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001660.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001661.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001665.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001667.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001675.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001678.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001681.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001688.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001691.tif  \n","  inflating: data/fabi-data/test/7/none-0000001693.tif  \n","  inflating: data/fabi-data/test/7/none-0000001694.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001696.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001701.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001704.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001706.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001707.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001708.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001711.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001712.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001730.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001751.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001754.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001756.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001762.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001763.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001764.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001766.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001772.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001776.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001777.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001781.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001783.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001794.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001796.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001797.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001801.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001803.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001817.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001826.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001828.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001832.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001835.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001841.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001843.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001854.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001863.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001867.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001875.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001876.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001882.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001884.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001886.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001888.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001905.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001919.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001935.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001943.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001945.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001949.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001950.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001962.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001965.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001973.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001978.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001979.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001982.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001990.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001994.jpg  \n","  inflating: data/fabi-data/test/7/none-0000001997.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002003.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002005.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002006.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002011.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002013.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002017.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002021.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002024.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002028.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002030.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002036.jpg  \n","  inflating: data/fabi-data/test/7/none-0000002037.jpg  \n","   creating: data/fabi-data/test/8/\n","  inflating: data/fabi-data/test/8/Dissected_gall3.jpg  \n","  inflating: data/fabi-data/test/8/DSCN6609.JPG  \n","  inflating: data/fabi-data/test/8/DSCN6616.JPG  \n","  inflating: data/fabi-data/test/8/FB6A01A9-D6DD-4B2D-92B9-4D40FAF11427_1_105_c.jpeg  \n","  inflating: data/fabi-data/test/8/IMG_0335.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0343.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0350.JPG  \n","  inflating: data/fabi-data/test/8/IMG_0358.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1971.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1986.JPG  \n","  inflating: data/fabi-data/test/8/IMG_1988.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2000.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2011.JPG  \n","  inflating: data/fabi-data/test/8/IMG_2013.JPG  \n","  inflating: data/fabi-data/test/8/Leptocybe invasa pupae 1.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe invasa pupae 2.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe male 3.jpg  \n","  inflating: data/fabi-data/test/8/Leptocybe4.JPG_Female_wasp.jpg  \n","  inflating: data/fabi-data/test/8/Oviposition_scars_Linvasa4.jpg  \n","   creating: data/fabi-data/test/9/\n","  inflating: data/fabi-data/test/9/Megastigmus female brown 1b.jpg  \n","  inflating: data/fabi-data/test/9/Megastigmus Female cream 1a.jpg  \n","   creating: data/fabi-data/train/\n","   creating: data/fabi-data/train/0/\n","  inflating: data/fabi-data/train/0/Anitens_female_c.jpg  \n","   creating: data/fabi-data/train/1/\n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentails nymph 2.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis adult.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis adult4.jpg  \n","  inflating: data/fabi-data/train/1/SBush_Blastopsylla occidentalis nymph 1.jpg  \n","   creating: data/fabi-data/train/10/\n","  inflating: data/fabi-data/train/10/Galls of Ophelimus maskelli.jpeg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 1bresized.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 2a.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 2b.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus 3a.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 11.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 3.tif  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 5.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes female 9.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 13.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes males 2.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes males 3.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 4.jpg  \n","  inflating: data/fabi-data/train/10/SA Ophelimus galls Selitrichodes Males 7.jpg  \n","   creating: data/fabi-data/train/11/\n","  inflating: data/fabi-data/train/11/DSC_0136 15 Edited.jpg  \n","  inflating: data/fabi-data/train/11/DSC_0142 21 Edited.jpg  \n","  inflating: data/fabi-data/train/11/DSC_0345.jpeg  \n","  inflating: data/fabi-data/train/11/DSC_9267.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9284.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9300.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9303.NEF  \n","  inflating: data/fabi-data/train/11/DSC_9313.NEF  \n","  inflating: data/fabi-data/train/11/IMG_3374.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3422.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3423.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3424.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3427.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3428.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3431.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3466.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3467.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3468.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3471.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3474.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3477.JPG  \n","  inflating: data/fabi-data/train/11/IMG_3480.JPG  \n","   creating: data/fabi-data/train/12/\n","  inflating: data/fabi-data/train/12/043C2B0E-C3E7-41B9-9BDB-C01CCA3C162D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/07026B37-232F-472A-BD0F-B017A7E5D5FA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0810112E-10C8-4ED0-9631-F61C83B146AD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0C7BA9F2-1A31-402A-A934-5CCE57889440_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/0F76D033-8624-48C3-AA22-C33F924A3BFB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1506CA87-DCA3-47BD-881A-F968632308A0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1B6118D6-9FCE-4E11-9321-B0B7FD32570E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/1C463DFF-26B7-4B58-BA3D-30E3AAFF74F3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/2A8EA66A-47FA-4675-8A0F-A176E733F78F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/2B21E100-F2D3-4E88-A0B8-2DD7CF73D3CB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/3776CD74-D775-4549-9FC4-DE0900DE8D20_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/39A60521-0B4D-4BBE-8FC5-AEFACC231A03_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/40FB2F99-92B1-4B88-B3C0-4B69C7C85FC7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/526A8779-51E4-4F73-BA4F-D91565CC716D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/54C59E62-E004-4BC6-8970-CE5BC29CF418_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/56D75DDE-CECE-4354-AC15-3D34AD513CA4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/5E308468-AE09-40CC-8D27-F09E25F0ACE1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/6BBF1749-8DAB-44F8-A278-4358F7A13615_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/6D4A3BE3-862A-4CE5-BF87-23FB9228E00B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/724605D2-B7D1-44D2-8124-E760342BFC17_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/76839D17-EAD6-4101-BA01-F63B919210AD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/782BB49D-9DBC-483C-9287-4CC6B53D273D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7C3DF9AE-3B0E-4897-AE00-BBD3F399201A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7D1AAA3E-56B4-45C6-B12F-CA100601C8C5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/7EB5BBAF-7087-4C26-9437-57E968136593_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/809A0A24-F9B9-4431-980F-42302DEA701F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/81A74718-6965-4B9B-BC5D-7755B3C0CFD5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A18C6426-F85D-4E8C-8586-756DFC19D558_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A2545607-6019-4246-B72C-4468F0731855_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/A83C99CE-D70E-42FE-9904-F5D4B38D428E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/B1DDF735-DFB9-4554-9B04-EEB62685A0CC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/B510B188-C0A3-4D3F-87EE-113B3184B211_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/BB66BC9C-7344-430C-95A2-FAA61725E76C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/CA051F33-8495-478B-9BA0-ED423A3887E6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/CA94B040-9C6D-4D2E-8DC6-D8CED087FA33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D0B97ABD-EEF2-462D-87F9-B9FB84F31D3E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D1C50C5E-5AA3-42A5-956A-076AB7980730_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/D5FB90EA-0D5F-4846-B752-54F838173840_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/DC9F9526-78EC-4D61-BC13-B957FBAAA3E9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/DEC15466-3268-4182-9714-C3E4E5CE946C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/E12927A8-3986-4FCF-8AEE-89151DA14143_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/E74FD0F1-6A4C-42A4-8D5D-5BF11CBCF057_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/ECF5A653-FF4E-4F7E-860C-1B886D70BA76_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/EF507F58-1071-481D-AB04-F2EF3D2A532F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/F05DDFC7-4F98-4057-B6DC-2D0A75CF6D3D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/12/Larvae in growing tip of pine tree.jpeg  \n","   creating: data/fabi-data/train/13/\n","  inflating: data/fabi-data/train/13/Contents of dissected mummy.tif  \n","  inflating: data/fabi-data/train/13/Contents of dissected mummy2.tif  \n","  inflating: data/fabi-data/train/13/Dark.tif  \n","  inflating: data/fabi-data/train/13/Dark2.tif  \n","  inflating: data/fabi-data/train/13/Dark3.tif  \n","  inflating: data/fabi-data/train/13/Dark4.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 1a.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 1b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 2a.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 2b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 4.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 5a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 5b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 6a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 6b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 7a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 7b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 8a.tif  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 8b.jpg  \n","  inflating: data/fabi-data/train/13/Female mystery wasp 9 middle leg.jpg  \n","  inflating: data/fabi-data/train/13/From Gb mummies.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 2 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 3 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 4 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 5 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 6 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male F1 7 April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 1 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 10 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 11  Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 12 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 2 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 3 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 4 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 5 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 6 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Male mystery wasp 7 Oct 2013.jpg  \n","  inflating: data/fabi-data/train/13/Mummy April 2015.jpg  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus resized autocorrected.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus resized.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus2.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus3.tif  \n","  inflating: data/fabi-data/train/13/Ovipositing SA Psyllaephagus4.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 1a.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 4.tif  \n","  inflating: data/fabi-data/train/13/P bliteus Female Oct 2013 5.tif  \n","  inflating: data/fabi-data/train/13/P bliteus female Oct 2013 8a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus female Oct 2013 8b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 1a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 2b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 4.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 5.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 6.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 7.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Male Oct 2013 8.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus males feeding on honey paper Oct 2013 12.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Males feeding on honey paper Oct2013 13.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 2.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Newly emerged male 5.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph 2a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph 2b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Parasitised nymph and lerp 3.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae 1a.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae 1b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus Pupae 2.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus pupae underneath 3b.jpg  \n","  inflating: data/fabi-data/train/13/P bliteus two parasitized nymphs 4b.jpg  \n","  inflating: data/fabi-data/train/13/Parasitised nymph Oct2013 1a.tif  \n","  inflating: data/fabi-data/train/13/Parasitised nymph Oct2013 1b.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper2.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper3.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper5.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper6.jpg  \n","  inflating: data/fabi-data/train/13/Pb possibly hyper7.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02a.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02c.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02d.jpg  \n","  inflating: data/fabi-data/train/13/SpObsBr02f.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA04.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA04b.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05b edited.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05b.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA05d.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA09 2.tif  \n","  inflating: data/fabi-data/train/13/SpObsSA10 1.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA10 2.jpg  \n","  inflating: data/fabi-data/train/13/SpObsSA10 3.jpg  \n","   creating: data/fabi-data/train/14/\n","  inflating: data/fabi-data/train/14/Possibly Quadristichus 2.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus 3 resized for email.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus autocorrected.jpg  \n","  inflating: data/fabi-data/train/14/Possibly Quadristichus.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli2.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli5.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli6.jpg  \n","  inflating: data/fabi-data/train/14/Qmendeli8.jpg  \n","   creating: data/fabi-data/train/15/\n","  inflating: data/fabi-data/train/15/Copy of SA Ophelimus galls Selitrichodes Males 7.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 3.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 4.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5a.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 5b.jpg  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 6.tif  \n","  inflating: data/fabi-data/train/15/Selitrichodes female 7a resized.jpg  \n","   creating: data/fabi-data/train/16/\n","  inflating: data/fabi-data/train/16/Image_120.jpg  \n","  inflating: data/fabi-data/train/16/Image_120b.jpg  \n","  inflating: data/fabi-data/train/16/Image_122.jpg  \n","  inflating: data/fabi-data/train/16/Image_125.jpg  \n","  inflating: data/fabi-data/train/16/Image_125b.jpg  \n","  inflating: data/fabi-data/train/16/Image_126.jpg  \n","  inflating: data/fabi-data/train/16/Image_127.jpg  \n","  inflating: data/fabi-data/train/16/Image_127b.jpg  \n","   creating: data/fabi-data/train/17/\n","  inflating: data/fabi-data/train/17/0CFB1157-776A-40EB-9023-8BFC1744B30A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/412F67DA-6F11-470D-BF35-4404E7BE6794_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/536385B9-96F1-4710-9D87-DA9AAE64D896_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/BDA98CCA-8E7D-417B-999D-5A5F0BAF1600_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/17/Browned larvae 1.jpg  \n","  inflating: data/fabi-data/train/17/DSCN0183.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0197.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0200.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0203.JPG  \n","  inflating: data/fabi-data/train/17/DSCN0320.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1113.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1114.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1119.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1126.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1442.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1443.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1773.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1783.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1786.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1787.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1790.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1795.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1796.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1799.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1800.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1819.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1825.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1827.JPG  \n","  inflating: data/fabi-data/train/17/DSCN1831.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2292.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2293.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2294.JPG  \n","  inflating: data/fabi-data/train/17/DSCN2304.JPG  \n","  inflating: data/fabi-data/train/17/DSCN6938.JPG  \n","  inflating: data/fabi-data/train/17/DSCN6956.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7018.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7034.JPG  \n","  inflating: data/fabi-data/train/17/DSCN7475.JPG  \n","  inflating: data/fabi-data/train/17/For scale.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no5.jpg  \n","  inflating: data/fabi-data/train/17/Larvae1 no8.jpg  \n","  inflating: data/fabi-data/train/17/Larvae2 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae2 no2.jpg  \n","  inflating: data/fabi-data/train/17/Larvae3 no1.jpg  \n","  inflating: data/fabi-data/train/17/Larvae4 stacked.jpg  \n","  inflating: data/fabi-data/train/17/Scale for Sirex larvae scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex larvae scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex scarred back.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 1.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 3.jpg  \n","  inflating: data/fabi-data/train/17/Sirex with nematodes 4.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs 1.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs 2.jpg  \n","  inflating: data/fabi-data/train/17/Strange eggs with scale.jpg  \n","   creating: data/fabi-data/train/18/\n","  inflating: data/fabi-data/train/18/305175B6-1E20-49BE-AB7D-71FEEFEC553B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/406E93C6-E8FC-4B57-BFE7-F37D04B308C3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/4F1A6E39-3E0A-45F5-991D-556269EA3360_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/5C3B7AB7-6306-4F36-8E38-464431484BE0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/8E9BB382-24AB-4D23-A302-9701614AA090_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/90D2C906-6218-4A8D-886B-BFFA0D34ECC6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Adult Middelburg Spondyliaspis wing.jpg  \n","  inflating: data/fabi-data/train/18/Adult Middelburg Spondyliaspis2.jpg  \n","  inflating: data/fabi-data/train/18/Adult3.jpg  \n","  inflating: data/fabi-data/train/18/Adult4.jpg  \n","  inflating: data/fabi-data/train/18/AE3F0360-0A86-4032-A276-A92FCD48A89E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/B60AF6E2-271A-4CEE-A109-4AB2B67173E4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/B9D76401-6115-4CD8-8DF1-92DEEF573860_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/BDD5449D-7C42-48DB-AB3D-2444F43340AE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Damage 1.jpg  \n","  inflating: data/fabi-data/train/18/DC45DECE-6511-4F0B-8ECB-ADDC0265CC47_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/E2E5D0CA-FE62-4E35-9F00-D1EEF0620D71_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/ECA42586-03C5-4E02-A0ED-A57CF4A14C27_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/EF6AA3C8-9E98-4386-923D-A93977BF475A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Eggs.jpg  \n","  inflating: data/fabi-data/train/18/F1DCE26E-119E-4667-B246-85DBF0EC0FCC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/F45DF301-8453-4CAD-A8F7-47728C6DA032_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/F4829593-B31B-4AF4-98BE-3665CF01FF4E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/FCFBDFFE-4377-4114-AB98-49C6053B261F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/18/Hatched eggs and lerps3.jpg  \n","  inflating: data/fabi-data/train/18/Hatched eggs and lerps5.jpg  \n","  inflating: data/fabi-data/train/18/Individual2 pic1.tif  \n","  inflating: data/fabi-data/train/18/Individual2 pic2.jpg  \n","  inflating: data/fabi-data/train/18/Individual2 pic4.tif  \n","  inflating: data/fabi-data/train/18/Largest lerp1.jpg  \n","  inflating: data/fabi-data/train/18/Medium Lerp.jpg  \n","  inflating: data/fabi-data/train/18/Medium Lerp3.jpg  \n","  inflating: data/fabi-data/train/18/Newly emerged adult.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002206.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002207.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002208.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002209.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002210.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002211.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002213.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002215.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002216.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002219.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002220.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002221.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002222.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002224.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002225.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002226.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002227.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002228.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002230.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002232.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002233.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002235.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002238.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002240.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002242.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002243.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002244.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002246.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002247.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002248.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002249.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002250.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002251.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002252.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002253.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002255.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002256.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002257.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002258.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002259.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002260.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002261.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002262.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002263.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002264.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002266.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002267.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002268.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002269.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002270.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002272.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002273.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002274.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002275.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002276.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002277.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002278.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002279.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002280.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002281.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002284.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002285.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002287.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002288.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002290.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002291.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002292.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002294.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002296.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002297.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002298.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002299.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002301.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002303.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002304.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002305.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002306.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002308.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002309.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002310.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002311.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002312.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002313.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002314.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002315.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002316.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002318.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002320.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002321.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002322.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002323.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002324.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002325.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002326.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002327.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002328.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002329.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002330.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002331.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002332.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002333.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002334.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002336.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002337.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002338.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002339.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002340.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002341.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002342.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002343.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002344.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002347.jpeg  \n","  inflating: data/fabi-data/train/18/none-0000002349.tif  \n","  inflating: data/fabi-data/train/18/none-0000002350.tif  \n","  inflating: data/fabi-data/train/18/none-0000002351.tif  \n","  inflating: data/fabi-data/train/18/none-0000002352.tif  \n","  inflating: data/fabi-data/train/18/none-0000002353.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002354.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002355.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002356.tif  \n","  inflating: data/fabi-data/train/18/none-0000002357.tif  \n","  inflating: data/fabi-data/train/18/none-0000002359.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002360.png  \n","  inflating: data/fabi-data/train/18/none-0000002361.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002362.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002363.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002364.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002365.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002366.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002367.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002368.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002369.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002370.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002371.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002372.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002373.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002374.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002375.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002376.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002377.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002378.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002379.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002380.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002381.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002382.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002383.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002385.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002386.tif  \n","  inflating: data/fabi-data/train/18/none-0000002387.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002388.tif  \n","  inflating: data/fabi-data/train/18/none-0000002389.tif  \n","  inflating: data/fabi-data/train/18/none-0000002390.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002392.jpg  \n","  inflating: data/fabi-data/train/18/none-0000002393.tif  \n","  inflating: data/fabi-data/train/18/Nymph3.jpg  \n","  inflating: data/fabi-data/train/18/Nymph4.jpg  \n","  inflating: data/fabi-data/train/18/NymphLargeStackD.tif  \n","  inflating: data/fabi-data/train/18/Spondyliapsis adult 1a.jpg  \n","  inflating: data/fabi-data/train/18/Spondyliaspis wing different angle.jpg  \n","  inflating: data/fabi-data/train/18/Spondyliaspis wing.jpg  \n","  inflating: data/fabi-data/train/18/Stack with mesurements.jpg  \n","  inflating: data/fabi-data/train/18/Stack2C.jpg  \n","  inflating: data/fabi-data/train/18/Stack3B.jpg  \n","  inflating: data/fabi-data/train/18/Stack3C.jpg  \n","  inflating: data/fabi-data/train/18/StackB.jpg  \n","   creating: data/fabi-data/train/19/\n","  inflating: data/fabi-data/train/19/DSCN0817.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0819.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0821.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0827.JPG  \n","  inflating: data/fabi-data/train/19/DSCN0828.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1595.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1596.JPG  \n","  inflating: data/fabi-data/train/19/DSCN1597.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5446.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5456.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5457.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5458.JPG  \n","  inflating: data/fabi-data/train/19/DSCN5463.JPG  \n","  inflating: data/fabi-data/train/19/eggs3.jpg  \n","  inflating: data/fabi-data/train/19/eggs4.jpg  \n","  inflating: data/fabi-data/train/19/Image14(2).jpg  \n","  inflating: data/fabi-data/train/19/Image21.jpg  \n","  inflating: data/fabi-data/train/19/Image26(2).jpg  \n","  inflating: data/fabi-data/train/19/Image43.jpg  \n","  inflating: data/fabi-data/train/19/Image46.jpg  \n","  inflating: data/fabi-data/train/19/IMG_0584.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0585.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0635.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0640.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0641.JPG  \n","  inflating: data/fabi-data/train/19/IMG_0643.JPG  \n","  inflating: data/fabi-data/train/19/none-0000001395.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001396.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001400.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001402.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001403.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001404.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001406.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001407.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001408.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001409.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001410.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001411.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001412.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001413.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001414.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001415.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001416.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001417.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001418.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001419.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001420.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001421.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001422.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001423.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001424.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001426.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001427.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001428.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001429.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001430.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001431.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001432.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001435.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001437.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001439.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001441.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001442.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001443.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001444.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001445.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001446.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001447.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001448.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001449.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001450.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001451.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001453.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001454.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001455.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001456.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001457.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001458.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001460.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001462.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001463.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001464.jpeg  \n","  inflating: data/fabi-data/train/19/none-0000001465.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001466.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001467.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001470.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001471.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001473.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001474.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001475.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001476.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001477.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001478.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001479.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001480.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001481.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001482.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001484.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001485.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001486.jpg  \n","  inflating: data/fabi-data/train/19/none-0000001488.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 2.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 3.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 4.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris female 7.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 1.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 3.jpg  \n","  inflating: data/fabi-data/train/19/Thaumastocoris Male 5.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph2.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph3 adjusted.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph3.jpg  \n","  inflating: data/fabi-data/train/19/Tperegrinus nymph4.jpg  \n","   creating: data/fabi-data/train/2/\n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_male1.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_male2.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_ovipositing in Thaumastocoris peregrinus eggs1.jpg  \n","  inflating: data/fabi-data/train/2/SBush_Cleruchoides noackae_with Thaumastocoris peregrinus eggs 2.jpg  \n","   creating: data/fabi-data/train/20/\n","  inflating: data/fabi-data/train/20/DSCN1695.JPG  \n","  inflating: data/fabi-data/train/20/DSCN1702.JPG  \n","  inflating: data/fabi-data/train/20/DSCN1708.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2289.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2290.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2314.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2324.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2329.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2341.JPG  \n","  inflating: data/fabi-data/train/20/IMG_2342.JPG  \n","   creating: data/fabi-data/train/21/\n","  inflating: data/fabi-data/train/21/58BF974D-D89C-4CC6-AD15-9A702742AF8E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/A78D97FB-7738-4289-A3DD-9CD6B3F31A12_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/E8D65B8D-BDF5-4DA6-B8A4-3197AE190C51_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/21/FAA4C561-105A-4A9C-9ED5-9B945CD7B9F3_1_105_c.jpeg  \n","   creating: data/fabi-data/train/22/\n","  inflating: data/fabi-data/train/22/Armillaria fuscipes mushrooms Martin.bmp  \n","  inflating: data/fabi-data/train/22/Armillaria GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria internal_Izette.JPG  \n","  inflating: data/fabi-data/train/22/Armillaria mellea clumps mushrooms Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria mellea mushroom clumps Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria mellea mushrooms on stump Bavarian Germany 2009.jpg  \n","  inflating: data/fabi-data/train/22/Armillaria montagnei mushrooms and spores Argentina 2012.jpg  \n","   creating: data/fabi-data/train/23/\n","  inflating: data/fabi-data/train/23/Austropuccinia1.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia2.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia3- ginna.bmp  \n","  inflating: data/fabi-data/train/23/Austropuccinia3.JPG  \n","  inflating: data/fabi-data/train/23/none-0000000729.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000731.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000733.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000734.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000735.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000737.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000738.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000739.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000740.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000741.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000743.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000744.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000745.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000746.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000748.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000749.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000750.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000751.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000752.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000753.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000754.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000755.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000756.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000757.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000758.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000759.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000761.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000762.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000763.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000764.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000765.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000766.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000767.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000768.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000771.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000772.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000773.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000774.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000775.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000778.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000780.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000782.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000783.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000785.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000786.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000787.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000788.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000789.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000790.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000791.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000793.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000794.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000795.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000797.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000798.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000799.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000800.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000801.tif  \n","  inflating: data/fabi-data/train/23/none-0000000802.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000804.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000806.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000807.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000808.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000811.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000814.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000815.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000816.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000818.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000819.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000820.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000821.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000822.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000823.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000824.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000825.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000826.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000827.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000828.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000829.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000830.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000831.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000832.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000833.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000835.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000836.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000837.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000838.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000839.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000840.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000841.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000842.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000843.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000844.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000846.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000847.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000848.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000849.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000850.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000852.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000853.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000854.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000855.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000856.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000857.jpeg  \n","  inflating: data/fabi-data/train/23/none-0000000859.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000861.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000863.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000864.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000865.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000866.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000867.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000868.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000869.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000870.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000871.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000872.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000873.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000874.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000875.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000877.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000878.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000879.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000880.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000881.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000882.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000883.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000884.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000885.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000886.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000887.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000888.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000889.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000890.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000891.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000892.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000894.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000896.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000897.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000898.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000899.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000900.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000901.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000902.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000903.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000904.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000905.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000906.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000907.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000908.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000909.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000910.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000911.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000912.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000913.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000914.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000915.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000918.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000919.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000920.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000921.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000922.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000923.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000924.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000925.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000926.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000927.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000928.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000930.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000931.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000933.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000934.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000935.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000936.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000937.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000938.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000941.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000942.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000943.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000945.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000946.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000948.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000949.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000950.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000951.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000952.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000954.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000955.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000958.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000959.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000960.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000961.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000962.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000963.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000964.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000965.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000966.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000968.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000969.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000970.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000971.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000972.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000973.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000974.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000976.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000977.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000978.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000979.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000980.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000981.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000982.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000983.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000985.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000986.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000987.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000988.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000989.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000990.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000992.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000993.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000994.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000995.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000996.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000997.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000998.jpg  \n","  inflating: data/fabi-data/train/23/none-0000000999.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001000.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001003.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001004.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001005.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001006.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001007.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001008.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001009.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001010.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001011.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001013.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001014.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001016.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001018.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001019.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001020.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001021.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001022.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001023.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001026.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001027.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001028.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001030.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001031.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001032.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001033.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001034.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001035.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001036.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001037.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001038.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001039.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001041.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001042.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001043.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001044.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001045.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001046.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001047.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001048.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001049.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001052.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001053.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001055.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001056.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001057.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001058.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001059.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001060.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001061.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001062.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001063.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001064.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001066.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001067.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001068.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001069.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001070.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001071.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001072.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001074.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001075.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001077.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001079.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001081.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001082.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001083.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001084.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001085.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001086.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001088.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001089.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001090.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001092.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001094.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001095.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001096.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001098.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001099.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001100.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001101.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001102.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001103.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001105.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001107.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001108.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001109.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001110.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001111.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001112.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001113.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001114.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001115.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001116.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001118.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001119.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001121.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001122.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001123.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001124.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001125.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001126.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001128.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001129.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001130.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001132.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001133.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001134.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001137.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001138.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001139.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001140.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001141.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001142.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001143.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001144.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001147.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001148.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001149.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001151.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001153.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001156.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001157.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001161.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001164.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001165.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001166.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001167.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001168.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001169.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001171.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001172.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001173.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001174.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001175.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001176.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001177.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001178.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001180.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001181.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001182.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001184.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001186.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001187.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001188.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001189.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001190.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001191.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001192.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001193.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001194.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001196.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001197.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001198.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001199.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001200.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001201.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001202.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001204.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001205.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001206.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001207.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001208.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001211.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001212.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001215.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001216.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001217.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001219.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001220.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001221.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001222.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001223.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001225.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001226.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001227.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001228.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001229.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001230.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001231.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001232.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001233.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001234.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001235.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001236.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001237.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001239.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001240.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001242.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001243.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001244.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001245.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001247.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001249.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001250.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001251.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001252.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001253.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001254.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001255.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001256.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001257.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001258.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001260.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001261.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001262.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001263.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001264.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001265.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001271.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001272.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001273.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001274.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001275.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001277.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001278.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001279.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001280.jpg  \n","  inflating: data/fabi-data/train/23/none-0000001283.jpg  \n","   creating: data/fabi-data/train/24/\n","  inflating: data/fabi-data/train/24/Bacterial wilt cause by _i_Erwinia psidii_i_.png  \n","  inflating: data/fabi-data/train/24/Blisters on bark caused by _i_Erwinia psidii_i_.png  \n","  inflating: data/fabi-data/train/24/Dieback caused by _i_Erwinia psidii_i_ .png  \n","  inflating: data/fabi-data/train/24/Internal discolouration caused by _i_Erwinia psidii_i_.png  \n","   creating: data/fabi-data/train/25/\n","  inflating: data/fabi-data/train/25/Bacterial exudate of _i_Ralstonia solanacearum_i_.png  \n","  inflating: data/fabi-data/train/25/Bacterial Wilt external Ralstonia pseudosolanacearum_Izette.JPG  \n","  inflating: data/fabi-data/train/25/Bacterial wilt internal Ralstonia pseudosolanacearum_Izette.JPG  \n","  inflating: data/fabi-data/train/25/Discoloured wood caused by _i_Ralstonia solanacearum_i_.png  \n","   creating: data/fabi-data/train/26/\n","  inflating: data/fabi-data/train/26/Botryosphaeria canker external Neofusicoccum spp_Izette.jpg  \n","   creating: data/fabi-data/train/27/\n","  inflating: data/fabi-data/train/27/Fig. 3. A. Botryosphaeria canker on Eucalyptus sp. (F Jami).JPG  \n","  inflating: data/fabi-data/train/27/Fig. 4 Fruiting structures with abundant mature conidia.jpg  \n","  inflating: data/fabi-data/train/27/Fig. 4 Fruiting structures with abundant mature conidia2.jpg  \n","   creating: data/fabi-data/train/28/\n","  inflating: data/fabi-data/train/28/Calonectria leaf blight in _i_Eucalyptus plantation_i_.jpg  \n","   creating: data/fabi-data/train/29/\n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados02.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados03.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados04.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados05.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados07.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados08.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados09.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados10.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados12.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados13.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados15.JPG  \n","  inflating: data/fabi-data/train/29/Ceratocystis_Nseleni_KZN_GU_GMGranados16.JPG  \n","   creating: data/fabi-data/train/3/\n","  inflating: data/fabi-data/train/3/0C5FDA49-F7CC-461A-ACAB-8CEF936B2FE7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/1821C4A9-0060-49BE-875A-D2744FCDF300_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/26436902-1326-48AC-94C1-D954563010E8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/276FEA3C-D13E-485A-A77A-20E7C27AEEB7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/2FE42382-B37D-4589-B270-C18228444240_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/301FB1EE-42D3-4F1F-9557-AFEAC0D5ACED_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/3A380EEF-EDAA-4536-BAD3-53647701CA33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/40DF7AC2-02EB-431F-8187-C14CE5FDF3E5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/44CB9D90-2F90-45AC-BE5B-538DB9C1AF33_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/5BC678E2-0A5E-400D-81D6-EFD53035639A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/684A08F6-7BC3-4457-A9DF-8CACFE2F1AB0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/7666AC27-C65B-4142-95E0-8FFF0A777124_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/8208F627-9878-4D90-B66C-5F86798F4212_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/9F3A3A9A-FB7A-4B9A-B40C-5525472CAF3A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/A0439CCD-AB23-4F24-A2A4-F87F227B439A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/BEBEEB9B-4406-40C0-BC1C-EC8464BA873F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/D7FD66DA-BC1C-451F-8AAA-D8A5FAF8F23A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/DE9DC524-5B08-4B39-BE83-C1B056B79B2C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/DSC000012.JPG  \n","  inflating: data/fabi-data/train/3/DSC00004.JPG  \n","  inflating: data/fabi-data/train/3/DSC00005.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0028.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0055.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0056.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0095.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0097.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0101.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0117.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0364.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0395.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0399.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0802.JPG  \n","  inflating: data/fabi-data/train/3/DSCN0984.JPG  \n","  inflating: data/fabi-data/train/3/DSCN1332.JPG  \n","  inflating: data/fabi-data/train/3/DSCN1580.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4175.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4178.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4181.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4182.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4185.JPG  \n","  inflating: data/fabi-data/train/3/DSCN4204.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5066.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5070.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5071.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5073.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5490.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5494.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5500.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5501.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5503.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5507.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5508.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5509.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5511.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5579.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5581.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5584.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5585.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5586.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5587.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5590.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5592.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5594.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5595.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5596.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5598.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5602.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5603.JPG  \n","  inflating: data/fabi-data/train/3/DSCN5604.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6962.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6963.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6964.JPG  \n","  inflating: data/fabi-data/train/3/DSCN6975.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7292.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7294.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7295.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7296.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7300.JPG  \n","  inflating: data/fabi-data/train/3/DSCN7322.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8135.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8136.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8139.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8140.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8147.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8148.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8152.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8153.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8155.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8157.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8159.JPG  \n","  inflating: data/fabi-data/train/3/DSCN8160.JPG  \n","  inflating: data/fabi-data/train/3/E02A74C2-39AE-4ADA-B062-448A5F21DD79_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/E6979849-8FE9-4D9B-86D4-BB6CE1C225C2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/FC0B348D-0788-4094-B2AA-68CD3DA014C2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/3/IMG_0674.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0675.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0677.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0678.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0679.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0680.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0681.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0685.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0686.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0696.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0697.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0698.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0700.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0701.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0702.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0711.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0712.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0713.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0714.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0715.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0716.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0717.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0718.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0721.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0723.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0725.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0726.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0727.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0728.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0735.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0740.JPG  \n","  inflating: data/fabi-data/train/3/IMG_0742.JPG  \n","  inflating: data/fabi-data/train/3/larvae and tunnels.JPG  \n","   creating: data/fabi-data/train/30/\n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/30/T.zuluensis KZN ZG14 GM Granados 07.JPG  \n","   creating: data/fabi-data/train/31/\n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 05.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 07.JPG  \n","  inflating: data/fabi-data/train/31/D.sapinea GM Granados 08.JPG  \n","  inflating: data/fabi-data/train/31/Diplodia1 Ginna.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia1.JPG  \n","  inflating: data/fabi-data/train/31/Diplodia2.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia3.bmp  \n","  inflating: data/fabi-data/train/31/Diplodia4.JPG  \n","   creating: data/fabi-data/train/32/\n","  inflating: data/fabi-data/train/32/Fusarium circinatum collar rot.jpg  \n","   creating: data/fabi-data/train/33/\n","  inflating: data/fabi-data/train/33/Glycaspis 2.png  \n","  inflating: data/fabi-data/train/33/Glycaspis 3.png  \n","  inflating: data/fabi-data/train/33/P bliteus 1.png  \n","  inflating: data/fabi-data/train/33/P bliteus 2.png  \n","   creating: data/fabi-data/train/34/\n","  inflating: data/fabi-data/train/34/_i_Acacia mearnsii_i_ gummosis.JPG  \n","   creating: data/fabi-data/train/35/\n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/35/E.salmonicolor Podocarpus GM Granados 05.JPG  \n","   creating: data/fabi-data/train/36/\n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/36/F.circinatum GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/36/Fuarium circinatum caramel coloured lessions and cankers.JPG  \n","  inflating: data/fabi-data/train/36/Fusarium circinatum resin exudation down main stem.JPG  \n","   creating: data/fabi-data/train/37/\n","  inflating: data/fabi-data/train/37/Powdery mildew GM Granados02.JPG  \n","  inflating: data/fabi-data/train/37/Powdery mildew GM Granados03.JPG  \n","   creating: data/fabi-data/train/38/\n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/38/Pseudophaeolus KZN GM Granados 06.JPG  \n","   creating: data/fabi-data/train/39/\n","  inflating: data/fabi-data/train/39/none-0000001284.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001285.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001286.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001287.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001289.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001290.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001291.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001292.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001293.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001294.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001295.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001296.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001297.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001298.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001299.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001300.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001301.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001303.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001305.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001306.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001307.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001308.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001309.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001310.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001311.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001313.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001314.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001315.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001317.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001318.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001321.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001322.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001323.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001325.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001326.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001327.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001329.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001332.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001333.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001334.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001335.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001336.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001337.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001338.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001340.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001341.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001342.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001343.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001344.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001345.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001346.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001348.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001349.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001350.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001352.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001353.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001354.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001355.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001356.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001357.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001359.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001360.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001361.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001362.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001364.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001365.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001367.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001368.jpg  \n","  inflating: data/fabi-data/train/39/none-0000001369.jpg  \n","  inflating: data/fabi-data/train/39/Quambalaria GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/39/Quambalaria GM Granados 03.JPG  \n","   creating: data/fabi-data/train/4/\n","  inflating: data/fabi-data/train/4/Ctenarytaina male 1 SBush.tif  \n","  inflating: data/fabi-data/train/4/Ctenarytaina male 3 SBush.tif  \n","  inflating: data/fabi-data/train/4/Ctenarytaina male resized SBush.tif  \n","  inflating: data/fabi-data/train/4/Nymph 1 SBush.tif  \n","  inflating: data/fabi-data/train/4/Nymph 2 SBush.tif  \n","   creating: data/fabi-data/train/40/\n","  inflating: data/fabi-data/train/40/Fig.1b_FABInews.jpg  \n","  inflating: data/fabi-data/train/40/Fig.1e_Greyling_2016.png  \n","  inflating: data/fabi-data/train/40/Fig1.e_Greyling_2016.png  \n","  inflating: data/fabi-data/train/40/Fig1d_Dell_2008.png  \n","   creating: data/fabi-data/train/41/\n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5706.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5707.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5715.JPG  \n","  inflating: data/fabi-data/train/41/Rhizina T Paap_IMG_5718.JPG  \n","   creating: data/fabi-data/train/42/\n","  inflating: data/fabi-data/train/42/Fig1_Conio.JPG  \n","   creating: data/fabi-data/train/43/\n","  inflating: data/fabi-data/train/43/none-0000000001.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000002.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000003.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000004.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000005.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000006.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000008.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000013.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000014.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000015.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000016.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000019.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000021.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000022.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000023.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000024.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000025.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000026.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000027.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000028.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000029.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000030.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000032.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000033.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000034.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000036.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000037.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000039.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000040.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000042.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000043.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000044.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000045.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000047.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000048.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000049.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000053.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000055.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000056.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000057.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000058.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000059.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000061.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000062.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000063.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000064.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000065.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000066.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000067.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000068.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000069.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000070.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000071.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000072.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000074.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000075.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000076.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000077.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000080.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000081.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000082.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000083.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000084.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000085.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000086.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000088.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000089.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000090.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000091.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000093.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000094.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000095.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000097.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000098.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000099.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000100.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000101.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000102.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000103.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000104.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000106.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000107.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000108.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000109.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000110.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000111.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000112.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000113.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000114.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000116.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000117.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000118.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000119.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000120.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000121.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000123.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000124.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000125.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000126.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000127.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000128.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000129.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000132.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000134.jpeg  \n","  inflating: data/fabi-data/train/43/none-0000000135.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000136.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000137.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000139.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000140.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000141.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000143.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000145.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000146.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000147.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000148.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000151.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000153.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000154.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000155.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000157.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000158.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000160.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000161.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000162.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000163.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000165.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000166.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000167.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000168.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000169.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000170.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000171.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000173.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000174.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000175.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000176.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000178.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000179.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000180.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000181.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000182.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000185.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000186.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000188.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000190.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000192.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000193.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000195.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000196.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000197.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000198.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000199.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000200.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000201.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000202.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000203.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000204.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000205.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000206.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000207.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000208.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000209.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000210.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000211.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000212.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000213.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000214.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000215.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000217.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000218.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000219.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000220.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000221.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000222.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000223.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000224.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000225.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000226.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000227.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000229.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000230.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000234.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000235.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000238.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000239.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000241.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000242.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000243.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000245.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000246.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000247.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000248.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000249.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000250.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000251.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000252.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000253.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000255.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000256.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000257.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000258.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000259.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000260.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000263.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000266.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000267.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000268.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000269.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000270.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000271.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000273.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000274.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000275.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000276.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000277.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000280.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000281.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000282.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000283.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000284.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000285.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000286.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000287.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000288.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000289.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000290.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000293.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000294.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000295.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000296.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000297.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000298.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000299.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000301.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000302.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000303.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000304.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000305.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000306.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000307.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000309.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000310.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000312.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000313.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000314.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000316.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000317.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000318.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000319.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000320.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000322.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000324.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000326.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000327.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000328.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000329.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000330.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000331.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000332.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000333.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000335.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000336.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000337.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000338.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000339.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000340.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000341.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000343.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000344.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000345.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000347.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000348.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000349.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000350.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000351.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000353.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000354.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000356.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000357.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000358.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000359.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000360.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000361.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000362.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000363.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000365.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000366.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000367.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000368.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000370.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000371.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000372.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000373.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000374.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000375.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000376.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000377.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000379.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000380.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000381.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000383.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000385.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000387.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000388.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000389.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000390.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000391.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000392.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000393.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000394.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000395.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000396.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000397.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000398.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000400.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000401.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000402.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000403.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000404.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000405.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000406.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000407.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000408.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000409.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000410.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000411.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000412.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000413.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000414.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000416.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000417.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000418.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000419.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000420.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000421.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000423.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000424.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000425.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000426.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000427.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000428.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000429.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000430.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000431.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000432.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000433.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000434.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000435.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000436.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000437.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000439.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000440.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000441.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000442.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000443.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000444.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000445.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000446.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000447.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000449.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000450.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000452.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000453.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000454.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000455.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000457.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000458.jpg  \n","  inflating: data/fabi-data/train/43/none-0000000459.jpg  \n","  inflating: data/fabi-data/train/43/T.destructans KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/43/T.destructans KZN GM Granados 04.JPG  \n","  inflating: data/fabi-data/train/43/Teratoshaeria destructans.jpg  \n","   creating: data/fabi-data/train/44/\n","  inflating: data/fabi-data/train/44/none-0000000460.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000461.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000462.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000464.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000465.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000466.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000467.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000468.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000469.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000470.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000472.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000473.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000475.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000476.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000477.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000478.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000479.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000482.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000483.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000484.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000485.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000486.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000487.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000488.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000490.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000491.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000492.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000493.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000494.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000495.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000496.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000497.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000500.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000501.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000502.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000504.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000505.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000507.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000508.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000509.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000510.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000511.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000512.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000514.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000515.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000516.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000517.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000519.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000520.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000523.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000526.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000529.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000530.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000531.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000532.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000533.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000534.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000535.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000536.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000537.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000538.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000539.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000540.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000542.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000543.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000544.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000547.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000548.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000549.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000550.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000551.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000552.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000553.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000554.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000555.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000556.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000557.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000558.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000561.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000563.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000565.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000566.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000568.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000569.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000570.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000572.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000573.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000574.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000575.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000576.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000577.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000578.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000579.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000581.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000582.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000583.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000584.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000585.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000586.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000587.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000588.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000589.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000591.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000592.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000593.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000594.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000595.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000596.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000597.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000598.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000599.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000600.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000601.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000602.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000603.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000604.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000605.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000606.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000607.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000608.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000609.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000610.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000611.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000612.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000613.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000614.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000615.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000617.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000620.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000621.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000622.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000623.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000624.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000625.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000626.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000627.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000628.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000629.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000630.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000632.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000634.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000635.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000636.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000637.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000638.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000639.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000640.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000643.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000645.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000646.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000647.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000648.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000649.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000651.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000652.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000654.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000655.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000657.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000658.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000659.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000660.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000661.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000663.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000664.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000666.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000668.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000669.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000670.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000671.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000672.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000673.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000674.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000676.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000677.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000679.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000680.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000681.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000682.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000683.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000684.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000685.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000686.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000690.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000692.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000693.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000694.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000695.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000697.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000698.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000699.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000700.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000701.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000702.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000704.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000705.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000706.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000708.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000709.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000711.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000712.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000713.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000714.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000715.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000716.jpeg  \n","  inflating: data/fabi-data/train/44/none-0000000717.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000718.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000719.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000720.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000721.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000722.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000724.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000725.jpg  \n","  inflating: data/fabi-data/train/44/none-0000000727.jpg  \n","  inflating: data/fabi-data/train/44/T.nubilosa KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/44/T.nubilosa KZN GM Granados 02.JPG  \n","  inflating: data/fabi-data/train/44/T.nubilosa_Bulwer_KZN_GMGranados01.JPG  \n","   creating: data/fabi-data/train/45/\n","  inflating: data/fabi-data/train/45/1_Wattle rust.jpg  \n","  inflating: data/fabi-data/train/45/2_Wattle rust.jpg  \n","  inflating: data/fabi-data/train/45/3_Wattle rust.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust KZN GM Granados 01.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust KZN GM Granados 03.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados01.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados04.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados05.JPG  \n","  inflating: data/fabi-data/train/45/Wattle rust_Harding_KZN_GMGranados06.JPG  \n","   creating: data/fabi-data/train/5/\n","  inflating: data/fabi-data/train/5/DSCN1541.JPG  \n","  inflating: data/fabi-data/train/5/DSCN1549.JPG  \n","  inflating: data/fabi-data/train/5/Euproctis cocoon B.Hurley.JPG  \n","  inflating: data/fabi-data/train/5/Euproctis terminalis B. Hurley.JPG  \n","   creating: data/fabi-data/train/6/\n","  inflating: data/fabi-data/train/6/008BC3B2-28D6-45B9-A396-8822E5E3A58B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/067988C4-4AFC-4DA3-A5D5-687876C4B4A9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/0F8B2F51-6A54-4A64-BADC-CC0EFFE0934A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/10B0A8D6-8FD2-406B-9F34-9AD5A56C9420_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/17F73DE4-F546-458F-80F1-DCC7499D37BD_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/1C2840B7-C814-4172-9B3C-5BB1B7BC4757_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/1DF0306F-7101-4631-A7CE-8DF7119E0FA1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/25F81054-30FB-4ABA-A07F-40605F97A934_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/2A87BA12-D302-40C3-8569-094A73128648_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/31C7381C-E429-48C3-A6F6-9E6103B86052_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/36CFC62C-3086-4F07-9C8E-714F2384BDE9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/37A1A534-49F9-4B08-B57C-9B5D536094B5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3A63507C-1ABB-4F55-B8BB-7F2BBC9B6400_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3A83DF2D-05D0-431F-ACEA-40555D61C20A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/3FC127DE-2627-46ED-8C67-D906B08A6C86_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/406B6686-9138-49E1-9773-318D220E51C0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/410388D7-E42A-4913-809A-2B8BC0442EC6_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/41C7A748-8639-4342-906B-98AC55BE0ED8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/44DF8EC3-6AFC-4946-8A9B-40EA3AC95B8B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/4592FF6A-E4DA-4C36-BAF8-62DAC82F88CA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/47F54696-67B1-4D5D-9933-6178C2488712_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/4900B533-0FA9-4FC5-AEE3-D832A9976AAC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/52A34225-1C60-4680-B04D-B1792C38816F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/55A7DDA7-36BE-4A98-8AAB-FD031C8085AA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5B4D5D9D-D3AE-43C6-81A3-82392A6E3248_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5B88AFAE-D521-4BF8-ADC4-51FC951A9F25_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5C639684-900F-4D05-B13F-2843921F3055_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/5D49D5A6-D1E5-4019-9651-82B6AA8EA3A1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/653DE0A2-63F5-420A-B4D0-83E2618DF34A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6699EB04-A667-40C4-9534-F78893A6DC1F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6A28C7FA-3EAE-440C-B3CE-5B6116C6701A_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6D1B7897-BB90-4641-925F-EE0C4F6A26C4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6D6AE712-2E5C-49D2-80E6-002D14BCAF3D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/6ED38195-2E97-4A53-A527-51988D543EA7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/714D13EC-D9E5-4502-BFA6-80A20D8F06EB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/792CC3A1-F25E-407F-AA5B-0E3232483F07_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/7F123EB0-6C9F-48AF-A128-0A8A85B79D09_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/85A96CB6-A4EC-4A6A-B5C9-8C8094EF41A5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/861FFB9D-5531-4642-8952-A54A4BF2B663_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8E797FD5-764A-4106-8397-25B71F84731C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8F62CAF6-E978-4A22-93AC-27B1B280EF45_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/8FAA8E86-0973-4580-A425-312E7AB2994C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9101F939-105B-40DA-BA93-2CFEB3FA403D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/924D457F-2FD6-4267-8556-1DE0511C80EE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9289C91F-6C77-4A79-83DE-497EF75195C4_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/98E942F6-E83D-4FF1-ABFF-B252C6EA3FBB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/98F72AC5-B862-4C0F-A210-56EE67431AFE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9E0C2B5F-8283-471F-BBFF-9B28A568896D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/9E79BC7E-504C-4EBD-9371-9512DB8D8CA7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A06EDB66-69B9-43B5-9122-E2D069F77381_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A1C66A43-6445-4541-A771-E908E8275172_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A2A5E574-BE8F-429C-891F-56EC825E6B6F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/A475D807-1F06-45BF-9ABC-EDE7B2E68DEA_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/ABCDBE90-942B-423B-A0D3-75FFFC366940_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/AD1C0931-F655-43B8-A6BE-006D76310911_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 1 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 6 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 7 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult Gb emerging 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult male and female pic2SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult pic1SBush.jpg  \n","  inflating: data/fabi-data/train/6/Adult pic2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/B3272EC8-1856-4251-9BA2-4D382451CA1E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/B4DCC730-0B2F-41AB-A322-F7D9A8AC1E20_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C03B96D6-3E0D-4D79-B5A4-13DE12EB5314_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C04F90D5-BD60-40D8-AAB1-843738491FA0_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C05CEC9E-4080-4DFE-9EC4-1F584E249B02_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C7F56B3D-6E73-4A02-B284-18CA7174AC8E_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/C8BE8542-DA18-4A7B-A04C-4D9C1799AF14_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/CF814A5F-819F-4848-B4C4-3B7DA3A1AAF8_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/D0C4528B-7057-454D-831B-9566E339C8C7_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/DCC84013-65D4-4D77-888E-B80221B916C3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/DCDAFC70-5C5E-4FCF-B2EF-6EA4FAD852B3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/E0DB2F7F-F9DD-49A3-A7ED-45818B8C1D4F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/E93FCE5F-27C0-4B95-8DA2-586C96728BCE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F11737D3-4F4A-4339-9310-1FC4ABD59BB9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F17502FC-4180-4B66-A99F-E57D19A8B593_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F24245A6-D94E-4867-8892-F47891568EE3_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F302C29D-0144-4765-9C00-09AAD371B55D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F4CB8B2E-93B4-4F99-B6C9-E997F8B29B38_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F4E1557D-7413-473D-AD1E-137CCA4788E5_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F5B4CFE7-E58E-4EF7-841D-8DC4CDAAEADB_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/F90135E3-52DE-4D0B-B062-E1EDCB1DFA14_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/FDC8CA73-FB4F-4F94-A0F5-471B85D54A4B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/6/Female 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 6 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 7 two females SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Female 9 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 1SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 1 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb eggs and all instars 3 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth instar nymph 2 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb Fifth instar nymph 3 SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb fifth instar nymph 4 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 7 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb Fifth Instar Nymph 8 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 1 to 4SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 1 to 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 2 to 3 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb instar 4 to 5 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb male 1b SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb nymph building lerp SBush.tif  \n","  inflating: data/fabi-data/train/6/Gb nymph inside lerp SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly second instar SBush(1).jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly second instar SBush.jpg  \n","  inflating: data/fabi-data/train/6/Gb possibly third Instar 2 SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis  Male 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis Female 1a SBush.jpg  \n","  inflating: data/fabi-data/train/6/Glycaspis SBush.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002041.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002043.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002044.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002045.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002046.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002047.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002048.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002049.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002050.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002051.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002052.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002055.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002056.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002058.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002059.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002060.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002061.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002063.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002065.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002066.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002067.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002069.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002070.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002071.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002072.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002073.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002074.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002075.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002076.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002077.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002078.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002079.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002081.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002082.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002086.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002087.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002089.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002091.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002092.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002093.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002094.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002095.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002096.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002097.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002098.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002099.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002100.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002101.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002103.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002104.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002105.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002106.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002107.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002109.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002110.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002111.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002114.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002115.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002116.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002118.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002120.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002121.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002122.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002123.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002124.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002125.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002126.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002127.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002130.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002132.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002133.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002136.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002137.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002138.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002139.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002140.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002142.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002145.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002146.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002148.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002149.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002151.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002152.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002154.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002155.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002156.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002157.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002158.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002159.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002161.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002162.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002164.jpeg  \n","  inflating: data/fabi-data/train/6/none-0000002166.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002167.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002168.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002169.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002170.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002171.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002172.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002173.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002175.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002176.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002177.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002178.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002180.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002181.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002182.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002183.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002185.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002186.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002189.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002190.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002191.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002192.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002193.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002194.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002195.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002196.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002197.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002199.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002200.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002201.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002202.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002203.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002204.jpg  \n","  inflating: data/fabi-data/train/6/none-0000002205.jpg  \n","   creating: data/fabi-data/train/7/\n","  inflating: data/fabi-data/train/7/1DD57CB0-BFEC-45B9-B2AA-41B8D73A118C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/28691AD0-0258-4D30-8170-394D4D8A7613_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2AA5A159-FB90-4634-ADDE-21FBBB1EAA59_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2F9D85EE-96F1-462B-A174-A26CB53CB378_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/2FC02089-0F14-4F28-88CB-0FEBEFC426A1_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/307E680B-5376-42FD-977D-068536FBCCBC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/457015D5-3B01-4980-91D3-D642A7D4AE32_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/4A0B9067-B991-4183-B1E7-AD732F081857_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/4BE8C325-C19A-4296-939D-C57E261734CC_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/5295E23D-672A-4969-BFC8-BA6521C6924C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/5F340E00-6A07-4826-8A2E-434CBDCAC71F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/73B4ABE3-C24D-4B78-8E62-741793B9F018_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/A81B0FF7-5E8E-4E2C-835B-D7F65A4B5F2C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/A8633E3F-0D82-4BB6-96F9-3E09E4F1321C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/AD52FC80-9E88-4717-8751-AB485DE2B711_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/C9511A33-B2CB-40C4-B073-7B4E9E9A03B2_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/CC90ECCD-079A-4F7D-B2FE-58261862CE90_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/DBE6BEF6-8AC8-4BE9-8630-47BB21C2EF8F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/DSC_4180.JPG  \n","  inflating: data/fabi-data/train/7/DSC_4204.JPG  \n","  inflating: data/fabi-data/train/7/E27726E7-101A-446A-A3EE-90DB010D6D85_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E286D740-DCBF-4245-AC9F-AE0DC27DF482_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E2D0BE4A-37BC-4722-BABF-B27914A2FB6F_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/E6C17234-303F-4516-864C-11D091D235B9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/7/Egg capsule.jpg  \n","  inflating: data/fabi-data/train/7/Egg packet (2) - Copy.jpg  \n","  inflating: data/fabi-data/train/7/Egg packet (2).jpg  \n","  inflating: data/fabi-data/train/7/Egg packet.jpg  \n","  inflating: data/fabi-data/train/7/Egg_packet_dissected.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult1.tif  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult2.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult3.tif  \n","  inflating: data/fabi-data/train/7/Gonipterus_adult9.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg3.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg4.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_egg6.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_larvae.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_mating_pair.jpg  \n","  inflating: data/fabi-data/train/7/Gonipterus_oviposit2.jpg  \n","  inflating: data/fabi-data/train/7/IMG_0338.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0364.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0411.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0415.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0418.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0419.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0420.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0421.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0434.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0439.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0444.JPG  \n","  inflating: data/fabi-data/train/7/IMG_0447.JPG  \n","  inflating: data/fabi-data/train/7/IMG_1801.JPG  \n","  inflating: data/fabi-data/train/7/IMG_1807.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4239.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4245 (2).JPG  \n","  inflating: data/fabi-data/train/7/IMG_4245.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4249.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4462.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4464.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4465.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4466.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4468 (2).JPG  \n","  inflating: data/fabi-data/train/7/IMG_4468.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4469.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4470.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4471.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4472.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4473.JPG  \n","  inflating: data/fabi-data/train/7/IMG_4474.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6090.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6091.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6092.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6112.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6113.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6115.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6116.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6117.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6118.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6119.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6131.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6132.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6133.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6134.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6135.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6136.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6159.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6160.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6162.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6163.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6164.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6165.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6166.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6167.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6169.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6171.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6172.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6173.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6193.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6194.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6196.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6197.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6198.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6200.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6201.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6204.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6205.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6206.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6207.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6208.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6209.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6210.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6211.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6212.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6214.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6215.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6216.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6225.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6226.JPG  \n","  inflating: data/fabi-data/train/7/IMG_6228.JPG  \n","  inflating: data/fabi-data/train/7/none-0000001489.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001490.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001493.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001494.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001495.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001497.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001498.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001499.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001500.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001502.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001503.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001504.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001507.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001508.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001509.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001510.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001511.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001512.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001513.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001514.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001517.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001518.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001519.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001520.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001521.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001522.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001524.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001525.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001526.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001527.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001528.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001529.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001530.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001531.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001532.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001533.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001536.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001537.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001539.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001541.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001542.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001543.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001544.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001545.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001546.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001547.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001548.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001549.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001550.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001552.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001553.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001554.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001555.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001557.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001558.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001561.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001562.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001564.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001566.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001567.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001569.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001570.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001571.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001572.jpeg  \n","  inflating: data/fabi-data/train/7/none-0000001573.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001574.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001575.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001576.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001577.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001578.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001579.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001580.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001581.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001582.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001583.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001584.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001585.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001586.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001587.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001588.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001591.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001592.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001593.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001594.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001596.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001599.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001600.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001601.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001603.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001604.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001606.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001607.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001609.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001610.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001612.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001614.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001616.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001617.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001618.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001619.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001620.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001621.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001622.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001624.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001625.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001626.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001627.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001628.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001629.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001630.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001631.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001632.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001633.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001634.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001635.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001636.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001637.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001639.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001640.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001641.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001642.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001644.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001645.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001646.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001647.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001649.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001650.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001651.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001653.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001654.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001655.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001656.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001657.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001658.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001662.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001663.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001664.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001666.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001668.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001669.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001670.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001671.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001672.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001673.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001674.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001676.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001677.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001679.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001680.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001682.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001683.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001684.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001685.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001686.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001687.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001689.tif  \n","  inflating: data/fabi-data/train/7/none-0000001690.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001692.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001695.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001697.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001698.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001699.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001700.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001702.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001703.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001705.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001709.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001710.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001713.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001714.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001715.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001716.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001717.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001718.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001719.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001720.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001721.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001722.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001723.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001724.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001725.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001726.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001727.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001728.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001729.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001731.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001732.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001733.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001734.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001735.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001736.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001737.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001738.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001739.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001740.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001741.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001742.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001743.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001744.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001745.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001746.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001747.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001748.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001749.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001750.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001752.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001753.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001755.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001757.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001758.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001759.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001760.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001761.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001765.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001767.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001768.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001769.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001770.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001771.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001773.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001774.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001775.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001778.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001779.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001780.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001782.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001784.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001785.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001786.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001787.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001788.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001789.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001790.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001791.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001792.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001793.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001795.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001798.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001799.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001800.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001802.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001804.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001805.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001806.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001807.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001808.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001809.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001810.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001811.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001812.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001813.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001814.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001815.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001816.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001818.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001819.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001820.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001821.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001822.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001823.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001824.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001825.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001827.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001829.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001830.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001831.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001833.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001834.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001836.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001837.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001838.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001839.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001840.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001842.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001844.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001845.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001846.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001847.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001848.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001849.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001850.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001851.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001852.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001853.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001855.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001856.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001857.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001858.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001859.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001860.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001861.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001862.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001864.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001865.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001866.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001868.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001869.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001870.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001871.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001872.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001873.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001874.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001877.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001878.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001879.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001880.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001881.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001883.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001885.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001887.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001889.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001890.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001891.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001892.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001893.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001894.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001895.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001896.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001897.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001898.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001899.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001900.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001901.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001902.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001903.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001904.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001906.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001907.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001908.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001909.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001910.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001911.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001912.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001913.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001914.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001915.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001916.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001917.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001918.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001920.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001921.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001922.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001923.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001924.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001925.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001926.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001927.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001928.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001929.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001930.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001931.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001932.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001933.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001934.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001936.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001937.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001938.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001939.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001940.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001941.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001942.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001944.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001946.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001947.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001948.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001951.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001952.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001953.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001954.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001955.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001956.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001957.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001958.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001959.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001960.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001961.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001963.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001964.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001966.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001967.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001968.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001969.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001970.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001971.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001972.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001974.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001975.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001976.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001977.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001980.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001981.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001983.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001984.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001985.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001986.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001987.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001988.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001989.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001991.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001992.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001993.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001995.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001996.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001998.jpg  \n","  inflating: data/fabi-data/train/7/none-0000001999.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002000.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002001.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002002.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002004.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002007.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002008.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002009.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002010.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002012.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002014.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002015.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002016.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002018.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002019.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002020.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002022.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002023.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002025.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002026.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002027.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002029.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002031.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002032.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002033.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002034.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002035.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002038.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002039.jpg  \n","  inflating: data/fabi-data/train/7/none-0000002040.jpg  \n","   creating: data/fabi-data/train/8/\n","  inflating: data/fabi-data/train/8/1790F922-A855-4344-AD90-8B36530B71B9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/34210192-72DB-4F8C-971C-0BE8B2729412_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/36A5E91F-0B22-44B3-B65E-2F9D8B385502_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/394A3D8D-E242-4644-B0CA-6B2CB88F6561_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/404AD23D-88D0-4918-9CF6-A0C4A79361E9_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/52C32865-F47F-4CA7-A49B-534F8F31B8CE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/572FBADC-28B6-4376-BFF3-FC29A6C87C0C_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/80E39107-4E33-4DFD-B0EF-C51B242B1E09_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/D9C1DFA0-BC15-4218-A21E-CAF2B5A8953D_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/Dissected_gall.jpg  \n","  inflating: data/fabi-data/train/8/Dissected_gall2.jpg  \n","  inflating: data/fabi-data/train/8/Dissected_gall4.jpg  \n","  inflating: data/fabi-data/train/8/DSCN5873.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6606.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6608.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6610.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6612.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6614.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6615.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6617.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6618.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6619.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6620.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6621.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6622.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6624.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6625.JPG  \n","  inflating: data/fabi-data/train/8/DSCN6626.JPG  \n","  inflating: data/fabi-data/train/8/E8B8A6B0-208E-497C-89F1-BD949E702178_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/EA08DD96-0C39-4B8A-A10B-632E321C2DFE_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/ECA50AD6-98EF-4400-A749-3B7D8E6F1C2B_1_105_c.jpeg  \n","  inflating: data/fabi-data/train/8/IMG_0336.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0337.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0338.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0340.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0346.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0349.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0354.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0355.JPG  \n","  inflating: data/fabi-data/train/8/IMG_0370.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1867.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1870.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1874.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1969.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1970.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1987.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1990.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1992.JPG  \n","  inflating: data/fabi-data/train/8/IMG_1999.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2001.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2002.JPG  \n","  inflating: data/fabi-data/train/8/IMG_2012.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe female 1.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe female 1b.tif  \n","  inflating: data/fabi-data/train/8/Leptocybe female 2a.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe female 4a.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 1.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 2.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 4.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 5.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 6.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 7.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe male 8.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe Samantha Bush.jpg  \n","  inflating: data/fabi-data/train/8/Leptocybe1.JPG_Oviposition_scarring.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe2.JPG_Egg.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe3.JPG_Emergence_hole.JPG  \n","  inflating: data/fabi-data/train/8/Leptocybe5.JPG_Gall_formation_on_the_leaf_midrib.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa resized.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa2.jpg  \n","  inflating: data/fabi-data/train/8/Oviposition_scars_Linvasa3.jpg  \n","   creating: data/fabi-data/train/9/\n","  inflating: data/fabi-data/train/9/Megastigmus female brown 1a.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus female brown 2a.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus female brown 2b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus Female cream 1b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus male cream 1b.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus pretorianensis.jpg  \n","  inflating: data/fabi-data/train/9/Megastigmus zebrinus.jpg  \n"]}],"source":["# Downlaod and extract ip102 data\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1s7vhippKC3poqkVAWtgcBGun1nq3EDKF\" -O fabi-data.zip && rm -rf /tmp/cookies.txt\n","\n","!unzip fabi-data.zip -d data\n","\n","# https://drive.google.com/file/d/1s7vhippKC3poqkVAWtgcBGun1nq3EDKF/view?usp=drive_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkVR32jQqNjx"},"outputs":[],"source":["# Downlaod cnn script\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1NRyT9j-FOB-ZKdGsALJzSX-vEh0i5RyA' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1NRyT9j-FOB-ZKdGsALJzSX-vEh0i5RyA\" -O cnn.py && rm -rf /tmp/cookies.txt\n","\n","# https://drive.google.com/file/d/1NRyT9j-FOB-ZKdGsALJzSX-vEh0i5RyA/view?usp=drive_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0mnXKIz3SC6"},"outputs":[],"source":["# !python cnn-fabi.py \\\n","#     --model_name microsoft/resnet-18 \\\n","#     --model_type resnet \\\n","#     --train_dir data/fabi-data/train \\\n","#     --validation_dir data/fabi-data/test \\\n","#     --output_dir drive/MyDrive/hons-research/output/cnn/fabi/fabiPandD_outputs_1/ \\\n","#     --remove_unused_columns False \\\n","#     --do_train \\\n","#     --do_eval \\\n","#     --push_to_hub False \\\n","#     --optim adamw_torch \\\n","#     --learning_rate 0.001 \\\n","#     --num_train_epochs 300 \\\n","#     --per_device_train_batch_size 512 \\\n","#     --per_device_eval_batch_size 512 \\\n","#     --logging_strategy steps \\\n","#     --logging_steps 10 \\\n","#     --evaluation_strategy epoch \\\n","#     --save_strategy epoch \\\n","#     --load_best_model_at_end True \\\n","#     --save_total_limit 3 \\\n","#     --save_steps 500 \\\n","#     --data_seed 1 \\\n","#     --seed 1 \\\n","#     --report_to wandb \\\n","#     --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ej1I0zV6_yv8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698132738431,"user_tz":-120,"elapsed":2474902,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"}},"outputId":"4a9aedf3-1eac-4485-c22e-9b8c7e6ca353"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-24 06:51:08.099627: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-10-24 06:51:08.099681: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-10-24 06:51:08.099724: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-10-24 06:51:09.219512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231024_065116-dmfsjons\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mazure-forest-1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnn50Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnn50Fabi/runs/dmfsjons\u001b[0m\n","10/24/2023 06:51:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/24/2023 06:51:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=1,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/runs/Oct24_06-51-17_b67ca34c50c5,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=1,\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 56678.96it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 361396.95it/s]\n","Downloading data files: 100% 2796/2796 [00:00<00:00, 79431.01it/s]\n","Downloading data files: 100% 5/5 [00:00<00:00, 22215.59it/s]\n","Extracting data files: 100% 5/5 [00:00<00:00, 2237.68it/s]\n","Downloading data files: 100% 722/722 [00:00<00:00, 79631.00it/s]\n","Downloading data files: 100% 1/1 [00:00<00:00, 7194.35it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00, 1598.44it/s]\n","Generating train split: 2796 examples [00:00, 9842.07 examples/s]\n","Generating validation split: 722 examples [00:00, 12188.47 examples/s]\n","Casting the dataset: 100% 2796/2796 [00:00<00:00, 70333.54 examples/s]\n","Casting the dataset: 100% 722/722 [00:00<00:00, 19120.39 examples/s]\n","Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 11.3MB/s]\n","\n","\n","\n","None\n","\n","\n","\n","Downloading (…)lve/main/config.json: 100% 69.6k/69.6k [00:00<00:00, 27.2MB/s]\n","[INFO|configuration_utils.py:716] 2023-10-24 06:51:24,192 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/config.json\n","[INFO|configuration_utils.py:776] 2023-10-24 06:51:24,194 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-50\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    3,\n","    4,\n","    6,\n","    3\n","  ],\n","  \"downsample_in_bottleneck\": false,\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    256,\n","    512,\n","    1024,\n","    2048\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"bottleneck\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.35.0.dev0\"\n","}\n","\n","Downloading pytorch_model.bin: 100% 103M/103M [00:00<00:00, 268MB/s] \n","[INFO|modeling_utils.py:3015] 2023-10-24 06:51:25,430 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/pytorch_model.bin\n","[INFO|modeling_utils.py:3805] 2023-10-24 06:51:25,816 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3826] 2023-10-24 06:51:25,816 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([46, 2048]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Downloading (…)rocessor_config.json: 100% 266/266 [00:00<00:00, 946kB/s]\n","[INFO|image_processing_utils.py:369] 2023-10-24 06:51:26,322 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/preprocessor_config.json\n","[WARNING|image_processing_auto.py:359] 2023-10-24 06:51:26,322 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-24 06:51:26,324 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-24 06:51:26,324 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1669] 2023-10-24 06:51:31,674 >> ***** Running training *****\n","[INFO|trainer.py:1670] 2023-10-24 06:51:31,674 >>   Num examples = 2,796\n","[INFO|trainer.py:1671] 2023-10-24 06:51:31,674 >>   Num Epochs = 300\n","[INFO|trainer.py:1672] 2023-10-24 06:51:31,674 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1675] 2023-10-24 06:51:31,675 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1676] 2023-10-24 06:51:31,675 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1677] 2023-10-24 06:51:31,675 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1678] 2023-10-24 06:51:31,676 >>   Number of trainable parameters = 23,602,286\n","[INFO|integration_utils.py:718] 2023-10-24 06:51:31,677 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:12<3:43:52,  4.08s/it][INFO|trainer.py:3083] 2023-10-24 06:52:44,578 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 06:52:44,578 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 06:52:44,578 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.4776642322540283, 'eval_accuracy': 0.3074792243767313, 'eval_runtime': 20.1929, 'eval_samples_per_second': 35.755, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n","  0% 11/3300 [01:33<3:43:52,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 06:53:04,776 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-11\n","[INFO|configuration_utils.py:461] 2023-10-24 06:53:04,783 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 06:53:05,013 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 06:53:05,017 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:35<3:48:18,  4.18s/it][INFO|trainer.py:3083] 2023-10-24 06:54:07,388 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 06:54:07,388 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 06:54:07,388 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.9167438745498657, 'eval_accuracy': 0.4445983379501385, 'eval_runtime': 19.9673, 'eval_samples_per_second': 36.159, 'eval_steps_per_second': 0.15, 'epoch': 2.0}\n","  1% 22/3300 [02:55<3:48:18,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 06:54:27,362 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-22\n","[INFO|configuration_utils.py:461] 2023-10-24 06:54:27,367 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 06:54:27,598 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 06:54:27,603 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:58<3:44:53,  4.13s/it][INFO|trainer.py:3083] 2023-10-24 06:55:30,093 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 06:55:30,093 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 06:55:30,093 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.6611179113388062, 'eval_accuracy': 0.5720221606648199, 'eval_runtime': 20.2334, 'eval_samples_per_second': 35.684, 'eval_steps_per_second': 0.148, 'epoch': 3.0}\n","  1% 33/3300 [04:18<3:44:53,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 06:55:50,333 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-33\n","[INFO|configuration_utils.py:461] 2023-10-24 06:55:50,340 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 06:55:50,596 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 06:55:50,600 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:22<3:28:29,  3.84s/it][INFO|trainer.py:3083] 2023-10-24 06:56:53,849 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 06:56:53,850 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 06:56:53,850 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3558080196380615, 'eval_accuracy': 0.6260387811634349, 'eval_runtime': 19.9992, 'eval_samples_per_second': 36.101, 'eval_steps_per_second': 0.15, 'epoch': 4.0}\n","  1% 44/3300 [05:42<3:28:29,  3.84s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 06:57:13,855 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-44\n","[INFO|configuration_utils.py:461] 2023-10-24 06:57:13,861 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 06:57:14,103 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 06:57:14,108 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 06:57:14,538 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:45<3:45:43,  4.17s/it][INFO|trainer.py:3083] 2023-10-24 06:58:16,835 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 06:58:16,835 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 06:58:16,835 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0063245296478271, 'eval_accuracy': 0.7451523545706371, 'eval_runtime': 19.7759, 'eval_samples_per_second': 36.509, 'eval_steps_per_second': 0.152, 'epoch': 5.0}\n","  2% 55/3300 [07:04<3:45:43,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 06:58:36,616 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-55\n","[INFO|configuration_utils.py:461] 2023-10-24 06:58:36,621 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 06:58:36,859 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 06:58:36,863 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 06:58:37,292 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:07<3:40:30,  4.09s/it][INFO|trainer.py:3083] 2023-10-24 06:59:39,666 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 06:59:39,666 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 06:59:39,666 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9152474999427795, 'eval_accuracy': 0.7617728531855956, 'eval_runtime': 19.7086, 'eval_samples_per_second': 36.634, 'eval_steps_per_second': 0.152, 'epoch': 6.0}\n","  2% 66/3300 [08:27<3:40:30,  4.09s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 06:59:59,380 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-66\n","[INFO|configuration_utils.py:461] 2023-10-24 06:59:59,386 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 06:59:59,615 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 06:59:59,619 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:00:00,057 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:30<3:51:02,  4.30s/it][INFO|trainer.py:3083] 2023-10-24 07:01:02,471 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:01:02,471 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:01:02,472 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7885148525238037, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.7555, 'eval_samples_per_second': 36.547, 'eval_steps_per_second': 0.152, 'epoch': 7.0}\n","  2% 77/3300 [09:50<3:51:02,  4.30s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:01:22,231 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-77\n","[INFO|configuration_utils.py:461] 2023-10-24 07:01:22,236 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:01:22,468 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:01:22,473 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:01:22,923 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:53<3:37:12,  4.06s/it][INFO|trainer.py:3083] 2023-10-24 07:02:25,617 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:02:25,617 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:02:25,617 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.007791519165039, 'eval_accuracy': 0.7423822714681441, 'eval_runtime': 19.7165, 'eval_samples_per_second': 36.619, 'eval_steps_per_second': 0.152, 'epoch': 8.0}\n","  3% 88/3300 [11:13<3:37:12,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:02:45,340 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-88\n","[INFO|configuration_utils.py:461] 2023-10-24 07:02:45,359 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:02:45,585 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:02:45,589 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:02:46,020 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:16<3:35:00,  4.03s/it][INFO|trainer.py:3083] 2023-10-24 07:03:48,198 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:03:48,198 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:03:48,198 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.8201309442520142, 'eval_accuracy': 0.7659279778393352, 'eval_runtime': 19.7848, 'eval_samples_per_second': 36.493, 'eval_steps_per_second': 0.152, 'epoch': 9.0}\n","  3% 99/3300 [12:36<3:35:00,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:04:07,988 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-99\n","[INFO|configuration_utils.py:461] 2023-10-24 07:04:07,993 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:04:08,220 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:04:08,224 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:04:08,676 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-66] due to args.save_total_limit\n","{'loss': 1.1129, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:39<3:40:46,  4.15s/it][INFO|trainer.py:3083] 2023-10-24 07:05:10,855 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:05:10,855 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:05:10,855 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6099489331245422, 'eval_accuracy': 0.8365650969529086, 'eval_runtime': 19.7737, 'eval_samples_per_second': 36.513, 'eval_steps_per_second': 0.152, 'epoch': 10.0}\n","  3% 110/3300 [13:58<3:40:46,  4.15s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:05:30,634 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-110\n","[INFO|configuration_utils.py:461] 2023-10-24 07:05:30,639 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:05:30,884 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:05:30,889 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:05:31,347 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [15:02<3:36:40,  4.09s/it][INFO|trainer.py:3083] 2023-10-24 07:06:33,870 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:06:33,870 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:06:33,871 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6865516901016235, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 19.6523, 'eval_samples_per_second': 36.739, 'eval_steps_per_second': 0.153, 'epoch': 11.0}\n","  4% 121/3300 [15:21<3:36:40,  4.09s/it]\n","100% 3/3 [00:05<00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:06:53,528 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-121\n","[INFO|configuration_utils.py:461] 2023-10-24 07:06:53,533 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:06:53,763 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:06:53,767 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:06:54,220 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [16:24<3:43:43,  4.24s/it][INFO|trainer.py:3083] 2023-10-24 07:07:56,476 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:07:56,477 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:07:56,477 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6774932146072388, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.9321, 'eval_samples_per_second': 36.223, 'eval_steps_per_second': 0.151, 'epoch': 12.0}\n","  4% 132/3300 [16:44<3:43:43,  4.24s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:08:16,414 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-132\n","[INFO|configuration_utils.py:461] 2023-10-24 07:08:16,420 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:08:16,650 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:08:16,655 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:08:17,102 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:47<3:39:00,  4.16s/it][INFO|trainer.py:3083] 2023-10-24 07:09:19,258 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:09:19,259 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:09:19,259 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6447852253913879, 'eval_accuracy': 0.8240997229916898, 'eval_runtime': 19.7763, 'eval_samples_per_second': 36.508, 'eval_steps_per_second': 0.152, 'epoch': 13.0}\n","  4% 143/3300 [18:07<3:39:00,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:09:39,041 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-143\n","[INFO|configuration_utils.py:461] 2023-10-24 07:09:39,046 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:09:39,280 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:09:39,284 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:09:39,727 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [19:10<3:40:09,  4.20s/it][INFO|trainer.py:3083] 2023-10-24 07:10:42,276 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:10:42,276 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:10:42,276 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6378073692321777, 'eval_accuracy': 0.8379501385041551, 'eval_runtime': 19.8529, 'eval_samples_per_second': 36.367, 'eval_steps_per_second': 0.151, 'epoch': 14.0}\n","  5% 154/3300 [19:30<3:40:09,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:11:02,134 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-154\n","[INFO|configuration_utils.py:461] 2023-10-24 07:11:02,140 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:11:02,396 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:11:02,401 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:11:02,866 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:33<3:37:18,  4.16s/it][INFO|trainer.py:3083] 2023-10-24 07:12:05,252 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:12:05,252 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:12:05,252 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6357234120368958, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 19.7618, 'eval_samples_per_second': 36.535, 'eval_steps_per_second': 0.152, 'epoch': 15.0}\n","  5% 165/3300 [20:53<3:37:18,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:12:25,019 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-165\n","[INFO|configuration_utils.py:461] 2023-10-24 07:12:25,024 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:12:25,254 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:12:25,258 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:12:25,712 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:56<3:35:53,  4.15s/it][INFO|trainer.py:3083] 2023-10-24 07:13:28,385 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:13:28,385 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:13:28,385 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6505787968635559, 'eval_accuracy': 0.8407202216066482, 'eval_runtime': 19.8501, 'eval_samples_per_second': 36.373, 'eval_steps_per_second': 0.151, 'epoch': 16.0}\n","  5% 176/3300 [22:16<3:35:53,  4.15s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:13:48,241 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-176\n","[INFO|configuration_utils.py:461] 2023-10-24 07:13:48,246 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:13:48,476 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:13:48,480 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:13:48,913 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [23:19<3:37:48,  4.20s/it][INFO|trainer.py:3083] 2023-10-24 07:14:51,522 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:14:51,522 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:14:51,522 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7036474943161011, 'eval_accuracy': 0.8005540166204986, 'eval_runtime': 19.951, 'eval_samples_per_second': 36.189, 'eval_steps_per_second': 0.15, 'epoch': 17.0}\n","  6% 187/3300 [23:39<3:37:48,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:15:11,477 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-187\n","[INFO|configuration_utils.py:461] 2023-10-24 07:15:11,483 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:15:11,709 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:15:11,713 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:15:12,150 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [24:43<3:46:57,  4.39s/it][INFO|trainer.py:3083] 2023-10-24 07:16:14,783 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:16:14,783 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:16:14,783 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6404653787612915, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.9445, 'eval_samples_per_second': 36.201, 'eval_steps_per_second': 0.15, 'epoch': 18.0}\n","  6% 198/3300 [25:03<3:46:57,  4.39s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:16:34,733 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-198\n","[INFO|configuration_utils.py:461] 2023-10-24 07:16:34,739 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:16:34,977 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:16:34,982 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:16:35,444 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2203, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [26:05<3:38:56,  4.25s/it][INFO|trainer.py:3083] 2023-10-24 07:17:37,677 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:17:37,677 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:17:37,678 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5953850746154785, 'eval_accuracy': 0.832409972299169, 'eval_runtime': 19.7979, 'eval_samples_per_second': 36.468, 'eval_steps_per_second': 0.152, 'epoch': 19.0}\n","  6% 209/3300 [26:25<3:38:56,  4.25s/it]\n","100% 3/3 [00:05<00:00,  2.59s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:17:57,480 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-209\n","[INFO|configuration_utils.py:461] 2023-10-24 07:17:57,486 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:17:57,722 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:17:57,726 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:17:58,160 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-110] due to args.save_total_limit\n","  7% 220/3300 [27:28<3:26:47,  4.03s/it][INFO|trainer.py:3083] 2023-10-24 07:19:00,226 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:19:00,226 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:19:00,226 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6325816512107849, 'eval_accuracy': 0.8351800554016621, 'eval_runtime': 19.8275, 'eval_samples_per_second': 36.414, 'eval_steps_per_second': 0.151, 'epoch': 20.0}\n","  7% 220/3300 [27:48<3:26:47,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:19:20,059 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-220\n","[INFO|configuration_utils.py:461] 2023-10-24 07:19:20,065 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:19:20,295 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:19:20,299 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:19:20,729 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [28:51<3:36:24,  4.23s/it][INFO|trainer.py:3083] 2023-10-24 07:20:23,585 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:20:23,585 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:20:23,585 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6312359571456909, 'eval_accuracy': 0.8421052631578947, 'eval_runtime': 19.8369, 'eval_samples_per_second': 36.397, 'eval_steps_per_second': 0.151, 'epoch': 21.0}\n","  7% 231/3300 [29:11<3:36:24,  4.23s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:20:43,427 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-231\n","[INFO|configuration_utils.py:461] 2023-10-24 07:20:43,433 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:20:43,675 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:20:43,680 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:20:44,161 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-198] due to args.save_total_limit\n","  7% 242/3300 [30:14<3:32:40,  4.17s/it][INFO|trainer.py:3083] 2023-10-24 07:21:46,309 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:21:46,309 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:21:46,309 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6175452470779419, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 19.7038, 'eval_samples_per_second': 36.643, 'eval_steps_per_second': 0.152, 'epoch': 22.0}\n","  7% 242/3300 [30:34<3:32:40,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:22:06,019 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-242\n","[INFO|configuration_utils.py:461] 2023-10-24 07:22:06,024 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:22:06,253 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:22:06,257 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:22:06,691 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [31:37<3:33:01,  4.19s/it][INFO|trainer.py:3083] 2023-10-24 07:23:09,590 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:23:09,590 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:23:09,590 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.700872540473938, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.7541, 'eval_samples_per_second': 36.549, 'eval_steps_per_second': 0.152, 'epoch': 23.0}\n","  8% 253/3300 [31:57<3:33:01,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:23:29,348 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-253\n","[INFO|configuration_utils.py:461] 2023-10-24 07:23:29,354 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:23:29,593 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:23:29,597 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:23:30,033 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [33:00<3:29:20,  4.14s/it][INFO|trainer.py:3083] 2023-10-24 07:24:31,986 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:24:31,986 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:24:31,986 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6659634709358215, 'eval_accuracy': 0.8227146814404432, 'eval_runtime': 19.8039, 'eval_samples_per_second': 36.457, 'eval_steps_per_second': 0.151, 'epoch': 24.0}\n","  8% 264/3300 [33:20<3:29:20,  4.14s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:24:51,796 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-264\n","[INFO|configuration_utils.py:461] 2023-10-24 07:24:51,802 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:24:52,031 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:24:52,047 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:24:52,480 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [34:22<3:38:41,  4.34s/it][INFO|trainer.py:3083] 2023-10-24 07:25:54,673 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:25:54,673 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:25:54,673 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6406394243240356, 'eval_accuracy': 0.8254847645429363, 'eval_runtime': 19.8632, 'eval_samples_per_second': 36.349, 'eval_steps_per_second': 0.151, 'epoch': 25.0}\n","  8% 275/3300 [34:42<3:38:41,  4.34s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:26:14,541 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-275\n","[INFO|configuration_utils.py:461] 2023-10-24 07:26:14,547 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:26:14,782 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:26:14,786 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:26:15,232 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [35:45<3:34:17,  4.27s/it][INFO|trainer.py:3083] 2023-10-24 07:27:17,564 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:27:17,565 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:27:17,565 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6636935472488403, 'eval_accuracy': 0.832409972299169, 'eval_runtime': 19.7586, 'eval_samples_per_second': 36.541, 'eval_steps_per_second': 0.152, 'epoch': 26.0}\n","  9% 286/3300 [36:05<3:34:17,  4.27s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:27:37,329 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-286\n","[INFO|configuration_utils.py:461] 2023-10-24 07:27:37,334 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:27:37,564 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:27:37,568 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:27:38,000 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [37:08<3:33:13,  4.26s/it][INFO|trainer.py:3083] 2023-10-24 07:28:40,159 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:28:40,159 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:28:40,159 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6783831715583801, 'eval_accuracy': 0.8060941828254847, 'eval_runtime': 19.7943, 'eval_samples_per_second': 36.475, 'eval_steps_per_second': 0.152, 'epoch': 27.0}\n","  9% 297/3300 [37:28<3:33:13,  4.26s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:28:59,958 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-297\n","[INFO|configuration_utils.py:461] 2023-10-24 07:28:59,975 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:29:00,210 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:29:00,214 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:29:00,666 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1533, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [38:31<3:27:07,  4.15s/it][INFO|trainer.py:3083] 2023-10-24 07:30:03,105 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:30:03,105 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:30:03,106 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6648032665252686, 'eval_accuracy': 0.817174515235457, 'eval_runtime': 19.9088, 'eval_samples_per_second': 36.265, 'eval_steps_per_second': 0.151, 'epoch': 28.0}\n","  9% 308/3300 [38:51<3:27:07,  4.15s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:30:23,020 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-308\n","[INFO|configuration_utils.py:461] 2023-10-24 07:30:23,026 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:30:23,266 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:30:23,270 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:30:23,745 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [39:54<3:22:07,  4.07s/it][INFO|trainer.py:3083] 2023-10-24 07:31:26,004 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:31:26,004 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:31:26,004 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6493263244628906, 'eval_accuracy': 0.8310249307479224, 'eval_runtime': 19.7856, 'eval_samples_per_second': 36.491, 'eval_steps_per_second': 0.152, 'epoch': 29.0}\n"," 10% 319/3300 [40:14<3:22:07,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:31:45,794 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-319\n","[INFO|configuration_utils.py:461] 2023-10-24 07:31:45,799 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:31:46,029 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:31:46,034 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:31:46,467 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-297] due to args.save_total_limit\n","[INFO|trainer.py:1902] 2023-10-24 07:31:46,488 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2081] 2023-10-24 07:31:46,489 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/checkpoint-209 (score: 0.5953850746154785).\n","{'train_runtime': 2414.9421, 'train_samples_per_second': 347.338, 'train_steps_per_second': 1.366, 'train_loss': 0.47412791371719215, 'epoch': 29.0}\n"," 10% 319/3300 [40:14<6:16:07,  7.57s/it]\n","[INFO|trainer.py:2806] 2023-10-24 07:31:46,623 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/\n","[INFO|configuration_utils.py:461] 2023-10-24 07:31:46,629 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:31:46,870 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:31:46,892 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       29.0\n","  train_loss               =     0.4741\n","  train_runtime            = 0:40:14.94\n","  train_samples_per_second =    347.338\n","  train_steps_per_second   =      1.366\n","[INFO|trainer.py:3083] 2023-10-24 07:31:46,906 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:31:46,907 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:31:46,907 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.81s/it]\n","***** eval metrics *****\n","  epoch                   =       29.0\n","  eval_accuracy           =     0.8324\n","  eval_loss               =     0.5954\n","  eval_runtime            = 0:00:20.09\n","  eval_samples_per_second =     35.922\n","  eval_steps_per_second   =      0.149\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▄▅▇▇▇▇▇███████▇█████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▅▄▃▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▅█▅▂▂▂▂▃▂▁▄▂▃▂▃▅▅▃▃▃▂▂▃▄▂▃▄▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▄▁▄▆▇▇▇▆▇█▅▆▆▇▆▄▄▆▆▆▇▇▆▅▇▆▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▂▄▁▄▇▇▇▇▇▇█▅▇▅▇▅▄▄▇▅▅▇▇▅▅▇▇▅▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.83241\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.59539\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.0989\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.922\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.149\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 29.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 319\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1533\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.72845379847902e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.47413\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2414.9421\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 347.338\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.366\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mazure-forest-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnn50Fabi/runs/dmfsjons\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231024_065116-dmfsjons/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-50 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_1/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 1 \\\n","    --seed 1 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAU8ZpJXhnFG","outputId":"b9c7234a-7c01-4731-c9b0-a48a9b05d3db"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-24 07:32:57.322910: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-10-24 07:32:57.322967: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-10-24 07:32:57.322995: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-10-24 07:32:58.431900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231024_073304-u9xxnnx8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mspring-glade-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnn50Fabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnn50Fabi/runs/u9xxnnx8\u001b[0m\n","10/24/2023 07:33:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/24/2023 07:33:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=2,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/runs/Oct24_07-33-06_b67ca34c50c5,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=2,\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 59852.69it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 331890.31it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:716] 2023-10-24 07:33:10,492 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/config.json\n","[INFO|configuration_utils.py:776] 2023-10-24 07:33:10,493 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-50\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    3,\n","    4,\n","    6,\n","    3\n","  ],\n","  \"downsample_in_bottleneck\": false,\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    256,\n","    512,\n","    1024,\n","    2048\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"bottleneck\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.35.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:3015] 2023-10-24 07:33:10,495 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/pytorch_model.bin\n","[INFO|modeling_utils.py:3805] 2023-10-24 07:33:10,838 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3826] 2023-10-24 07:33:10,838 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n","- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([46, 2048]) in the model instantiated\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-24 07:33:11,654 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/preprocessor_config.json\n","[WARNING|image_processing_auto.py:359] 2023-10-24 07:33:11,655 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-24 07:33:11,656 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-24 07:33:11,657 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1669] 2023-10-24 07:33:13,603 >> ***** Running training *****\n","[INFO|trainer.py:1670] 2023-10-24 07:33:13,604 >>   Num examples = 2,796\n","[INFO|trainer.py:1671] 2023-10-24 07:33:13,604 >>   Num Epochs = 300\n","[INFO|trainer.py:1672] 2023-10-24 07:33:13,604 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1675] 2023-10-24 07:33:13,604 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1676] 2023-10-24 07:33:13,604 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1677] 2023-10-24 07:33:13,605 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1678] 2023-10-24 07:33:13,606 >>   Number of trainable parameters = 23,602,286\n","[INFO|integration_utils.py:718] 2023-10-24 07:33:13,607 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:06<3:42:32,  4.06s/it][INFO|trainer.py:3083] 2023-10-24 07:34:19,663 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:34:19,663 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:34:19,663 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.5033953189849854, 'eval_accuracy': 0.33102493074792244, 'eval_runtime': 20.1061, 'eval_samples_per_second': 35.91, 'eval_steps_per_second': 0.149, 'epoch': 1.0}\n","  0% 11/3300 [01:26<3:42:32,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:34:39,774 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-11\n","[INFO|configuration_utils.py:461] 2023-10-24 07:34:39,780 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:34:40,012 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:34:40,016 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:29<3:47:29,  4.16s/it][INFO|trainer.py:3083] 2023-10-24 07:35:43,063 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:35:43,064 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:35:43,064 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.9010775089263916, 'eval_accuracy': 0.46398891966759004, 'eval_runtime': 20.0749, 'eval_samples_per_second': 35.965, 'eval_steps_per_second': 0.149, 'epoch': 2.0}\n","  1% 22/3300 [02:49<3:47:29,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:36:03,144 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-22\n","[INFO|configuration_utils.py:461] 2023-10-24 07:36:03,150 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:36:03,394 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:36:03,399 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:52<3:51:47,  4.26s/it][INFO|trainer.py:3083] 2023-10-24 07:37:06,542 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:37:06,542 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:37:06,542 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.547408938407898, 'eval_accuracy': 0.590027700831025, 'eval_runtime': 19.7968, 'eval_samples_per_second': 36.47, 'eval_steps_per_second': 0.152, 'epoch': 3.0}\n","  1% 33/3300 [04:12<3:51:47,  4.26s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:37:26,345 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-33\n","[INFO|configuration_utils.py:461] 2023-10-24 07:37:26,350 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:37:26,580 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:37:26,585 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:15<3:41:22,  4.08s/it][INFO|trainer.py:3083] 2023-10-24 07:38:29,186 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:38:29,187 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:38:29,187 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2488726377487183, 'eval_accuracy': 0.6717451523545707, 'eval_runtime': 19.8436, 'eval_samples_per_second': 36.385, 'eval_steps_per_second': 0.151, 'epoch': 4.0}\n","  1% 44/3300 [05:35<3:41:22,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:38:49,036 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-44\n","[INFO|configuration_utils.py:461] 2023-10-24 07:38:49,042 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:38:49,270 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:38:49,275 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:38:49,719 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:38<3:56:25,  4.37s/it][INFO|trainer.py:3083] 2023-10-24 07:39:52,503 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:39:52,503 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:39:52,503 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1866098642349243, 'eval_accuracy': 0.703601108033241, 'eval_runtime': 19.9468, 'eval_samples_per_second': 36.196, 'eval_steps_per_second': 0.15, 'epoch': 5.0}\n","  2% 55/3300 [06:58<3:56:25,  4.37s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:40:12,457 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-55\n","[INFO|configuration_utils.py:461] 2023-10-24 07:40:12,463 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:40:12,696 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:40:12,713 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:40:13,165 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:01<3:39:53,  4.08s/it][INFO|trainer.py:3083] 2023-10-24 07:41:15,392 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:41:15,392 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:41:15,392 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9595373868942261, 'eval_accuracy': 0.7590027700831025, 'eval_runtime': 19.8851, 'eval_samples_per_second': 36.309, 'eval_steps_per_second': 0.151, 'epoch': 6.0}\n","  2% 66/3300 [08:21<3:39:53,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:41:35,281 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-66\n","[INFO|configuration_utils.py:461] 2023-10-24 07:41:35,287 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:41:35,516 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:41:35,520 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:41:35,954 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:25<3:48:27,  4.25s/it][INFO|trainer.py:3083] 2023-10-24 07:42:38,724 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:42:38,725 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:42:38,725 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1495572328567505, 'eval_accuracy': 0.7174515235457064, 'eval_runtime': 19.9687, 'eval_samples_per_second': 36.157, 'eval_steps_per_second': 0.15, 'epoch': 7.0}\n","  2% 77/3300 [09:45<3:48:27,  4.25s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:42:58,700 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-77\n","[INFO|configuration_utils.py:461] 2023-10-24 07:42:58,705 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:42:58,934 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:42:58,939 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:42:59,373 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:48<3:34:31,  4.01s/it][INFO|trainer.py:3083] 2023-10-24 07:44:02,180 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:44:02,181 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:44:02,181 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7270788550376892, 'eval_accuracy': 0.7922437673130194, 'eval_runtime': 19.8189, 'eval_samples_per_second': 36.43, 'eval_steps_per_second': 0.151, 'epoch': 8.0}\n","  3% 88/3300 [11:08<3:34:31,  4.01s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:44:22,005 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-88\n","[INFO|configuration_utils.py:461] 2023-10-24 07:44:22,011 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:44:22,254 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:44:22,270 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:44:22,707 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:11<3:34:57,  4.03s/it][INFO|trainer.py:3083] 2023-10-24 07:45:25,136 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:45:25,136 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:45:25,136 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.7513182759284973, 'eval_accuracy': 0.8088642659279779, 'eval_runtime': 19.8951, 'eval_samples_per_second': 36.29, 'eval_steps_per_second': 0.151, 'epoch': 9.0}\n","  3% 99/3300 [12:31<3:34:57,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:45:45,038 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-99\n","[INFO|configuration_utils.py:461] 2023-10-24 07:45:45,044 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:45:45,276 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:45:45,280 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:45:45,716 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-66] due to args.save_total_limit\n","{'loss': 1.1366, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:34<3:34:53,  4.04s/it][INFO|trainer.py:3083] 2023-10-24 07:46:48,169 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:46:48,169 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:46:48,170 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7264432311058044, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.7785, 'eval_samples_per_second': 36.504, 'eval_steps_per_second': 0.152, 'epoch': 10.0}\n","  3% 110/3300 [13:54<3:34:53,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:47:07,953 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-110\n","[INFO|configuration_utils.py:461] 2023-10-24 07:47:07,958 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:47:08,188 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:47:08,192 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:47:08,627 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [14:56<3:37:04,  4.10s/it][INFO|trainer.py:3083] 2023-10-24 07:48:10,409 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:48:10,409 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:48:10,409 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6329479813575745, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 19.8385, 'eval_samples_per_second': 36.394, 'eval_steps_per_second': 0.151, 'epoch': 11.0}\n","  4% 121/3300 [15:16<3:37:04,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:48:30,252 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-121\n","[INFO|configuration_utils.py:461] 2023-10-24 07:48:30,259 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:48:30,515 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:48:30,530 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:48:30,972 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [16:19<3:54:20,  4.44s/it][INFO|trainer.py:3083] 2023-10-24 07:49:33,410 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:49:33,410 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:49:33,411 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7192878127098083, 'eval_accuracy': 0.7950138504155124, 'eval_runtime': 19.7194, 'eval_samples_per_second': 36.614, 'eval_steps_per_second': 0.152, 'epoch': 12.0}\n","  4% 132/3300 [16:39<3:54:20,  4.44s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:49:53,135 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-132\n","[INFO|configuration_utils.py:461] 2023-10-24 07:49:53,141 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:49:53,372 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:49:53,377 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:49:53,815 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:42<3:33:59,  4.07s/it][INFO|trainer.py:3083] 2023-10-24 07:50:56,415 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:50:56,415 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:50:56,415 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8049871921539307, 'eval_accuracy': 0.7770083102493075, 'eval_runtime': 19.8482, 'eval_samples_per_second': 36.376, 'eval_steps_per_second': 0.151, 'epoch': 13.0}\n","  4% 143/3300 [18:02<3:33:59,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:51:16,269 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-143\n","[INFO|configuration_utils.py:461] 2023-10-24 07:51:16,275 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:51:16,507 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:51:16,511 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:51:16,948 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [19:05<3:37:48,  4.15s/it][INFO|trainer.py:3083] 2023-10-24 07:52:19,389 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:52:19,389 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:52:19,389 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6584932804107666, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 19.8179, 'eval_samples_per_second': 36.432, 'eval_steps_per_second': 0.151, 'epoch': 14.0}\n","  5% 154/3300 [19:25<3:37:48,  4.15s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:52:39,213 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-154\n","[INFO|configuration_utils.py:461] 2023-10-24 07:52:39,218 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:52:39,461 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:52:39,466 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:52:39,903 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:28<3:39:47,  4.21s/it][INFO|trainer.py:3083] 2023-10-24 07:53:42,329 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:53:42,330 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:53:42,330 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6215421557426453, 'eval_accuracy': 0.8199445983379502, 'eval_runtime': 19.8715, 'eval_samples_per_second': 36.334, 'eval_steps_per_second': 0.151, 'epoch': 15.0}\n","  5% 165/3300 [20:48<3:39:47,  4.21s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:54:02,206 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-165\n","[INFO|configuration_utils.py:461] 2023-10-24 07:54:02,212 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:54:02,453 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:54:02,457 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:54:02,928 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-121] due to args.save_total_limit\n","  5% 176/3300 [21:51<3:38:46,  4.20s/it][INFO|trainer.py:3083] 2023-10-24 07:55:05,215 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:55:05,215 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:55:05,216 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6359816193580627, 'eval_accuracy': 0.8157894736842105, 'eval_runtime': 19.7204, 'eval_samples_per_second': 36.612, 'eval_steps_per_second': 0.152, 'epoch': 16.0}\n","  5% 176/3300 [22:11<3:38:46,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:55:24,942 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-176\n","[INFO|configuration_utils.py:461] 2023-10-24 07:55:24,947 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:55:25,175 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:55:25,180 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:55:25,616 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-143] due to args.save_total_limit\n","  6% 187/3300 [23:14<3:33:08,  4.11s/it][INFO|trainer.py:3083] 2023-10-24 07:56:28,077 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:56:28,077 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:56:28,077 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6590130925178528, 'eval_accuracy': 0.814404432132964, 'eval_runtime': 19.8902, 'eval_samples_per_second': 36.299, 'eval_steps_per_second': 0.151, 'epoch': 17.0}\n","  6% 187/3300 [23:34<3:33:08,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:56:47,972 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-187\n","[INFO|configuration_utils.py:461] 2023-10-24 07:56:47,990 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:56:48,222 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:56:48,226 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:56:48,660 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-154] due to args.save_total_limit\n","  6% 198/3300 [24:37<3:32:43,  4.11s/it][INFO|trainer.py:3083] 2023-10-24 07:57:51,247 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:57:51,247 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:57:51,247 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6985467076301575, 'eval_accuracy': 0.8019390581717452, 'eval_runtime': 19.9186, 'eval_samples_per_second': 36.248, 'eval_steps_per_second': 0.151, 'epoch': 18.0}\n","  6% 198/3300 [24:57<3:32:43,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:58:11,170 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-198\n","[INFO|configuration_utils.py:461] 2023-10-24 07:58:11,176 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:58:11,414 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:58:11,419 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:58:11,876 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.2377, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [26:00<3:29:16,  4.06s/it][INFO|trainer.py:3083] 2023-10-24 07:59:13,972 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 07:59:13,972 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 07:59:13,972 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.5816198587417603, 'eval_accuracy': 0.8365650969529086, 'eval_runtime': 19.8561, 'eval_samples_per_second': 36.362, 'eval_steps_per_second': 0.151, 'epoch': 19.0}\n","  6% 209/3300 [26:20<3:29:16,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 07:59:33,833 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-209\n","[INFO|configuration_utils.py:461] 2023-10-24 07:59:33,838 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 07:59:34,076 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 07:59:34,082 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 07:59:34,552 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-165] due to args.save_total_limit\n","  7% 220/3300 [27:23<3:34:47,  4.18s/it][INFO|trainer.py:3083] 2023-10-24 08:00:36,785 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:00:36,785 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:00:36,785 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.726349949836731, 'eval_accuracy': 0.8074792243767313, 'eval_runtime': 20.0148, 'eval_samples_per_second': 36.073, 'eval_steps_per_second': 0.15, 'epoch': 20.0}\n","  7% 220/3300 [27:43<3:34:47,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:00:56,806 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-220\n","[INFO|configuration_utils.py:461] 2023-10-24 08:00:56,812 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:00:57,047 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:00:57,051 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:00:57,483 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [28:46<3:34:36,  4.20s/it][INFO|trainer.py:3083] 2023-10-24 08:02:00,082 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:02:00,082 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:02:00,082 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6338094472885132, 'eval_accuracy': 0.8282548476454293, 'eval_runtime': 19.9022, 'eval_samples_per_second': 36.277, 'eval_steps_per_second': 0.151, 'epoch': 21.0}\n","  7% 231/3300 [29:06<3:34:36,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:02:19,989 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-231\n","[INFO|configuration_utils.py:461] 2023-10-24 08:02:19,995 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:02:20,222 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:02:20,226 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:02:20,673 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-198] due to args.save_total_limit\n","  7% 242/3300 [30:09<3:22:33,  3.97s/it][INFO|trainer.py:3083] 2023-10-24 08:03:23,402 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:03:23,402 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:03:23,402 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6993557214736938, 'eval_accuracy': 0.8116343490304709, 'eval_runtime': 20.1762, 'eval_samples_per_second': 35.785, 'eval_steps_per_second': 0.149, 'epoch': 22.0}\n","  7% 242/3300 [30:29<3:22:33,  3.97s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:03:43,584 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-242\n","[INFO|configuration_utils.py:461] 2023-10-24 08:03:43,590 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:03:43,834 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:03:43,838 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:03:44,297 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [31:32<3:28:46,  4.11s/it][INFO|trainer.py:3083] 2023-10-24 08:04:46,261 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:04:46,261 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:04:46,262 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6429024934768677, 'eval_accuracy': 0.8157894736842105, 'eval_runtime': 19.8772, 'eval_samples_per_second': 36.323, 'eval_steps_per_second': 0.151, 'epoch': 23.0}\n","  8% 253/3300 [31:52<3:28:46,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:05:06,144 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-253\n","[INFO|configuration_utils.py:461] 2023-10-24 08:05:06,150 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:05:06,409 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:05:06,414 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:05:06,876 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [32:55<3:28:17,  4.12s/it][INFO|trainer.py:3083] 2023-10-24 08:06:08,931 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:06:08,931 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:06:08,932 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6972977519035339, 'eval_accuracy': 0.8213296398891967, 'eval_runtime': 20.2568, 'eval_samples_per_second': 35.642, 'eval_steps_per_second': 0.148, 'epoch': 24.0}\n","  8% 264/3300 [33:15<3:28:17,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:06:29,194 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-264\n","[INFO|configuration_utils.py:461] 2023-10-24 08:06:29,199 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:06:29,429 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:06:29,433 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:06:29,879 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [34:18<3:24:33,  4.06s/it][INFO|trainer.py:3083] 2023-10-24 08:07:32,456 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:07:32,456 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:07:32,456 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8222994208335876, 'eval_accuracy': 0.8185595567867036, 'eval_runtime': 20.1216, 'eval_samples_per_second': 35.882, 'eval_steps_per_second': 0.149, 'epoch': 25.0}\n","  8% 275/3300 [34:38<3:24:33,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:07:52,583 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-275\n","[INFO|configuration_utils.py:461] 2023-10-24 08:07:52,589 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:07:52,822 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:07:52,826 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:07:53,267 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [35:42<3:38:32,  4.35s/it][INFO|trainer.py:3083] 2023-10-24 08:08:55,823 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:08:55,824 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:08:55,824 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6734297275543213, 'eval_accuracy': 0.8254847645429363, 'eval_runtime': 20.0101, 'eval_samples_per_second': 36.082, 'eval_steps_per_second': 0.15, 'epoch': 26.0}\n","  9% 286/3300 [36:02<3:38:32,  4.35s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:09:15,839 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-286\n","[INFO|configuration_utils.py:461] 2023-10-24 08:09:15,845 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:09:16,085 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:09:16,089 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:09:16,528 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [37:05<3:24:37,  4.09s/it][INFO|trainer.py:3083] 2023-10-24 08:10:18,824 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:10:18,824 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:10:18,824 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7303439974784851, 'eval_accuracy': 0.8102493074792244, 'eval_runtime': 20.0731, 'eval_samples_per_second': 35.969, 'eval_steps_per_second': 0.149, 'epoch': 27.0}\n","  9% 297/3300 [37:25<3:24:37,  4.09s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:10:38,901 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-297\n","[INFO|configuration_utils.py:461] 2023-10-24 08:10:38,907 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:10:39,137 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:10:39,141 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:10:39,588 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.1546, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [38:28<3:35:33,  4.32s/it][INFO|trainer.py:3083] 2023-10-24 08:11:41,968 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:11:41,968 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:11:41,969 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6729550361633301, 'eval_accuracy': 0.8296398891966759, 'eval_runtime': 19.9519, 'eval_samples_per_second': 36.187, 'eval_steps_per_second': 0.15, 'epoch': 28.0}\n","  9% 308/3300 [38:48<3:35:33,  4.32s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:12:01,925 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-308\n","[INFO|configuration_utils.py:461] 2023-10-24 08:12:01,931 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:12:02,160 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:12:02,164 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:12:02,597 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [39:51<3:13:58,  3.90s/it][INFO|trainer.py:3083] 2023-10-24 08:13:05,348 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:13:05,348 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:13:05,348 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.6596731543540955, 'eval_accuracy': 0.8337950138504155, 'eval_runtime': 20.2594, 'eval_samples_per_second': 35.638, 'eval_steps_per_second': 0.148, 'epoch': 29.0}\n"," 10% 319/3300 [40:11<3:13:58,  3.90s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2806] 2023-10-24 08:13:25,612 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-319\n","[INFO|configuration_utils.py:461] 2023-10-24 08:13:25,618 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:13:25,859 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:13:25,863 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:2896] 2023-10-24 08:13:26,305 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-297] due to args.save_total_limit\n","[INFO|trainer.py:1902] 2023-10-24 08:13:26,326 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2081] 2023-10-24 08:13:26,327 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/checkpoint-209 (score: 0.5816198587417603).\n","{'train_runtime': 2412.8798, 'train_samples_per_second': 347.634, 'train_steps_per_second': 1.368, 'train_loss': 0.48700854845555225, 'epoch': 29.0}\n"," 10% 319/3300 [40:12<6:15:47,  7.56s/it]\n","[INFO|trainer.py:2806] 2023-10-24 08:13:26,490 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/\n","[INFO|configuration_utils.py:461] 2023-10-24 08:13:26,496 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/config.json\n","[INFO|modeling_utils.py:2123] 2023-10-24 08:13:26,735 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-24 08:13:26,739 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       29.0\n","  train_loss               =      0.487\n","  train_runtime            = 0:40:12.87\n","  train_samples_per_second =    347.634\n","  train_steps_per_second   =      1.368\n","[INFO|trainer.py:3083] 2023-10-24 08:13:26,874 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3085] 2023-10-24 08:13:26,874 >>   Num examples = 722\n","[INFO|trainer.py:3088] 2023-10-24 08:13:26,874 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.77s/it]\n","***** eval metrics *****\n","  epoch                   =       29.0\n","  eval_accuracy           =     0.8366\n","  eval_loss               =     0.5816\n","  eval_runtime            = 0:00:20.11\n","  eval_samples_per_second =     35.895\n","  eval_steps_per_second   =      0.149\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▅▆▆▇▆▇███▇▇█████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▅▃▃▂▃▂▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▂▁▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▆▆▂▃▄▃▄▂▃▂▃▁▃▂▃▁▃▄▃▅▃▇▃█▆▅▆▄█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▃▃▇▆▅▆▅▇▆▇▆█▆▇▆█▆▅▆▄▆▂▆▁▃▄▃▅▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▃▃█▆▅▆▅▆▆█▆█▆▆▆█▆▆▆▅▆▃▆▁▃▅▃▅▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.83657\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.58162\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.1141\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.895\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.149\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 29.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 319\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1546\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.72845379847902e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.48701\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2412.8798\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 347.634\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.368\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mspring-glade-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnn50Fabi/runs/u9xxnnx8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231024_073304-u9xxnnx8/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-50 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn50/fabi/fabiPandD_outputs_2/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 2 \\\n","    --seed 2 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EuAxN-9TFIeO","outputId":"2a65f08c-3107-4080-d838-6b5d634812c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 16:04:49.469823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_160458-9hfshy3w\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-salad-3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/9hfshy3w\u001b[0m\n","10/02/2023 16:04:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 16:04:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=3,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/runs/Oct02_16-04-59_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=3,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 374159.86it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 298443.24it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 16:05:03,223 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 16:05:03,225 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 16:05:03,227 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 16:05:03,349 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 16:05:03,350 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 16:05:03,611 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 16:05:03,612 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 16:05:03,613 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 16:05:03,614 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 16:05:05,546 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 16:05:05,546 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 16:05:05,547 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 16:05:05,547 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 16:05:05,547 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 16:05:05,547 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 16:05:05,547 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 16:05:05,548 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 16:05:05,549 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:07<3:48:46,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 16:06:13,305 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:06:13,305 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:06:13,305 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 6.174312114715576, 'eval_accuracy': 0.3878116343490305, 'eval_runtime': 20.5178, 'eval_samples_per_second': 35.189, 'eval_steps_per_second': 0.146, 'epoch': 1.0}\n","  0% 11/3300 [01:28<3:48:46,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:06:33,829 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 16:06:33,854 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:06:33,981 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:06:33,986 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:31<3:37:23,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 16:07:36,858 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:07:36,858 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:07:36,858 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.145099401473999, 'eval_accuracy': 0.5221606648199446, 'eval_runtime': 20.2432, 'eval_samples_per_second': 35.666, 'eval_steps_per_second': 0.148, 'epoch': 2.0}\n","  1% 22/3300 [02:51<3:37:23,  3.98s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:07:57,108 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 16:07:57,115 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:07:57,230 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:07:57,235 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:53<3:48:13,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 16:08:59,352 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:08:59,352 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:08:59,352 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.6798802614212036, 'eval_accuracy': 0.554016620498615, 'eval_runtime': 20.2008, 'eval_samples_per_second': 35.741, 'eval_steps_per_second': 0.149, 'epoch': 3.0}\n","  1% 33/3300 [04:13<3:48:13,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:09:19,560 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 16:09:19,568 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:09:19,701 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:09:19,705 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:16<3:37:22,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 16:10:21,908 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:10:21,909 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:10:21,909 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.63679039478302, 'eval_accuracy': 0.6024930747922438, 'eval_runtime': 20.0651, 'eval_samples_per_second': 35.983, 'eval_steps_per_second': 0.15, 'epoch': 4.0}\n","  1% 44/3300 [05:36<3:37:22,  4.01s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:10:41,981 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 16:10:41,987 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:10:42,101 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:10:42,105 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:10:42,330 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:39<3:51:22,  4.28s/it][INFO|trainer.py:3213] 2023-10-02 16:11:44,634 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:11:44,634 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:11:44,635 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.9565829038619995, 'eval_accuracy': 0.5152354570637119, 'eval_runtime': 20.1123, 'eval_samples_per_second': 35.898, 'eval_steps_per_second': 0.149, 'epoch': 5.0}\n","  2% 55/3300 [06:59<3:51:22,  4.28s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:12:04,754 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 16:12:04,760 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:12:04,872 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:12:04,876 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:12:05,097 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:02<3:42:49,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 16:13:08,009 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:13:08,009 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:13:08,009 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0018378496170044, 'eval_accuracy': 0.7229916897506925, 'eval_runtime': 20.351, 'eval_samples_per_second': 35.477, 'eval_steps_per_second': 0.147, 'epoch': 6.0}\n","  2% 66/3300 [08:22<3:42:49,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:13:28,366 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 16:13:28,373 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:13:28,486 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:13:28,490 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:13:28,729 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:25<4:01:26,  4.49s/it][INFO|trainer.py:3213] 2023-10-02 16:14:31,454 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:14:31,454 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:14:31,454 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.689601182937622, 'eval_accuracy': 0.6052631578947368, 'eval_runtime': 20.2666, 'eval_samples_per_second': 35.625, 'eval_steps_per_second': 0.148, 'epoch': 7.0}\n","  2% 77/3300 [09:46<4:01:26,  4.49s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:14:51,730 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 16:14:51,736 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:14:51,850 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:14:51,854 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:14:52,094 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:48<3:36:06,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 16:15:54,378 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:15:54,379 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:15:54,379 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2673527002334595, 'eval_accuracy': 0.6537396121883656, 'eval_runtime': 20.5321, 'eval_samples_per_second': 35.164, 'eval_steps_per_second': 0.146, 'epoch': 8.0}\n","  3% 88/3300 [11:09<3:36:06,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:16:14,918 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 16:16:14,938 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:16:15,057 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:16:15,062 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:16:15,290 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:12<3:35:40,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 16:17:17,561 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:17:17,561 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:17:17,561 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1291362047195435, 'eval_accuracy': 0.6786703601108033, 'eval_runtime': 20.1677, 'eval_samples_per_second': 35.8, 'eval_steps_per_second': 0.149, 'epoch': 9.0}\n","  3% 99/3300 [12:32<3:35:40,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:17:37,736 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 16:17:37,743 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:17:37,863 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:17:37,868 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:17:38,095 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.7106, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:34<3:35:51,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 16:18:40,477 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:18:40,477 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:18:40,477 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8295572400093079, 'eval_accuracy': 0.7382271468144044, 'eval_runtime': 20.2525, 'eval_samples_per_second': 35.65, 'eval_steps_per_second': 0.148, 'epoch': 10.0}\n","  3% 110/3300 [13:55<3:35:51,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:19:00,736 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 16:19:00,743 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:19:00,873 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:19:00,878 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:19:01,127 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-66] due to args.save_total_limit\n","  4% 121/3300 [14:57<3:31:04,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 16:20:03,467 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:20:03,467 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:20:03,467 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8921493291854858, 'eval_accuracy': 0.7285318559556787, 'eval_runtime': 20.2034, 'eval_samples_per_second': 35.737, 'eval_steps_per_second': 0.148, 'epoch': 11.0}\n","  4% 121/3300 [15:18<3:31:04,  3.98s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:20:23,678 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 16:20:23,685 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:20:23,803 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:20:23,822 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:20:24,058 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [16:21<3:32:57,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 16:21:26,583 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:21:26,583 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:21:26,583 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.4244320392608643, 'eval_accuracy': 0.610803324099723, 'eval_runtime': 20.5387, 'eval_samples_per_second': 35.153, 'eval_steps_per_second': 0.146, 'epoch': 12.0}\n","  4% 132/3300 [16:41<3:32:57,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.81s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:21:47,128 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 16:21:47,135 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:21:47,247 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:21:47,251 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:21:47,486 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:44<3:32:50,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 16:22:49,920 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:22:49,921 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:22:49,921 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1746939420700073, 'eval_accuracy': 0.6911357340720221, 'eval_runtime': 20.2747, 'eval_samples_per_second': 35.611, 'eval_steps_per_second': 0.148, 'epoch': 13.0}\n","  4% 143/3300 [18:04<3:32:50,  4.05s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:23:10,203 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 16:23:10,210 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:23:10,335 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:23:10,339 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:23:10,566 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [19:07<3:31:42,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 16:24:12,988 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:24:12,988 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:24:12,988 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8955464363098145, 'eval_accuracy': 0.7409972299168975, 'eval_runtime': 20.2242, 'eval_samples_per_second': 35.7, 'eval_steps_per_second': 0.148, 'epoch': 14.0}\n","  5% 154/3300 [19:27<3:31:42,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:24:33,219 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 16:24:33,226 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:24:33,338 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:24:33,342 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:24:33,583 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:30<3:35:00,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 16:25:36,152 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:25:36,152 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:25:36,152 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.2381106615066528, 'eval_accuracy': 0.6440443213296398, 'eval_runtime': 20.1241, 'eval_samples_per_second': 35.877, 'eval_steps_per_second': 0.149, 'epoch': 15.0}\n","  5% 165/3300 [20:50<3:35:00,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:25:56,282 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 16:25:56,289 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:25:56,402 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:25:56,407 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:25:56,632 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:53<3:38:11,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 16:26:59,158 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:26:59,158 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:26:59,158 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9979334473609924, 'eval_accuracy': 0.7257617728531855, 'eval_runtime': 20.1828, 'eval_samples_per_second': 35.773, 'eval_steps_per_second': 0.149, 'epoch': 16.0}\n","  5% 176/3300 [22:13<3:38:11,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:27:19,348 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 16:27:19,355 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:27:19,467 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:27:19,472 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:27:19,694 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [23:16<3:37:26,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 16:28:21,967 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:28:21,967 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:28:21,967 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3915315866470337, 'eval_accuracy': 0.6980609418282548, 'eval_runtime': 20.247, 'eval_samples_per_second': 35.66, 'eval_steps_per_second': 0.148, 'epoch': 17.0}\n","  6% 187/3300 [23:36<3:37:26,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:28:42,220 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 16:28:42,226 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:28:42,343 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:28:42,347 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:28:42,587 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [24:39<3:46:38,  4.38s/it][INFO|trainer.py:3213] 2023-10-02 16:29:45,154 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:29:45,154 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:29:45,154 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8361097574234009, 'eval_accuracy': 0.7631578947368421, 'eval_runtime': 20.1953, 'eval_samples_per_second': 35.751, 'eval_steps_per_second': 0.149, 'epoch': 18.0}\n","  6% 198/3300 [24:59<3:46:38,  4.38s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:30:05,361 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 16:30:05,368 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:30:05,483 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:30:05,487 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:30:05,711 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.3124, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [26:02<3:31:42,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 16:31:08,147 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:31:08,147 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:31:08,148 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0696234703063965, 'eval_accuracy': 0.7354570637119113, 'eval_runtime': 20.4038, 'eval_samples_per_second': 35.386, 'eval_steps_per_second': 0.147, 'epoch': 19.0}\n","  6% 209/3300 [26:22<3:31:42,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:31:28,557 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 16:31:28,563 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:31:28,675 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:31:28,679 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:31:28,901 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [27:26<3:29:29,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 16:32:31,576 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:32:31,576 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:32:31,577 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9613627195358276, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 20.4864, 'eval_samples_per_second': 35.243, 'eval_steps_per_second': 0.146, 'epoch': 20.0}\n","  7% 220/3300 [27:46<3:29:29,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:32:52,071 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 16:32:52,078 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:32:52,192 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:32:52,209 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:32:52,436 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-198] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 16:32:52,455 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 16:32:52,455 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/checkpoint-110 (score: 0.8295572400093079).\n","{'train_runtime': 1666.9699, 'train_samples_per_second': 503.188, 'train_steps_per_second': 1.98, 'train_loss': 0.48874137184836647, 'epoch': 20.0}\n","  7% 220/3300 [27:46<6:28:57,  7.58s/it]\n","[INFO|trainer.py:2939] 2023-10-02 16:32:52,522 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/\n","[INFO|configuration_utils.py:460] 2023-10-02 16:32:52,527 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:32:52,639 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:32:52,643 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       20.0\n","  train_loss               =     0.4887\n","  train_runtime            = 0:27:46.96\n","  train_samples_per_second =    503.188\n","  train_steps_per_second   =       1.98\n","[INFO|trainer.py:3213] 2023-10-02 16:32:52,656 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:32:52,656 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:32:52,657 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.80s/it]\n","***** eval metrics *****\n","  epoch                   =       20.0\n","  eval_accuracy           =     0.7382\n","  eval_loss               =     0.8296\n","  eval_runtime            = 0:00:20.16\n","  eval_samples_per_second =     35.803\n","  eval_steps_per_second   =      0.149\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▄▅▃▇▅▆▆▇▇▅▆▇▆▇▇█▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▃▂▂▂▁▂▂▁▁▁▂▁▁▂▁▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▄▃▁▂▅▄█▃▄▃█▄▃▂▃▄▃▆▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▅▆█▇▄▅▁▆▅▆▁▅▆▇▆▅▆▃▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▅▆█▆▃▅▁▆▅▅▁▅▅▆▆▅▆▃▁▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.73823\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.82956\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.1657\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.803\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.149\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 20.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 220\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3124\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 5.656632866390016e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.48874\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1666.9699\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 503.188\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.98\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrose-salad-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/9hfshy3w\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_160458-9hfshy3w/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_3/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 3 \\\n","    --seed 3 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hw_zqhJyFKun","outputId":"2a782de8-b535-491b-a2f2-91ada4575374"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 16:33:28.487187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_163332-dmoyuc88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtreasured-wind-4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/dmoyuc88\u001b[0m\n","10/02/2023 16:33:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 16:33:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=4,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/runs/Oct02_16-33-33_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=4,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 67626.31it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 308964.01it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 16:33:37,809 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 16:33:37,811 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 16:33:37,813 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 16:33:37,935 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 16:33:37,935 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 16:33:38,204 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 16:33:38,205 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 16:33:38,207 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 16:33:38,207 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 16:33:40,171 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 16:33:40,172 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 16:33:40,172 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 16:33:40,172 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 16:33:40,172 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 16:33:40,172 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 16:33:40,173 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 16:33:40,173 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 16:33:40,174 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:07<3:28:32,  3.80s/it][INFO|trainer.py:3213] 2023-10-02 16:34:47,914 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:34:47,915 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:34:47,915 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 4.56152868270874, 'eval_accuracy': 0.31024930747922436, 'eval_runtime': 20.532, 'eval_samples_per_second': 35.165, 'eval_steps_per_second': 0.146, 'epoch': 1.0}\n","  0% 11/3300 [01:28<3:28:32,  3.80s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:35:08,454 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 16:35:08,461 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:35:08,582 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:35:08,586 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:30<3:44:08,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 16:36:11,163 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:36:11,163 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:36:11,163 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.399855136871338, 'eval_accuracy': 0.6094182825484764, 'eval_runtime': 20.2636, 'eval_samples_per_second': 35.63, 'eval_steps_per_second': 0.148, 'epoch': 2.0}\n","  1% 22/3300 [02:51<3:44:08,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:36:31,433 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 16:36:31,439 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:36:31,552 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:36:31,556 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:53<3:44:07,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 16:37:34,137 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:37:34,137 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:37:34,138 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4610397815704346, 'eval_accuracy': 0.628808864265928, 'eval_runtime': 20.1083, 'eval_samples_per_second': 35.906, 'eval_steps_per_second': 0.149, 'epoch': 3.0}\n","  1% 33/3300 [04:14<3:44:07,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:37:54,253 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 16:37:54,260 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:37:54,376 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:37:54,380 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:17<3:43:58,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 16:38:57,528 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:38:57,528 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:38:57,528 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.9268983602523804, 'eval_accuracy': 0.5803324099722992, 'eval_runtime': 20.1759, 'eval_samples_per_second': 35.785, 'eval_steps_per_second': 0.149, 'epoch': 4.0}\n","  1% 44/3300 [05:37<3:43:58,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:39:17,711 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 16:39:17,717 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:39:17,834 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:39:17,838 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:39:18,063 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:40<3:51:42,  4.28s/it][INFO|trainer.py:3213] 2023-10-02 16:40:20,808 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:40:20,809 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:40:20,809 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.7246441841125488, 'eval_accuracy': 0.556786703601108, 'eval_runtime': 20.2752, 'eval_samples_per_second': 35.61, 'eval_steps_per_second': 0.148, 'epoch': 5.0}\n","  2% 55/3300 [07:00<3:51:42,  4.28s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:40:41,091 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 16:40:41,097 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:40:41,210 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:40:41,215 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:40:41,436 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-33] due to args.save_total_limit\n","  2% 66/3300 [08:03<3:35:57,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 16:41:43,993 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:41:43,994 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:41:43,994 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2799170017242432, 'eval_accuracy': 0.6620498614958449, 'eval_runtime': 20.3162, 'eval_samples_per_second': 35.538, 'eval_steps_per_second': 0.148, 'epoch': 6.0}\n","  2% 66/3300 [08:24<3:35:57,  4.01s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:42:04,318 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 16:42:04,324 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:42:04,439 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:42:04,458 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:42:04,684 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-22] due to args.save_total_limit\n","  2% 77/3300 [09:26<3:45:42,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 16:43:07,035 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:43:07,036 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:43:07,036 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.8421756029129028, 'eval_accuracy': 0.556786703601108, 'eval_runtime': 20.3239, 'eval_samples_per_second': 35.525, 'eval_steps_per_second': 0.148, 'epoch': 7.0}\n","  2% 77/3300 [09:47<3:45:42,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:43:27,365 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 16:43:27,371 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:43:27,484 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:43:27,488 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:43:27,712 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:50<4:01:59,  4.52s/it][INFO|trainer.py:3213] 2023-10-02 16:44:30,581 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:44:30,582 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:44:30,582 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2037831544876099, 'eval_accuracy': 0.6634349030470914, 'eval_runtime': 20.2916, 'eval_samples_per_second': 35.581, 'eval_steps_per_second': 0.148, 'epoch': 8.0}\n","  3% 88/3300 [11:10<4:01:59,  4.52s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:44:50,882 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 16:44:50,888 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:44:51,001 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:44:51,005 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:44:51,226 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:13<3:52:58,  4.37s/it][INFO|trainer.py:3213] 2023-10-02 16:45:54,099 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:45:54,100 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:45:54,100 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1847862005233765, 'eval_accuracy': 0.6717451523545707, 'eval_runtime': 20.1944, 'eval_samples_per_second': 35.752, 'eval_steps_per_second': 0.149, 'epoch': 9.0}\n","  3% 99/3300 [12:34<3:52:58,  4.37s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:46:14,301 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 16:46:14,308 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:46:14,425 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:46:14,429 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:46:14,675 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-66] due to args.save_total_limit\n","{'loss': 0.7331, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:37<3:42:36,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 16:47:17,595 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:47:17,595 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:47:17,595 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9982007145881653, 'eval_accuracy': 0.7202216066481995, 'eval_runtime': 20.2704, 'eval_samples_per_second': 35.618, 'eval_steps_per_second': 0.148, 'epoch': 10.0}\n","  3% 110/3300 [13:57<3:42:36,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:47:37,873 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 16:47:37,879 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:47:37,994 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:47:37,998 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:47:38,229 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-77] due to args.save_total_limit\n","  4% 121/3300 [15:00<3:37:23,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 16:48:41,040 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:48:41,040 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:48:41,040 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 2.401190757751465, 'eval_accuracy': 0.5346260387811634, 'eval_runtime': 20.3092, 'eval_samples_per_second': 35.55, 'eval_steps_per_second': 0.148, 'epoch': 11.0}\n","  4% 121/3300 [15:21<3:37:23,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:49:01,356 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 16:49:01,362 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:49:01,482 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:49:01,486 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:49:01,728 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [16:24<3:35:42,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 16:50:05,027 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:50:05,027 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:50:05,027 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8311831951141357, 'eval_accuracy': 0.7437673130193906, 'eval_runtime': 20.2791, 'eval_samples_per_second': 35.603, 'eval_steps_per_second': 0.148, 'epoch': 12.0}\n","  4% 132/3300 [16:45<3:35:42,  4.09s/it]\n","100% 3/3 [00:05<00:00,  2.73s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:50:25,313 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 16:50:25,320 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:50:25,446 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:50:25,451 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:50:25,713 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:48<3:29:57,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 16:51:28,564 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:51:28,565 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:51:28,565 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9620856642723083, 'eval_accuracy': 0.7382271468144044, 'eval_runtime': 20.4801, 'eval_samples_per_second': 35.254, 'eval_steps_per_second': 0.146, 'epoch': 13.0}\n","  4% 143/3300 [18:08<3:29:57,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.78s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:51:49,053 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 16:51:49,059 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:51:49,179 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:51:49,183 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:51:49,424 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [19:11<3:26:15,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 16:52:52,148 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:52:52,148 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:52:52,148 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.668608546257019, 'eval_accuracy': 0.6246537396121884, 'eval_runtime': 20.2289, 'eval_samples_per_second': 35.691, 'eval_steps_per_second': 0.148, 'epoch': 14.0}\n","  5% 154/3300 [19:32<3:26:15,  3.93s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:53:12,385 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 16:53:12,392 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:53:12,511 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:53:12,515 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:53:12,759 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [20:35<3:37:09,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 16:54:15,299 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:54:15,299 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:54:15,299 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.4012486934661865, 'eval_accuracy': 0.6717451523545707, 'eval_runtime': 20.1836, 'eval_samples_per_second': 35.772, 'eval_steps_per_second': 0.149, 'epoch': 15.0}\n","  5% 165/3300 [20:55<3:37:09,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:54:35,489 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 16:54:35,495 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:54:35,608 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:54:35,612 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:54:35,859 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:58<3:44:01,  4.30s/it][INFO|trainer.py:3213] 2023-10-02 16:55:39,108 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:55:39,108 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:55:39,108 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1270657777786255, 'eval_accuracy': 0.7105263157894737, 'eval_runtime': 20.155, 'eval_samples_per_second': 35.822, 'eval_steps_per_second': 0.149, 'epoch': 16.0}\n","  5% 176/3300 [22:19<3:44:01,  4.30s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:55:59,269 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 16:55:59,275 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:55:59,393 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:55:59,397 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:55:59,632 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [23:22<3:37:45,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 16:57:02,302 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:57:02,303 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:57:02,303 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.4433674812316895, 'eval_accuracy': 0.6371191135734072, 'eval_runtime': 20.2895, 'eval_samples_per_second': 35.585, 'eval_steps_per_second': 0.148, 'epoch': 17.0}\n","  6% 187/3300 [23:42<3:37:45,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:57:22,600 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 16:57:22,606 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:57:22,732 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:57:22,736 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:57:22,962 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [24:45<3:35:10,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 16:58:25,590 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:58:25,591 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:58:25,591 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.6486613750457764, 'eval_accuracy': 0.6301939058171745, 'eval_runtime': 20.2474, 'eval_samples_per_second': 35.659, 'eval_steps_per_second': 0.148, 'epoch': 18.0}\n","  6% 198/3300 [25:05<3:35:10,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 16:58:45,846 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 16:58:45,853 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 16:58:45,968 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 16:58:45,972 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 16:58:46,202 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.3019, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [26:08<3:45:42,  4.38s/it][INFO|trainer.py:3213] 2023-10-02 16:59:48,827 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 16:59:48,827 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 16:59:48,827 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9624691009521484, 'eval_accuracy': 0.7506925207756233, 'eval_runtime': 20.2679, 'eval_samples_per_second': 35.623, 'eval_steps_per_second': 0.148, 'epoch': 19.0}\n","  6% 209/3300 [26:28<3:45:42,  4.38s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:00:09,103 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 17:00:09,109 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:00:09,222 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:00:09,226 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:00:09,452 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-187] due to args.save_total_limit\n","  7% 220/3300 [27:31<3:39:31,  4.28s/it][INFO|trainer.py:3213] 2023-10-02 17:01:11,988 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:01:11,988 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:01:11,988 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.14260995388031, 'eval_accuracy': 0.7451523545706371, 'eval_runtime': 20.1623, 'eval_samples_per_second': 35.809, 'eval_steps_per_second': 0.149, 'epoch': 20.0}\n","  7% 220/3300 [27:51<3:39:31,  4.28s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:01:32,156 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 17:01:32,163 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:01:32,292 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:01:32,298 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:01:32,531 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [28:55<3:45:30,  4.41s/it][INFO|trainer.py:3213] 2023-10-02 17:02:35,540 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:02:35,540 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:02:35,540 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0741136074066162, 'eval_accuracy': 0.7437673130193906, 'eval_runtime': 20.2652, 'eval_samples_per_second': 35.628, 'eval_steps_per_second': 0.148, 'epoch': 21.0}\n","  7% 231/3300 [29:15<3:45:30,  4.41s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:02:55,811 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 17:02:55,817 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:02:55,933 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:02:55,937 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:02:56,167 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [30:19<3:44:50,  4.41s/it][INFO|trainer.py:3213] 2023-10-02 17:03:59,317 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:03:59,317 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:03:59,317 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9970870614051819, 'eval_accuracy': 0.7562326869806094, 'eval_runtime': 20.2705, 'eval_samples_per_second': 35.618, 'eval_steps_per_second': 0.148, 'epoch': 22.0}\n","  7% 242/3300 [30:39<3:44:50,  4.41s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:04:19,596 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 17:04:19,602 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:04:19,719 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:04:19,724 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:04:19,952 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-220] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 17:04:19,972 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 17:04:19,973 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/checkpoint-132 (score: 0.8311831951141357).\n","{'train_runtime': 1839.8611, 'train_samples_per_second': 455.904, 'train_steps_per_second': 1.794, 'train_loss': 0.4737587211545834, 'epoch': 22.0}\n","  7% 242/3300 [30:39<6:27:29,  7.60s/it]\n","[INFO|trainer.py:2939] 2023-10-02 17:04:20,039 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/\n","[INFO|configuration_utils.py:460] 2023-10-02 17:04:20,045 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:04:20,163 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:04:20,167 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       22.0\n","  train_loss               =     0.4738\n","  train_runtime            = 0:30:39.86\n","  train_samples_per_second =    455.904\n","  train_steps_per_second   =      1.794\n","[INFO|trainer.py:3213] 2023-10-02 17:04:20,200 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:04:20,200 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:04:20,201 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.79s/it]\n","***** eval metrics *****\n","  epoch                   =       22.0\n","  eval_accuracy           =     0.7438\n","  eval_loss               =     0.8312\n","  eval_runtime            = 0:00:20.33\n","  eval_samples_per_second =     35.499\n","  eval_steps_per_second   =      0.148\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▆▆▅▅▇▅▇▇▇▅██▆▇▇▆▆█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▂▂▃▃▂▃▂▂▁▄▁▁▃▂▂▂▃▁▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▄▁▂▄▄▅▄▂▄▄▄▇▃▂▂▄▃▄▂▄▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▅█▇▅▅▄▅▇▅▅▅▂▆▇▇▅▆▅▇▅▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆██▆▆▆▆█▆▆▆▁▆██▆▆▆█▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.74377\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.83118\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.3386\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.499\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.148\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 22.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 242\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00094\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3019\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 6.222296153029018e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.47376\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1839.8611\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 455.904\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.794\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtreasured-wind-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/dmoyuc88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_163332-dmoyuc88/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_4/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 4 \\\n","    --seed 4 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lGabtHrcFOTk","outputId":"047c68aa-db39-47f9-905f-9e35600f6ce7"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 17:04:54.809057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_170459-y60kbbbp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-hill-5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/y60kbbbp\u001b[0m\n","10/02/2023 17:05:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 17:05:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=5,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/runs/Oct02_17-05-00_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=5,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 52339.81it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 297272.99it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 17:05:04,177 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 17:05:04,179 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 17:05:04,182 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 17:05:04,309 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 17:05:04,309 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 17:05:04,572 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 17:05:04,572 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 17:05:04,574 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 17:05:04,575 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 17:05:06,536 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 17:05:06,536 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 17:05:06,536 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 17:05:06,536 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 17:05:06,537 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 17:05:06,537 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 17:05:06,537 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 17:05:06,537 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 17:05:06,538 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:06<3:47:47,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 17:06:13,420 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:06:13,421 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:06:13,421 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.3907721042633057, 'eval_accuracy': 0.4279778393351801, 'eval_runtime': 20.6082, 'eval_samples_per_second': 35.035, 'eval_steps_per_second': 0.146, 'epoch': 1.0}\n","  0% 11/3300 [01:27<3:47:47,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.74s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:06:34,037 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 17:06:34,043 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:06:34,165 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:06:34,170 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:30<3:44:53,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 17:07:36,853 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:07:36,854 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:07:36,854 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.083123207092285, 'eval_accuracy': 0.4903047091412742, 'eval_runtime': 20.2493, 'eval_samples_per_second': 35.656, 'eval_steps_per_second': 0.148, 'epoch': 2.0}\n","  1% 22/3300 [02:50<3:44:53,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:07:57,120 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 17:07:57,129 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:07:57,247 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:07:57,252 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:53<3:52:11,  4.26s/it][INFO|trainer.py:3213] 2023-10-02 17:09:00,029 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:09:00,029 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:09:00,029 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.2837045192718506, 'eval_accuracy': 0.5290858725761773, 'eval_runtime': 20.0763, 'eval_samples_per_second': 35.963, 'eval_steps_per_second': 0.149, 'epoch': 3.0}\n","  1% 33/3300 [04:13<3:52:11,  4.26s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:09:20,113 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 17:09:20,120 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:09:20,238 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:09:20,243 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:16<3:47:41,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 17:10:23,374 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:10:23,374 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:10:23,375 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4289642572402954, 'eval_accuracy': 0.6121883656509696, 'eval_runtime': 20.1186, 'eval_samples_per_second': 35.887, 'eval_steps_per_second': 0.149, 'epoch': 4.0}\n","  1% 44/3300 [05:36<3:47:41,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:10:43,499 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 17:10:43,505 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:10:43,648 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:10:43,653 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:10:43,875 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:39<3:36:37,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 17:11:46,376 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:11:46,376 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:11:46,376 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3437480926513672, 'eval_accuracy': 0.6412742382271468, 'eval_runtime': 20.0922, 'eval_samples_per_second': 35.934, 'eval_steps_per_second': 0.149, 'epoch': 5.0}\n","  2% 55/3300 [06:59<3:36:37,  4.01s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:12:06,474 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 17:12:06,481 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:12:06,597 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:12:06,601 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:12:06,828 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:02<3:45:55,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 17:13:09,206 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:13:09,207 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:13:09,207 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0266201496124268, 'eval_accuracy': 0.7091412742382271, 'eval_runtime': 20.1123, 'eval_samples_per_second': 35.898, 'eval_steps_per_second': 0.149, 'epoch': 6.0}\n","  2% 66/3300 [08:22<3:45:55,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:13:29,326 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 17:13:29,333 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:13:29,450 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:13:29,454 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:13:29,682 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:25<3:33:35,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 17:14:31,727 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:14:31,728 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:14:31,728 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2960830926895142, 'eval_accuracy': 0.6578947368421053, 'eval_runtime': 20.1211, 'eval_samples_per_second': 35.883, 'eval_steps_per_second': 0.149, 'epoch': 7.0}\n","  2% 77/3300 [09:45<3:33:35,  3.98s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:14:51,856 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 17:14:51,863 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:14:51,979 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:14:51,983 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:14:52,229 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:48<3:45:31,  4.21s/it][INFO|trainer.py:3213] 2023-10-02 17:15:54,601 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:15:54,602 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:15:54,602 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3718500137329102, 'eval_accuracy': 0.6620498614958449, 'eval_runtime': 20.2937, 'eval_samples_per_second': 35.578, 'eval_steps_per_second': 0.148, 'epoch': 8.0}\n","  3% 88/3300 [11:08<3:45:31,  4.21s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:16:14,903 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 17:16:14,910 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:16:15,030 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:16:15,035 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:16:15,270 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:11<3:39:59,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 17:17:17,797 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:17:17,797 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:17:17,797 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3903502225875854, 'eval_accuracy': 0.6731301939058172, 'eval_runtime': 20.3409, 'eval_samples_per_second': 35.495, 'eval_steps_per_second': 0.147, 'epoch': 9.0}\n","  3% 99/3300 [12:31<3:39:59,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.77s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:17:38,144 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 17:17:38,150 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:17:38,264 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:17:38,268 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:17:38,500 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.7008, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:34<3:37:43,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 17:18:41,102 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:18:41,103 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:18:41,103 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.171663761138916, 'eval_accuracy': 0.7049861495844876, 'eval_runtime': 20.1765, 'eval_samples_per_second': 35.784, 'eval_steps_per_second': 0.149, 'epoch': 10.0}\n","  3% 110/3300 [13:54<3:37:43,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:19:01,286 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 17:19:01,292 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:19:01,408 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:19:01,413 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:19:01,658 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [14:57<3:45:26,  4.26s/it][INFO|trainer.py:3213] 2023-10-02 17:20:04,250 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:20:04,251 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:20:04,251 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.223466396331787, 'eval_accuracy': 0.6786703601108033, 'eval_runtime': 20.2184, 'eval_samples_per_second': 35.71, 'eval_steps_per_second': 0.148, 'epoch': 11.0}\n","  4% 121/3300 [15:17<3:45:26,  4.26s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:20:24,476 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 17:20:24,483 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:20:24,607 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:20:24,612 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:20:24,852 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [16:20<3:27:34,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 17:21:27,114 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:21:27,114 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:21:27,114 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.5490349531173706, 'eval_accuracy': 0.6620498614958449, 'eval_runtime': 20.1042, 'eval_samples_per_second': 35.913, 'eval_steps_per_second': 0.149, 'epoch': 12.0}\n","  4% 132/3300 [16:40<3:27:34,  3.93s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:21:47,225 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 17:21:47,232 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:21:47,351 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:21:47,355 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:21:47,593 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [17:43<3:32:46,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 17:22:50,363 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:22:50,364 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:22:50,364 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.4217708110809326, 'eval_accuracy': 0.6620498614958449, 'eval_runtime': 20.2892, 'eval_samples_per_second': 35.585, 'eval_steps_per_second': 0.148, 'epoch': 13.0}\n","  4% 143/3300 [18:04<3:32:46,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:23:10,662 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 17:23:10,669 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:23:10,784 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:23:10,788 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:23:11,038 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [19:07<3:36:04,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 17:24:13,609 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:24:13,610 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:24:13,610 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.390289068222046, 'eval_accuracy': 0.6454293628808865, 'eval_runtime': 20.2795, 'eval_samples_per_second': 35.603, 'eval_steps_per_second': 0.148, 'epoch': 14.0}\n","  5% 154/3300 [19:27<3:36:04,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:24:33,896 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 17:24:33,902 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:24:34,017 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:24:34,021 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:24:34,246 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:30<3:29:17,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 17:25:36,708 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:25:36,708 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:25:36,709 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1959494352340698, 'eval_accuracy': 0.7174515235457064, 'eval_runtime': 20.2567, 'eval_samples_per_second': 35.643, 'eval_steps_per_second': 0.148, 'epoch': 15.0}\n","  5% 165/3300 [20:50<3:29:17,  4.01s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:25:56,973 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 17:25:56,980 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:25:57,097 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:25:57,102 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:25:57,335 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:53<3:31:13,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 17:27:00,156 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:27:00,156 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:27:00,156 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1382267475128174, 'eval_accuracy': 0.7132963988919667, 'eval_runtime': 20.1575, 'eval_samples_per_second': 35.818, 'eval_steps_per_second': 0.149, 'epoch': 16.0}\n","  5% 176/3300 [22:13<3:31:13,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:27:20,321 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 17:27:20,327 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:27:20,440 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:27:20,456 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:27:20,680 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-154] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 17:27:20,696 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 17:27:20,696 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/checkpoint-66 (score: 1.0266201496124268).\n","{'train_runtime': 1334.2198, 'train_samples_per_second': 628.682, 'train_steps_per_second': 2.473, 'train_loss': 0.5379679203033447, 'epoch': 16.0}\n","  5% 176/3300 [22:14<6:34:42,  7.58s/it]\n","[INFO|trainer.py:2939] 2023-10-02 17:27:20,762 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/\n","[INFO|configuration_utils.py:460] 2023-10-02 17:27:20,768 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:27:20,879 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:27:20,883 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       16.0\n","  train_loss               =      0.538\n","  train_runtime            = 0:22:14.21\n","  train_samples_per_second =    628.682\n","  train_steps_per_second   =      2.473\n","[INFO|trainer.py:3213] 2023-10-02 17:27:20,897 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:27:20,898 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:27:20,898 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.80s/it]\n","***** eval metrics *****\n","  epoch                   =       16.0\n","  eval_accuracy           =     0.7091\n","  eval_loss               =     1.0266\n","  eval_runtime            = 0:00:20.41\n","  eval_samples_per_second =     35.364\n","  eval_steps_per_second   =      0.147\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▃▅▆█▇▇▇█▇▇▇▆███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▇▃▃▁▂▃▃▂▂▄▃▃▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▃▁▂▁▁▂▄▄▂▃▁▄▄▃▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▆█▇██▇▅▄▇▆█▅▅▆▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆█████▆▃█▆█▆▆▆█▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.70914\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.02662\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.4162\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.364\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.147\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 16.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 176\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00097\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7008\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4.525306293112013e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.53797\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1334.2198\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 628.682\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.473\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-hill-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/y60kbbbp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_170459-y60kbbbp/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_5/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 5 \\\n","    --seed 5 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"uWs01trKFRid","outputId":"1d170355-4d70-4f1d-acfb-f4428b78be18"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 17:27:53.429422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_172757-9kwy6con\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcolorful-music-6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/9kwy6con\u001b[0m\n","10/02/2023 17:27:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 17:27:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=6,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/runs/Oct02_17-27-58_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=6,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 50926.55it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 279930.01it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 17:28:02,958 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 17:28:02,960 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 17:28:02,962 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 17:28:03,084 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 17:28:03,084 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 17:28:03,349 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 17:28:03,350 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 17:28:03,351 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 17:28:03,352 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 17:28:05,266 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 17:28:05,266 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 17:28:05,266 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 17:28:05,267 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 17:28:05,267 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 17:28:05,267 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 17:28:05,267 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 17:28:05,268 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 17:28:05,268 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:07<3:31:54,  3.87s/it][INFO|trainer.py:3213] 2023-10-02 17:29:12,314 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:29:12,314 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:29:12,314 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 3.9210073947906494, 'eval_accuracy': 0.425207756232687, 'eval_runtime': 20.7384, 'eval_samples_per_second': 34.815, 'eval_steps_per_second': 0.145, 'epoch': 1.0}\n","  0% 11/3300 [01:27<3:31:54,  3.87s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:29:33,058 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 17:29:33,065 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:29:33,181 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:29:33,186 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:30<3:41:33,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 17:30:36,152 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:30:36,153 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:30:36,153 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 3.164733648300171, 'eval_accuracy': 0.38365650969529086, 'eval_runtime': 20.1582, 'eval_samples_per_second': 35.817, 'eval_steps_per_second': 0.149, 'epoch': 2.0}\n","  1% 22/3300 [02:51<3:41:33,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:30:56,317 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 17:30:56,324 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:30:56,455 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:30:56,460 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:53<3:32:05,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 17:31:59,116 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:31:59,117 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:31:59,117 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5511062145233154, 'eval_accuracy': 0.6191135734072022, 'eval_runtime': 20.0931, 'eval_samples_per_second': 35.933, 'eval_steps_per_second': 0.149, 'epoch': 3.0}\n","  1% 33/3300 [04:13<3:32:05,  3.90s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:32:19,217 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 17:32:19,223 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:32:19,338 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:32:19,342 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:16<3:30:42,  3.88s/it][INFO|trainer.py:3213] 2023-10-02 17:33:21,793 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:33:21,794 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:33:21,794 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.7228672504425049, 'eval_accuracy': 0.5831024930747922, 'eval_runtime': 20.1452, 'eval_samples_per_second': 35.84, 'eval_steps_per_second': 0.149, 'epoch': 4.0}\n","  1% 44/3300 [05:36<3:30:42,  3.88s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:33:41,945 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 17:33:41,951 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:33:42,065 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:33:42,070 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:33:42,308 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:39<3:32:12,  3.92s/it][INFO|trainer.py:3213] 2023-10-02 17:34:44,819 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:34:44,820 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:34:44,820 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.0080229043960571, 'eval_accuracy': 0.7326869806094183, 'eval_runtime': 20.1331, 'eval_samples_per_second': 35.861, 'eval_steps_per_second': 0.149, 'epoch': 5.0}\n","  2% 55/3300 [06:59<3:32:12,  3.92s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:35:04,959 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 17:35:04,965 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:35:05,081 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:35:05,085 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:35:05,310 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:02<3:44:10,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 17:36:07,759 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:36:07,759 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:36:07,759 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.571415662765503, 'eval_accuracy': 0.631578947368421, 'eval_runtime': 20.2517, 'eval_samples_per_second': 35.651, 'eval_steps_per_second': 0.148, 'epoch': 6.0}\n","  2% 66/3300 [08:22<3:44:10,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:36:28,017 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 17:36:28,023 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:36:28,141 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:36:28,145 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:36:28,373 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:25<3:46:16,  4.21s/it][INFO|trainer.py:3213] 2023-10-02 17:37:30,795 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:37:30,796 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:37:30,796 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.6570435762405396, 'eval_accuracy': 0.6080332409972299, 'eval_runtime': 20.3288, 'eval_samples_per_second': 35.516, 'eval_steps_per_second': 0.148, 'epoch': 7.0}\n","  2% 77/3300 [09:45<3:46:16,  4.21s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:37:51,131 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 17:37:51,137 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:37:51,250 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:37:51,254 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:37:51,502 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:48<3:33:50,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 17:38:53,497 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:38:53,497 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:38:53,498 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.053338885307312, 'eval_accuracy': 0.7022160664819944, 'eval_runtime': 20.2573, 'eval_samples_per_second': 35.642, 'eval_steps_per_second': 0.148, 'epoch': 8.0}\n","  3% 88/3300 [11:08<3:33:50,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:39:13,761 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 17:39:13,767 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:39:13,886 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:39:13,891 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:39:14,123 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-66] due to args.save_total_limit\n","  3% 99/3300 [12:10<3:39:41,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 17:40:16,078 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:40:16,078 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:40:16,078 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.531996250152588, 'eval_accuracy': 0.6094182825484764, 'eval_runtime': 20.1533, 'eval_samples_per_second': 35.825, 'eval_steps_per_second': 0.149, 'epoch': 9.0}\n","  3% 99/3300 [12:30<3:39:41,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:40:36,238 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 17:40:36,244 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:40:36,360 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:40:36,364 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:40:36,590 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.7032, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:33<3:41:47,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 17:41:38,763 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:41:38,763 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:41:38,764 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.863411009311676, 'eval_accuracy': 0.7479224376731302, 'eval_runtime': 20.426, 'eval_samples_per_second': 35.347, 'eval_steps_per_second': 0.147, 'epoch': 10.0}\n","  3% 110/3300 [13:53<3:41:47,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.73s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:41:59,197 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 17:41:59,204 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:41:59,331 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:41:59,336 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:41:59,624 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-55] due to args.save_total_limit\n","  4% 121/3300 [14:56<3:38:25,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 17:43:02,010 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:43:02,010 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:43:02,010 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9044581651687622, 'eval_accuracy': 0.7645429362880887, 'eval_runtime': 20.333, 'eval_samples_per_second': 35.509, 'eval_steps_per_second': 0.148, 'epoch': 11.0}\n","  4% 121/3300 [15:17<3:38:25,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.72s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:43:22,350 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 17:43:22,357 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:43:22,473 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:43:22,478 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:43:22,712 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-88] due to args.save_total_limit\n","  4% 132/3300 [16:19<3:38:14,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 17:44:24,960 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:44:24,960 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:44:24,961 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9288592338562012, 'eval_accuracy': 0.7590027700831025, 'eval_runtime': 20.3112, 'eval_samples_per_second': 35.547, 'eval_steps_per_second': 0.148, 'epoch': 12.0}\n","  4% 132/3300 [16:39<3:38:14,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:44:45,281 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 17:44:45,301 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:44:45,423 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:44:45,428 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:44:45,677 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:43<3:29:16,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 17:45:49,274 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:45:49,274 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:45:49,274 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8978614807128906, 'eval_accuracy': 0.7465373961218836, 'eval_runtime': 20.2765, 'eval_samples_per_second': 35.608, 'eval_steps_per_second': 0.148, 'epoch': 13.0}\n","  4% 143/3300 [18:04<3:29:16,  3.98s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:46:09,558 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 17:46:09,565 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:46:09,686 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:46:09,691 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:46:09,944 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-121] due to args.save_total_limit\n","  5% 154/3300 [19:07<3:24:05,  3.89s/it][INFO|trainer.py:3213] 2023-10-02 17:47:12,300 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:47:12,300 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:47:12,300 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9291321635246277, 'eval_accuracy': 0.7423822714681441, 'eval_runtime': 20.093, 'eval_samples_per_second': 35.933, 'eval_steps_per_second': 0.149, 'epoch': 14.0}\n","  5% 154/3300 [19:27<3:24:05,  3.89s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:47:32,402 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 17:47:32,408 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:47:32,523 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:47:32,528 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:47:32,760 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:29<3:40:07,  4.21s/it][INFO|trainer.py:3213] 2023-10-02 17:48:35,007 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:48:35,008 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:48:35,008 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.036091923713684, 'eval_accuracy': 0.7409972299168975, 'eval_runtime': 20.136, 'eval_samples_per_second': 35.856, 'eval_steps_per_second': 0.149, 'epoch': 15.0}\n","  5% 165/3300 [20:49<3:40:07,  4.21s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:48:55,149 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 17:48:55,155 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:48:55,270 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:48:55,275 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:48:55,501 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:52<3:35:21,  4.14s/it][INFO|trainer.py:3213] 2023-10-02 17:49:57,582 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:49:57,583 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:49:57,583 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.167363166809082, 'eval_accuracy': 0.7008310249307479, 'eval_runtime': 20.232, 'eval_samples_per_second': 35.686, 'eval_steps_per_second': 0.148, 'epoch': 16.0}\n","  5% 176/3300 [22:12<3:35:21,  4.14s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:50:17,820 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 17:50:17,836 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:50:17,949 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:50:17,954 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:50:18,194 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [23:15<3:31:46,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 17:51:20,730 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:51:20,730 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:51:20,730 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.7439157962799072, 'eval_accuracy': 0.6537396121883656, 'eval_runtime': 20.3818, 'eval_samples_per_second': 35.424, 'eval_steps_per_second': 0.147, 'epoch': 17.0}\n","  6% 187/3300 [23:35<3:31:46,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:51:41,117 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 17:51:41,123 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:51:41,238 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:51:41,242 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:51:41,465 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-165] due to args.save_total_limit\n","  6% 198/3300 [24:38<3:29:45,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 17:52:43,879 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:52:43,879 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:52:43,879 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.2016297578811646, 'eval_accuracy': 0.7285318559556787, 'eval_runtime': 20.3028, 'eval_samples_per_second': 35.562, 'eval_steps_per_second': 0.148, 'epoch': 18.0}\n","  6% 198/3300 [24:58<3:29:45,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:53:04,188 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 17:53:04,194 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:53:04,308 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:53:04,312 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:53:04,540 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-176] due to args.save_total_limit\n","{'loss': 0.3027, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [26:01<3:39:51,  4.27s/it][INFO|trainer.py:3213] 2023-10-02 17:54:07,053 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:54:07,054 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:54:07,054 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.7870882153511047, 'eval_accuracy': 0.775623268698061, 'eval_runtime': 20.3841, 'eval_samples_per_second': 35.42, 'eval_steps_per_second': 0.147, 'epoch': 19.0}\n","  6% 209/3300 [26:22<3:39:51,  4.27s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:54:27,442 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 17:54:27,449 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:54:27,562 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:54:27,566 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:54:27,807 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-110] due to args.save_total_limit\n","  7% 220/3300 [27:24<3:36:58,  4.23s/it][INFO|trainer.py:3213] 2023-10-02 17:55:29,992 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:55:29,992 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:55:29,992 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8025091886520386, 'eval_accuracy': 0.7825484764542936, 'eval_runtime': 20.3128, 'eval_samples_per_second': 35.544, 'eval_steps_per_second': 0.148, 'epoch': 20.0}\n","  7% 220/3300 [27:45<3:36:58,  4.23s/it]\n","100% 3/3 [00:05<00:00,  2.72s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:55:50,313 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 17:55:50,320 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:55:50,436 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:55:50,440 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:55:50,680 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [28:48<3:49:12,  4.48s/it][INFO|trainer.py:3213] 2023-10-02 17:56:53,395 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:56:53,395 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:56:53,395 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.2378571033477783, 'eval_accuracy': 0.7160664819944599, 'eval_runtime': 20.4348, 'eval_samples_per_second': 35.332, 'eval_steps_per_second': 0.147, 'epoch': 21.0}\n","  7% 231/3300 [29:08<3:49:12,  4.48s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:57:13,835 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 17:57:13,841 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:57:13,958 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:57:13,962 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:57:14,192 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-198] due to args.save_total_limit\n","  7% 242/3300 [30:11<3:27:29,  4.07s/it][INFO|trainer.py:3213] 2023-10-02 17:58:16,302 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:58:16,302 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:58:16,303 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9948828816413879, 'eval_accuracy': 0.7506925207756233, 'eval_runtime': 20.4132, 'eval_samples_per_second': 35.369, 'eval_steps_per_second': 0.147, 'epoch': 22.0}\n","  7% 242/3300 [30:31<3:27:29,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:58:36,721 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 17:58:36,741 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:58:36,860 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:58:36,864 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 17:58:37,110 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [31:34<3:27:09,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 17:59:39,656 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 17:59:39,656 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 17:59:39,656 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9622758626937866, 'eval_accuracy': 0.7576177285318559, 'eval_runtime': 20.2024, 'eval_samples_per_second': 35.738, 'eval_steps_per_second': 0.148, 'epoch': 23.0}\n","  8% 253/3300 [31:54<3:27:09,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 17:59:59,865 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-02 17:59:59,872 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 17:59:59,985 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 17:59:59,989 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:00:00,231 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [32:57<3:37:50,  4.31s/it][INFO|trainer.py:3213] 2023-10-02 18:01:02,529 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:01:02,529 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:01:02,529 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1611907482147217, 'eval_accuracy': 0.7174515235457064, 'eval_runtime': 20.2866, 'eval_samples_per_second': 35.59, 'eval_steps_per_second': 0.148, 'epoch': 24.0}\n","  8% 264/3300 [33:17<3:37:50,  4.31s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:01:22,820 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-02 18:01:22,826 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:01:22,947 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:01:22,951 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:01:23,180 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-242] due to args.save_total_limit\n","  8% 275/3300 [34:20<3:37:02,  4.30s/it][INFO|trainer.py:3213] 2023-10-02 18:02:25,925 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:02:25,925 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:02:25,925 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.120664119720459, 'eval_accuracy': 0.7340720221606648, 'eval_runtime': 20.2352, 'eval_samples_per_second': 35.68, 'eval_steps_per_second': 0.148, 'epoch': 25.0}\n","  8% 275/3300 [34:40<3:37:02,  4.30s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:02:46,165 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-02 18:02:46,171 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:02:46,282 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:02:46,286 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:02:46,505 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [35:43<3:25:09,  4.08s/it][INFO|trainer.py:3213] 2023-10-02 18:03:48,795 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:03:48,796 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:03:48,796 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3584445714950562, 'eval_accuracy': 0.685595567867036, 'eval_runtime': 20.1955, 'eval_samples_per_second': 35.751, 'eval_steps_per_second': 0.149, 'epoch': 26.0}\n","  9% 286/3300 [36:03<3:25:09,  4.08s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:04:08,998 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-02 18:04:09,005 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:04:09,125 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:04:09,130 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:04:09,367 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [37:06<3:28:46,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 18:05:11,953 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:05:11,953 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:05:11,953 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.656501054763794, 'eval_accuracy': 0.6648199445983379, 'eval_runtime': 20.221, 'eval_samples_per_second': 35.705, 'eval_steps_per_second': 0.148, 'epoch': 27.0}\n","  9% 297/3300 [37:26<3:28:46,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:05:32,179 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-02 18:05:32,185 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:05:32,300 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:05:32,304 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:05:32,547 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.2471, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [38:29<3:33:44,  4.29s/it][INFO|trainer.py:3213] 2023-10-02 18:06:35,122 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:06:35,122 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:06:35,123 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9990507364273071, 'eval_accuracy': 0.7631578947368421, 'eval_runtime': 20.3741, 'eval_samples_per_second': 35.437, 'eval_steps_per_second': 0.147, 'epoch': 28.0}\n","  9% 308/3300 [38:50<3:33:44,  4.29s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:06:55,508 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-02 18:06:55,515 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:06:55,636 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:06:55,641 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:06:55,866 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [39:52<3:20:41,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 18:07:57,850 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:07:57,850 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:07:57,850 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1284290552139282, 'eval_accuracy': 0.721606648199446, 'eval_runtime': 20.2203, 'eval_samples_per_second': 35.707, 'eval_steps_per_second': 0.148, 'epoch': 29.0}\n"," 10% 319/3300 [40:12<3:20:41,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:08:18,076 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-02 18:08:18,082 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:08:18,194 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:08:18,198 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:08:18,417 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-297] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 18:08:18,436 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 18:08:18,437 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/checkpoint-209 (score: 0.7870882153511047).\n","{'train_runtime': 2413.2299, 'train_samples_per_second': 347.584, 'train_steps_per_second': 1.367, 'train_loss': 0.4070381849163378, 'epoch': 29.0}\n"," 10% 319/3300 [40:13<6:15:51,  7.56s/it]\n","[INFO|trainer.py:2939] 2023-10-02 18:08:18,502 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/\n","[INFO|configuration_utils.py:460] 2023-10-02 18:08:18,507 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:08:18,619 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:08:18,623 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       29.0\n","  train_loss               =      0.407\n","  train_runtime            = 0:40:13.22\n","  train_samples_per_second =    347.584\n","  train_steps_per_second   =      1.367\n","[INFO|trainer.py:3213] 2023-10-02 18:08:18,654 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:08:18,654 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:08:18,654 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.79s/it]\n","***** eval metrics *****\n","  epoch                   =       29.0\n","  eval_accuracy           =     0.7756\n","  eval_loss               =     0.7871\n","  eval_runtime            = 0:00:20.35\n","  eval_samples_per_second =     35.477\n","  eval_steps_per_second   =      0.147\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▂▁▅▄▇▅▅▇▅▇██▇▇▇▇▆▇██▇▇█▇▇▆▆█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▆▃▃▁▃▃▂▃▁▁▁▁▁▂▂▃▂▁▁▂▁▁▂▂▂▃▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▂▁▂▁▃▄▃▂▅▄▃▃▁▁▃▄▃▄▃▅▄▂▃▃▂▂▄▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▇█▇█▆▅▆▇▄▅▆▆██▆▅▆▅▆▄▄▇▆▆▇▇▅▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁████▆▆▆█▅▆▆▆██▆▅▆▅▆▅▅▆▆▆█▆▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.77562\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.78709\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.3512\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.477\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.147\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 29.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 319\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2471\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 8.202117656265523e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.40704\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2413.2299\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 347.584\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.367\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcolorful-music-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/9kwy6con\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_172757-9kwy6con/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_6/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 6 \\\n","    --seed 6 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C5cP8uzzFVIB","outputId":"ebdb5745-870f-4793-de70-0962b9fdfbd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-02 18:08:54.079239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_180858-edokqlcq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mruby-moon-7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/edokqlcq\u001b[0m\n","10/02/2023 18:08:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 18:08:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=7,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/runs/Oct02_18-08-59_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=7,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 93877.07it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 302070.11it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 18:09:03,662 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 18:09:03,664 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 18:09:03,666 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 18:09:03,786 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 18:09:03,787 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 18:09:04,054 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 18:09:04,054 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 18:09:04,056 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 18:09:04,056 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 18:09:05,957 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 18:09:05,957 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 18:09:05,957 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 18:09:05,957 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 18:09:05,957 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 18:09:05,957 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 18:09:05,958 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 18:09:05,958 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 18:09:05,959 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:06<3:32:37,  3.88s/it][INFO|trainer.py:3213] 2023-10-02 18:10:12,074 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:10:12,074 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:10:12,075 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 3.865736246109009, 'eval_accuracy': 0.4390581717451524, 'eval_runtime': 20.4707, 'eval_samples_per_second': 35.27, 'eval_steps_per_second': 0.147, 'epoch': 1.0}\n","  0% 11/3300 [01:26<3:32:37,  3.88s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:10:32,551 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 18:10:32,557 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:10:32,668 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:10:32,672 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:29<3:31:15,  3.87s/it][INFO|trainer.py:3213] 2023-10-02 18:11:35,105 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:11:35,105 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:11:35,105 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5577818155288696, 'eval_accuracy': 0.5831024930747922, 'eval_runtime': 20.2386, 'eval_samples_per_second': 35.674, 'eval_steps_per_second': 0.148, 'epoch': 2.0}\n","  1% 22/3300 [02:49<3:31:15,  3.87s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:11:55,349 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 18:11:55,355 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:11:55,476 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:11:55,481 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:52<3:39:20,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 18:12:58,538 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:12:58,538 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:12:58,539 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4311273097991943, 'eval_accuracy': 0.6301939058171745, 'eval_runtime': 20.2955, 'eval_samples_per_second': 35.574, 'eval_steps_per_second': 0.148, 'epoch': 3.0}\n","  1% 33/3300 [04:12<3:39:20,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:13:18,839 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 18:13:18,844 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:13:18,957 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:13:18,975 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:15<3:27:45,  3.83s/it][INFO|trainer.py:3213] 2023-10-02 18:14:21,589 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:14:21,590 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:14:21,590 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5382661819458008, 'eval_accuracy': 0.6260387811634349, 'eval_runtime': 20.1474, 'eval_samples_per_second': 35.836, 'eval_steps_per_second': 0.149, 'epoch': 4.0}\n","  1% 44/3300 [05:35<3:27:45,  3.83s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:14:41,743 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 18:14:41,750 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:14:41,867 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:14:41,872 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:14:42,101 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:38<3:37:27,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 18:15:44,092 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:15:44,092 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:15:44,092 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.5502145290374756, 'eval_accuracy': 0.4792243767313019, 'eval_runtime': 20.2597, 'eval_samples_per_second': 35.637, 'eval_steps_per_second': 0.148, 'epoch': 5.0}\n","  2% 55/3300 [06:58<3:37:27,  4.02s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:16:04,358 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 18:16:04,365 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:16:04,486 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:16:04,490 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:16:04,722 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:01<3:44:50,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 18:17:07,060 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:17:07,061 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:17:07,061 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9732878804206848, 'eval_accuracy': 0.7368421052631579, 'eval_runtime': 20.2687, 'eval_samples_per_second': 35.621, 'eval_steps_per_second': 0.148, 'epoch': 6.0}\n","  2% 66/3300 [08:21<3:44:50,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:17:27,336 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 18:17:27,343 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:17:27,459 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:17:27,463 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:17:27,710 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:24<3:47:15,  4.23s/it][INFO|trainer.py:3213] 2023-10-02 18:18:30,128 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:18:30,128 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:18:30,128 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2678016424179077, 'eval_accuracy': 0.6551246537396122, 'eval_runtime': 20.3273, 'eval_samples_per_second': 35.519, 'eval_steps_per_second': 0.148, 'epoch': 7.0}\n","  2% 77/3300 [09:44<3:47:15,  4.23s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:18:50,460 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 18:18:50,467 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:18:50,581 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:18:50,585 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:18:50,813 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:46<3:46:37,  4.23s/it][INFO|trainer.py:3213] 2023-10-02 18:19:52,874 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:19:52,874 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:19:52,875 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.9863582253456116, 'eval_accuracy': 0.7285318559556787, 'eval_runtime': 20.2427, 'eval_samples_per_second': 35.667, 'eval_steps_per_second': 0.148, 'epoch': 8.0}\n","  3% 88/3300 [11:07<3:46:37,  4.23s/it]\n","100% 3/3 [00:05<00:00,  2.72s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:20:13,123 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 18:20:13,130 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:20:13,256 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:20:13,261 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:20:13,493 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:09<3:35:00,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 18:21:15,867 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:21:15,867 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:21:15,867 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1228528022766113, 'eval_accuracy': 0.7022160664819944, 'eval_runtime': 20.2922, 'eval_samples_per_second': 35.58, 'eval_steps_per_second': 0.148, 'epoch': 9.0}\n","  3% 99/3300 [12:30<3:35:00,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:21:36,165 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 18:21:36,171 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:21:36,285 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:21:36,289 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:21:36,524 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.7008, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:32<3:34:48,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 18:22:38,824 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:22:38,825 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:22:38,825 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3770157098770142, 'eval_accuracy': 0.6703601108033241, 'eval_runtime': 20.1579, 'eval_samples_per_second': 35.817, 'eval_steps_per_second': 0.149, 'epoch': 10.0}\n","  3% 110/3300 [13:53<3:34:48,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.72s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:22:58,989 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 18:22:58,996 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:22:59,121 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:22:59,125 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:22:59,375 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [14:55<3:30:38,  3.98s/it][INFO|trainer.py:3213] 2023-10-02 18:24:01,919 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:24:01,920 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:24:01,920 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9442209601402283, 'eval_accuracy': 0.7506925207756233, 'eval_runtime': 20.2696, 'eval_samples_per_second': 35.62, 'eval_steps_per_second': 0.148, 'epoch': 11.0}\n","  4% 121/3300 [15:16<3:30:38,  3.98s/it]\n","100% 3/3 [00:05<00:00,  2.73s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:24:22,196 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 18:24:22,203 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:24:22,322 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:24:22,327 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:24:22,565 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-66] due to args.save_total_limit\n","  4% 132/3300 [16:19<3:33:37,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 18:25:25,346 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:25:25,346 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:25:25,346 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.07481849193573, 'eval_accuracy': 0.7174515235457064, 'eval_runtime': 20.1389, 'eval_samples_per_second': 35.851, 'eval_steps_per_second': 0.149, 'epoch': 12.0}\n","  4% 132/3300 [16:39<3:33:37,  4.05s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:25:45,492 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 18:25:45,498 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:25:45,616 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:25:45,620 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:25:45,862 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:42<3:30:00,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 18:26:48,200 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:26:48,200 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:26:48,201 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0212430953979492, 'eval_accuracy': 0.7299168975069252, 'eval_runtime': 20.0611, 'eval_samples_per_second': 35.99, 'eval_steps_per_second': 0.15, 'epoch': 13.0}\n","  4% 143/3300 [18:02<3:30:00,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:27:08,268 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 18:27:08,275 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:27:08,389 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:27:08,393 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:27:08,620 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [19:05<3:35:35,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 18:28:11,108 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:28:11,108 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:28:11,108 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.24198579788208, 'eval_accuracy': 0.6717451523545707, 'eval_runtime': 20.2247, 'eval_samples_per_second': 35.699, 'eval_steps_per_second': 0.148, 'epoch': 14.0}\n","  5% 154/3300 [19:25<3:35:35,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:28:31,340 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 18:28:31,346 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:28:31,462 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:28:31,466 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:28:31,695 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:28<3:35:52,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 18:29:34,464 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:29:34,464 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:29:34,464 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3302810192108154, 'eval_accuracy': 0.6939058171745153, 'eval_runtime': 20.1213, 'eval_samples_per_second': 35.882, 'eval_steps_per_second': 0.149, 'epoch': 15.0}\n","  5% 165/3300 [20:48<3:35:52,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:29:54,596 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 18:29:54,602 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:29:54,713 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:29:54,717 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:29:54,956 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:52<3:37:21,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 18:30:58,239 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:30:58,239 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:30:58,240 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.4461395740509033, 'eval_accuracy': 0.6759002770083102, 'eval_runtime': 20.1072, 'eval_samples_per_second': 35.908, 'eval_steps_per_second': 0.149, 'epoch': 16.0}\n","  5% 176/3300 [22:12<3:37:21,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:31:18,354 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 18:31:18,361 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:31:18,476 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:31:18,481 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:31:18,711 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [23:14<3:36:55,  4.18s/it][INFO|trainer.py:3213] 2023-10-02 18:32:20,957 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:32:20,957 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:32:20,957 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9068929553031921, 'eval_accuracy': 0.7423822714681441, 'eval_runtime': 20.4682, 'eval_samples_per_second': 35.274, 'eval_steps_per_second': 0.147, 'epoch': 17.0}\n","  6% 187/3300 [23:35<3:36:55,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:32:41,432 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 18:32:41,438 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:32:41,556 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:32:41,560 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:32:41,789 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-121] due to args.save_total_limit\n","  6% 198/3300 [24:38<3:33:37,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 18:33:44,651 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:33:44,651 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:33:44,651 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0668423175811768, 'eval_accuracy': 0.7202216066481995, 'eval_runtime': 20.1925, 'eval_samples_per_second': 35.756, 'eval_steps_per_second': 0.149, 'epoch': 18.0}\n","  6% 198/3300 [24:58<3:33:37,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:34:04,850 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 18:34:04,856 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:34:04,974 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:34:04,979 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:34:05,222 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-165] due to args.save_total_limit\n","{'loss': 0.3125, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [26:01<3:34:25,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 18:35:07,715 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:35:07,716 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:35:07,716 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3935871124267578, 'eval_accuracy': 0.6565096952908587, 'eval_runtime': 20.1932, 'eval_samples_per_second': 35.755, 'eval_steps_per_second': 0.149, 'epoch': 19.0}\n","  6% 209/3300 [26:21<3:34:25,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:35:27,915 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 18:35:27,921 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:35:28,039 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:35:28,043 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:35:28,273 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-176] due to args.save_total_limit\n","  7% 220/3300 [27:24<3:31:13,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 18:36:30,958 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:36:30,959 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:36:30,959 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.20555579662323, 'eval_accuracy': 0.6966759002770083, 'eval_runtime': 20.3079, 'eval_samples_per_second': 35.553, 'eval_steps_per_second': 0.148, 'epoch': 20.0}\n","  7% 220/3300 [27:45<3:31:13,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.72s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:36:51,272 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 18:36:51,279 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:36:51,397 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:36:51,401 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:36:51,635 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [28:48<3:38:34,  4.27s/it][INFO|trainer.py:3213] 2023-10-02 18:37:54,610 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:37:54,611 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:37:54,611 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.2369837760925293, 'eval_accuracy': 0.6634349030470914, 'eval_runtime': 20.2114, 'eval_samples_per_second': 35.722, 'eval_steps_per_second': 0.148, 'epoch': 21.0}\n","  7% 231/3300 [29:08<3:38:34,  4.27s/it]\n","100% 3/3 [00:05<00:00,  2.72s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:38:14,829 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 18:38:14,835 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:38:14,949 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:38:14,954 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:38:15,202 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [30:11<3:27:36,  4.07s/it][INFO|trainer.py:3213] 2023-10-02 18:39:17,712 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:39:17,713 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:39:17,713 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.6487504243850708, 'eval_accuracy': 0.6578947368421053, 'eval_runtime': 20.2365, 'eval_samples_per_second': 35.678, 'eval_steps_per_second': 0.148, 'epoch': 22.0}\n","  7% 242/3300 [30:31<3:27:36,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.74s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:39:37,957 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 18:39:37,963 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:39:38,084 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:39:38,089 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:39:38,326 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [31:35<3:35:28,  4.24s/it][INFO|trainer.py:3213] 2023-10-02 18:40:40,983 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:40:40,983 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:40:40,984 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.7562131881713867, 'eval_accuracy': 0.6662049861495845, 'eval_runtime': 20.3896, 'eval_samples_per_second': 35.41, 'eval_steps_per_second': 0.147, 'epoch': 23.0}\n","  8% 253/3300 [31:55<3:35:28,  4.24s/it]\n","100% 3/3 [00:05<00:00,  2.77s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:41:01,378 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-02 18:41:01,385 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:41:01,506 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:41:01,511 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:41:01,750 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-231] due to args.save_total_limit\n","  8% 264/3300 [32:58<3:25:41,  4.07s/it][INFO|trainer.py:3213] 2023-10-02 18:42:04,107 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:42:04,108 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:42:04,108 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8947951793670654, 'eval_accuracy': 0.7451523545706371, 'eval_runtime': 20.3151, 'eval_samples_per_second': 35.54, 'eval_steps_per_second': 0.148, 'epoch': 24.0}\n","  8% 264/3300 [33:18<3:25:41,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:42:24,434 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-02 18:42:24,441 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:42:24,560 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:42:24,564 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:42:24,815 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-187] due to args.save_total_limit\n","  8% 275/3300 [34:21<3:36:43,  4.30s/it][INFO|trainer.py:3213] 2023-10-02 18:43:27,524 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:43:27,524 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:43:27,524 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.217234492301941, 'eval_accuracy': 0.7049861495844876, 'eval_runtime': 20.2667, 'eval_samples_per_second': 35.625, 'eval_steps_per_second': 0.148, 'epoch': 25.0}\n","  8% 275/3300 [34:41<3:36:43,  4.30s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:43:47,797 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-02 18:43:47,804 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:43:47,922 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:43:47,927 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:43:48,162 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-242] due to args.save_total_limit\n","  9% 286/3300 [35:44<3:30:29,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 18:44:50,521 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:44:50,521 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:44:50,521 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1074787378311157, 'eval_accuracy': 0.7285318559556787, 'eval_runtime': 20.202, 'eval_samples_per_second': 35.739, 'eval_steps_per_second': 0.149, 'epoch': 26.0}\n","  9% 286/3300 [36:04<3:30:29,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:45:10,728 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-02 18:45:10,734 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:45:10,848 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:45:10,852 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:45:11,078 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-253] due to args.save_total_limit\n","  9% 297/3300 [37:07<3:23:19,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 18:46:13,396 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:46:13,396 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:46:13,396 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9262974858283997, 'eval_accuracy': 0.7853185595567868, 'eval_runtime': 20.1142, 'eval_samples_per_second': 35.895, 'eval_steps_per_second': 0.149, 'epoch': 27.0}\n","  9% 297/3300 [37:27<3:23:19,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:46:33,516 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-02 18:46:33,523 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:46:33,641 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:46:33,646 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:46:33,888 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.2369, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [38:30<3:20:24,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 18:47:36,086 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:47:36,086 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:47:36,086 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8177291750907898, 'eval_accuracy': 0.7645429362880887, 'eval_runtime': 20.245, 'eval_samples_per_second': 35.663, 'eval_steps_per_second': 0.148, 'epoch': 28.0}\n","  9% 308/3300 [38:50<3:20:24,  4.02s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:47:56,337 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-02 18:47:56,344 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:47:56,459 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:47:56,463 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:47:56,685 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-264] due to args.save_total_limit\n"," 10% 319/3300 [39:53<3:28:34,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 18:48:59,257 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:48:59,258 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:48:59,258 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.192807912826538, 'eval_accuracy': 0.7423822714681441, 'eval_runtime': 20.3453, 'eval_samples_per_second': 35.487, 'eval_steps_per_second': 0.147, 'epoch': 29.0}\n"," 10% 319/3300 [40:13<3:28:34,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:49:19,609 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-02 18:49:19,616 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:49:19,738 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:49:19,742 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:49:19,981 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-286] due to args.save_total_limit\n"," 10% 330/3300 [41:16<3:26:59,  4.18s/it][INFO|trainer.py:3213] 2023-10-02 18:50:22,182 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:50:22,183 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:50:22,183 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9711021184921265, 'eval_accuracy': 0.7714681440443213, 'eval_runtime': 20.2074, 'eval_samples_per_second': 35.729, 'eval_steps_per_second': 0.148, 'epoch': 30.0}\n"," 10% 330/3300 [41:36<3:26:59,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:50:42,396 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-330\n","[INFO|configuration_utils.py:460] 2023-10-02 18:50:42,402 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-330/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:50:42,518 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-330/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:50:42,522 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-330/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:50:42,762 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-297] due to args.save_total_limit\n"," 10% 341/3300 [42:39<3:26:55,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 18:51:45,099 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:51:45,100 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:51:45,100 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0221964120864868, 'eval_accuracy': 0.7506925207756233, 'eval_runtime': 20.2838, 'eval_samples_per_second': 35.595, 'eval_steps_per_second': 0.148, 'epoch': 31.0}\n"," 10% 341/3300 [42:59<3:26:55,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:52:05,389 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-341\n","[INFO|configuration_utils.py:460] 2023-10-02 18:52:05,395 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-341/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:52:05,511 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-341/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:52:05,515 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-341/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:52:05,743 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-319] due to args.save_total_limit\n"," 11% 352/3300 [44:01<3:22:46,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 18:53:07,840 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:53:07,840 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:53:07,840 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9932622313499451, 'eval_accuracy': 0.7742382271468145, 'eval_runtime': 20.4265, 'eval_samples_per_second': 35.346, 'eval_steps_per_second': 0.147, 'epoch': 32.0}\n"," 11% 352/3300 [44:22<3:22:46,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:53:28,272 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-352\n","[INFO|configuration_utils.py:460] 2023-10-02 18:53:28,278 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-352/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:53:28,390 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-352/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:53:28,394 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-352/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:53:28,612 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-330] due to args.save_total_limit\n"," 11% 363/3300 [45:24<3:24:42,  4.18s/it][INFO|trainer.py:3213] 2023-10-02 18:54:30,643 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:54:30,644 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:54:30,644 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0212531089782715, 'eval_accuracy': 0.7617728531855956, 'eval_runtime': 20.1933, 'eval_samples_per_second': 35.754, 'eval_steps_per_second': 0.149, 'epoch': 33.0}\n"," 11% 363/3300 [45:44<3:24:42,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:54:50,847 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-363\n","[INFO|configuration_utils.py:460] 2023-10-02 18:54:50,853 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-363/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:54:50,972 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-363/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:54:50,978 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-363/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:54:51,218 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-341] due to args.save_total_limit\n"," 11% 374/3300 [46:47<3:25:45,  4.22s/it][INFO|trainer.py:3213] 2023-10-02 18:55:53,554 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:55:53,555 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:55:53,555 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9715341925621033, 'eval_accuracy': 0.7645429362880887, 'eval_runtime': 20.2871, 'eval_samples_per_second': 35.589, 'eval_steps_per_second': 0.148, 'epoch': 34.0}\n"," 11% 374/3300 [47:07<3:25:45,  4.22s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:56:13,847 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-374\n","[INFO|configuration_utils.py:460] 2023-10-02 18:56:13,854 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-374/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:56:13,979 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-374/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:56:13,984 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-374/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:56:14,245 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-352] due to args.save_total_limit\n"," 12% 385/3300 [48:10<3:23:42,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 18:57:16,611 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:57:16,612 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:57:16,612 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1873670816421509, 'eval_accuracy': 0.7368421052631579, 'eval_runtime': 20.237, 'eval_samples_per_second': 35.677, 'eval_steps_per_second': 0.148, 'epoch': 35.0}\n"," 12% 385/3300 [48:30<3:23:42,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:57:36,854 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-385\n","[INFO|configuration_utils.py:460] 2023-10-02 18:57:36,861 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-385/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:57:36,979 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-385/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:57:36,983 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-385/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:57:37,216 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-363] due to args.save_total_limit\n"," 12% 396/3300 [49:33<3:15:38,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 18:58:39,437 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 18:58:39,437 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 18:58:39,437 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9772872924804688, 'eval_accuracy': 0.7562326869806094, 'eval_runtime': 20.3756, 'eval_samples_per_second': 35.434, 'eval_steps_per_second': 0.147, 'epoch': 36.0}\n"," 12% 396/3300 [49:53<3:15:38,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 18:58:59,817 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-396\n","[INFO|configuration_utils.py:460] 2023-10-02 18:58:59,823 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-396/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 18:58:59,939 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-396/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 18:58:59,944 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-396/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 18:59:00,180 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-374] due to args.save_total_limit\n","{'loss': 0.202, 'learning_rate': 0.0008787878787878789, 'epoch': 36.36}\n"," 12% 407/3300 [50:56<3:21:16,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 19:00:02,529 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:00:02,530 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:00:02,530 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9965107440948486, 'eval_accuracy': 0.760387811634349, 'eval_runtime': 20.2996, 'eval_samples_per_second': 35.567, 'eval_steps_per_second': 0.148, 'epoch': 37.0}\n"," 12% 407/3300 [51:16<3:21:16,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:00:22,837 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-407\n","[INFO|configuration_utils.py:460] 2023-10-02 19:00:22,843 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-407/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:00:22,963 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-407/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:00:22,968 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-407/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:00:23,218 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-385] due to args.save_total_limit\n"," 13% 418/3300 [52:19<3:21:51,  4.20s/it][INFO|trainer.py:3213] 2023-10-02 19:01:25,536 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:01:25,536 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:01:25,536 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9418509006500244, 'eval_accuracy': 0.7839335180055401, 'eval_runtime': 20.335, 'eval_samples_per_second': 35.505, 'eval_steps_per_second': 0.148, 'epoch': 38.0}\n"," 13% 418/3300 [52:39<3:21:51,  4.20s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:01:45,878 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-418\n","[INFO|configuration_utils.py:460] 2023-10-02 19:01:45,884 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-418/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:01:45,998 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-418/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:01:46,002 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-418/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:01:46,222 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-396] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 19:01:46,252 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 19:01:46,252 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/checkpoint-308 (score: 0.8177291750907898).\n","{'train_runtime': 3160.3485, 'train_samples_per_second': 265.414, 'train_steps_per_second': 1.044, 'train_loss': 0.3558484469874624, 'epoch': 38.0}\n"," 13% 418/3300 [52:40<6:03:09,  7.56s/it]\n","[INFO|trainer.py:2939] 2023-10-02 19:01:46,311 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/\n","[INFO|configuration_utils.py:460] 2023-10-02 19:01:46,316 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:01:46,429 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:01:46,432 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       38.0\n","  train_loss               =     0.3558\n","  train_runtime            = 0:52:40.34\n","  train_samples_per_second =    265.414\n","  train_steps_per_second   =      1.044\n","[INFO|trainer.py:3213] 2023-10-02 19:01:46,446 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:01:46,446 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:01:46,447 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.83s/it]\n","***** eval metrics *****\n","  epoch                   =       38.0\n","  eval_accuracy           =     0.7645\n","  eval_loss               =     0.8177\n","  eval_runtime            = 0:00:20.32\n","  eval_samples_per_second =     35.528\n","  eval_steps_per_second   =      0.148\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▅▅▂▇▅▇▆▆▇▇▇▆▆▆▇▇▅▆▆▅▆▇▆▇██▇█▇███▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▃▂▃▅▁▂▁▂▂▁▂▁▂▂▂▁▂▂▂▂▃▃▁▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▄▅▂▄▅▆▄▅▃▅▂▁▄▂▂█▃▃▅▄▄▇▅▅▃▂▄▆▄▅▇▃▅▄▆▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▅▄▇▅▄▃▅▄▆▄▇█▅▇▇▁▆▆▄▅▅▂▄▄▆▇▅▃▅▄▂▆▄▅▃▄▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▃▃▆▃▃▃▃▃▆▃▆█▃▆▆▁▆▆▃▃▃▁▃▃▆▆▃▁▃▃▁▆▃▃▁▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.76454\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.81773\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.3222\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.528\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.148\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 38.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 418\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00088\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.202\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.074760244614103e+18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.35585\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 3160.3485\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 265.414\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.044\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mruby-moon-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/edokqlcq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_180858-edokqlcq/logs\u001b[0m\n","Exception in thread NetStatThr:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","Exception in thread IntMsgThr:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 267, in check_network_status\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 299, in check_internal_messages\n","    self._loop_check_status(\n","    self._loop_check_status(\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n","    local_handle = request()\n","    local_handle = request()\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 735, in deliver_network_status\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 743, in deliver_internal_messages\n","    return self._deliver_network_status(status)\n","    return self._deliver_internal_messages(internal_message)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 466, in _deliver_network_status\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 472, in _deliver_internal_messages\n","    return self._deliver_record(record)\n","    return self._deliver_record(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n","    handle = mailbox._deliver_record(record, interface=self)\n","    handle = mailbox._deliver_record(record, interface=self)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n","    interface._publish(record)\n","    interface._publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n","    self._sock_client.send_record_publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n","    self._sock_client.send_record_publish(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n","    self.send_server_request(server_req)\n","    self.send_server_request(server_req)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n","    self._send_message(msg)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n","    self._send_message(msg)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n","    self._sendall_with_error_handle(header + data)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n","    self._sendall_with_error_handle(header + data)\n","  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n","    sent = self._sock.send(data)\n","BrokenPipeError: [Errno 32] Broken pipe\n","    sent = self._sock.send(data)\n","BrokenPipeError: [Errno 32] Broken pipe\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_7/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 7 \\\n","    --seed 7 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nDNgY-PFYRZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696276109404,"user_tz":-120,"elapsed":1862836,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"}},"outputId":"9e56860d-d2a1-4dad-cce5-201940289dbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-02 19:02:21.568233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_190226-1q7dc0mo\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlegendary-lake-8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/1q7dc0mo\u001b[0m\n","10/02/2023 19:02:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 19:02:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=8,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/runs/Oct02_19-02-26_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=8,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 42105.54it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 301679.45it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 19:02:31,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 19:02:31,050 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 19:02:31,053 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 19:02:31,176 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 19:02:31,176 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 19:02:31,439 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 19:02:31,439 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 19:02:31,441 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 19:02:31,441 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 19:02:33,419 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 19:02:33,420 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 19:02:33,420 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 19:02:33,420 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 19:02:33,420 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 19:02:33,420 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 19:02:33,421 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 19:02:33,421 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 19:02:33,422 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:07<3:46:19,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 19:03:40,600 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:03:40,601 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:03:40,601 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 3.4115259647369385, 'eval_accuracy': 0.3781163434903047, 'eval_runtime': 20.5824, 'eval_samples_per_second': 35.078, 'eval_steps_per_second': 0.146, 'epoch': 1.0}\n","  0% 11/3300 [01:27<3:46:19,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:04:01,190 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 19:04:01,197 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:04:01,321 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:04:01,339 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:30<3:40:26,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 19:05:03,989 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:05:03,989 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:05:03,989 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 3.510180950164795, 'eval_accuracy': 0.3268698060941828, 'eval_runtime': 20.1395, 'eval_samples_per_second': 35.85, 'eval_steps_per_second': 0.149, 'epoch': 2.0}\n","  1% 22/3300 [02:50<3:40:26,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:05:24,136 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 19:05:24,141 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:05:24,261 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:05:24,265 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:53<3:49:42,  4.22s/it][INFO|trainer.py:3213] 2023-10-02 19:06:26,706 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:06:26,706 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:06:26,706 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4578888416290283, 'eval_accuracy': 0.6094182825484764, 'eval_runtime': 20.1101, 'eval_samples_per_second': 35.902, 'eval_steps_per_second': 0.149, 'epoch': 3.0}\n","  1% 33/3300 [04:13<3:49:42,  4.22s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:06:46,823 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 19:06:46,829 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:06:46,942 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:06:46,958 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:16<3:37:53,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 19:07:49,560 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:07:49,560 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:07:49,560 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2126258611679077, 'eval_accuracy': 0.667590027700831, 'eval_runtime': 20.2495, 'eval_samples_per_second': 35.655, 'eval_steps_per_second': 0.148, 'epoch': 4.0}\n","  1% 44/3300 [05:36<3:37:53,  4.02s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:08:09,814 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 19:08:09,820 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:08:09,933 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:08:09,938 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:08:10,162 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:39<3:44:07,  4.14s/it][INFO|trainer.py:3213] 2023-10-02 19:09:12,556 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:09:12,556 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:09:12,556 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.1224592924118042, 'eval_accuracy': 0.6911357340720221, 'eval_runtime': 20.1342, 'eval_samples_per_second': 35.859, 'eval_steps_per_second': 0.149, 'epoch': 5.0}\n","  2% 55/3300 [06:59<3:44:07,  4.14s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:09:32,697 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 19:09:32,703 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:09:32,819 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:09:32,823 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:09:33,053 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [08:02<3:45:45,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 19:10:35,438 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:10:35,438 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:10:35,438 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5332129001617432, 'eval_accuracy': 0.5969529085872576, 'eval_runtime': 20.4268, 'eval_samples_per_second': 35.346, 'eval_steps_per_second': 0.147, 'epoch': 6.0}\n","  2% 66/3300 [08:22<3:45:45,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.70s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:10:55,873 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 19:10:55,879 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:10:55,993 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:10:55,997 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:10:56,234 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:25<3:44:13,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 19:11:58,490 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:11:58,490 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:11:58,490 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3341861963272095, 'eval_accuracy': 0.6537396121883656, 'eval_runtime': 20.1846, 'eval_samples_per_second': 35.77, 'eval_steps_per_second': 0.149, 'epoch': 7.0}\n","  2% 77/3300 [09:45<3:44:13,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:12:18,683 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 19:12:18,688 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:12:18,798 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:12:18,802 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:12:19,023 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:48<3:42:28,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 19:13:21,568 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:13:21,568 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:13:21,569 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2872222661972046, 'eval_accuracy': 0.6731301939058172, 'eval_runtime': 20.2508, 'eval_samples_per_second': 35.653, 'eval_steps_per_second': 0.148, 'epoch': 8.0}\n","  3% 88/3300 [11:08<3:42:28,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:13:41,826 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 19:13:41,832 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:13:41,942 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:13:41,948 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:13:42,167 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-66] due to args.save_total_limit\n","  3% 99/3300 [12:10<3:48:16,  4.28s/it][INFO|trainer.py:3213] 2023-10-02 19:14:44,197 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:14:44,197 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:14:44,197 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.305738091468811, 'eval_accuracy': 0.6814404432132964, 'eval_runtime': 20.1916, 'eval_samples_per_second': 35.757, 'eval_steps_per_second': 0.149, 'epoch': 9.0}\n","  3% 99/3300 [12:30<3:48:16,  4.28s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:15:04,397 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 19:15:04,402 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:15:04,511 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:15:04,515 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:15:04,741 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.7107, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:33<3:35:30,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 19:16:06,759 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:16:06,759 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:16:06,759 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.5778392553329468, 'eval_accuracy': 0.5914127423822715, 'eval_runtime': 20.1689, 'eval_samples_per_second': 35.798, 'eval_steps_per_second': 0.149, 'epoch': 10.0}\n","  3% 110/3300 [13:53<3:35:30,  4.05s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:16:26,935 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 19:16:26,940 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:16:27,062 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:16:27,067 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:16:27,284 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [14:55<3:29:19,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 19:17:28,733 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:17:28,733 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:17:28,733 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 2.315624475479126, 'eval_accuracy': 0.5290858725761773, 'eval_runtime': 20.0023, 'eval_samples_per_second': 36.096, 'eval_steps_per_second': 0.15, 'epoch': 11.0}\n","  4% 121/3300 [15:15<3:29:19,  3.95s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:17:48,741 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 19:17:48,747 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:17:48,859 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:17:48,864 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:17:49,103 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-99] due to args.save_total_limit\n","  4% 132/3300 [16:17<3:37:56,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 19:18:51,061 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:18:51,062 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:18:51,062 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1744391918182373, 'eval_accuracy': 0.6814404432132964, 'eval_runtime': 19.9456, 'eval_samples_per_second': 36.199, 'eval_steps_per_second': 0.15, 'epoch': 12.0}\n","  4% 132/3300 [16:37<3:37:56,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:19:11,012 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 19:19:11,018 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:19:11,139 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:19:11,143 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:19:11,385 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-110] due to args.save_total_limit\n","  4% 143/3300 [17:39<3:27:36,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 19:20:13,187 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:20:13,187 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:20:13,187 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0921121835708618, 'eval_accuracy': 0.7077562326869806, 'eval_runtime': 19.9252, 'eval_samples_per_second': 36.236, 'eval_steps_per_second': 0.151, 'epoch': 13.0}\n","  4% 143/3300 [17:59<3:27:36,  3.95s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:20:33,119 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 19:20:33,125 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:20:33,238 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:20:33,242 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:20:33,462 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-55] due to args.save_total_limit\n","  5% 154/3300 [19:01<3:39:40,  4.19s/it][INFO|trainer.py:3213] 2023-10-02 19:21:35,315 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:21:35,316 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:21:35,316 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0023493766784668, 'eval_accuracy': 0.7340720221606648, 'eval_runtime': 20.0058, 'eval_samples_per_second': 36.089, 'eval_steps_per_second': 0.15, 'epoch': 14.0}\n","  5% 154/3300 [19:21<3:39:40,  4.19s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:21:55,328 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 19:21:55,334 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:21:55,452 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:21:55,456 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:21:55,688 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-121] due to args.save_total_limit\n","  5% 165/3300 [20:24<3:34:24,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 19:22:57,488 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:22:57,488 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:22:57,488 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.007875680923462, 'eval_accuracy': 0.7465373961218836, 'eval_runtime': 19.9538, 'eval_samples_per_second': 36.184, 'eval_steps_per_second': 0.15, 'epoch': 15.0}\n","  5% 165/3300 [20:44<3:34:24,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:23:17,449 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 19:23:17,455 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:23:17,570 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:23:17,575 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:23:17,815 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-132] due to args.save_total_limit\n","  5% 176/3300 [21:45<3:43:11,  4.29s/it][INFO|trainer.py:3213] 2023-10-02 19:24:19,389 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:24:19,390 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:24:19,390 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0960344076156616, 'eval_accuracy': 0.7132963988919667, 'eval_runtime': 19.9036, 'eval_samples_per_second': 36.275, 'eval_steps_per_second': 0.151, 'epoch': 16.0}\n","  5% 176/3300 [22:05<3:43:11,  4.29s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:24:39,300 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 19:24:39,307 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:24:39,418 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:24:39,423 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:24:39,648 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-143] due to args.save_total_limit\n","  6% 187/3300 [23:08<3:32:56,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 19:25:41,570 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:25:41,570 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:25:41,571 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8130218982696533, 'eval_accuracy': 0.7728531855955678, 'eval_runtime': 20.0495, 'eval_samples_per_second': 36.011, 'eval_steps_per_second': 0.15, 'epoch': 17.0}\n","  6% 187/3300 [23:28<3:32:56,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:26:01,625 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 19:26:01,631 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:26:01,743 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:26:01,747 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:26:01,980 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-154] due to args.save_total_limit\n","  6% 198/3300 [24:30<3:35:34,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 19:27:03,716 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:27:03,716 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:27:03,716 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.5840110778808594, 'eval_accuracy': 0.6149584487534626, 'eval_runtime': 20.0677, 'eval_samples_per_second': 35.978, 'eval_steps_per_second': 0.149, 'epoch': 18.0}\n","  6% 198/3300 [24:50<3:35:34,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:27:23,790 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 19:27:23,796 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:27:23,917 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:27:23,921 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:27:24,172 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-165] due to args.save_total_limit\n","{'loss': 0.31, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:52<3:22:56,  3.94s/it][INFO|trainer.py:3213] 2023-10-02 19:28:25,717 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:28:25,717 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:28:25,717 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3437273502349854, 'eval_accuracy': 0.7022160664819944, 'eval_runtime': 19.9286, 'eval_samples_per_second': 36.229, 'eval_steps_per_second': 0.151, 'epoch': 19.0}\n","  6% 209/3300 [26:12<3:22:56,  3.94s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:28:45,654 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 19:28:45,660 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:28:45,772 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:28:45,776 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:28:46,006 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-176] due to args.save_total_limit\n","  7% 220/3300 [27:14<3:20:59,  3.92s/it][INFO|trainer.py:3213] 2023-10-02 19:29:47,651 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:29:47,651 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:29:47,651 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.10868501663208, 'eval_accuracy': 0.7257617728531855, 'eval_runtime': 19.8831, 'eval_samples_per_second': 36.312, 'eval_steps_per_second': 0.151, 'epoch': 20.0}\n","  7% 220/3300 [27:34<3:20:59,  3.92s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:30:07,540 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 19:30:07,546 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:30:07,663 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:30:07,667 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:30:07,902 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-198] due to args.save_total_limit\n","  7% 231/3300 [28:36<3:20:55,  3.93s/it][INFO|trainer.py:3213] 2023-10-02 19:31:09,449 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:31:09,449 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:31:09,449 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3221161365509033, 'eval_accuracy': 0.6994459833795014, 'eval_runtime': 19.9065, 'eval_samples_per_second': 36.27, 'eval_steps_per_second': 0.151, 'epoch': 21.0}\n","  7% 231/3300 [28:55<3:20:55,  3.93s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:31:29,360 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 19:31:29,366 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:31:29,476 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:31:29,480 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:31:29,713 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [29:58<3:23:16,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 19:32:31,453 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:32:31,453 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:32:31,453 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1789618730545044, 'eval_accuracy': 0.6883656509695291, 'eval_runtime': 19.9792, 'eval_samples_per_second': 36.138, 'eval_steps_per_second': 0.15, 'epoch': 22.0}\n","  7% 242/3300 [30:18<3:23:16,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:32:51,438 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 19:32:51,443 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:32:51,556 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:32:51,560 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:32:51,780 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-220] due to args.save_total_limit\n","  8% 253/3300 [31:20<3:20:49,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 19:33:53,471 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:33:53,471 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:33:53,471 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.751430094242096, 'eval_accuracy': 0.7576177285318559, 'eval_runtime': 20.05, 'eval_samples_per_second': 36.01, 'eval_steps_per_second': 0.15, 'epoch': 23.0}\n","  8% 253/3300 [31:40<3:20:49,  3.95s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:34:13,528 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-02 19:34:13,533 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:34:13,643 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:34:13,647 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:34:13,879 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-187] due to args.save_total_limit\n","  8% 264/3300 [32:42<3:22:19,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 19:35:15,781 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:35:15,782 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:35:15,782 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.01900053024292, 'eval_accuracy': 0.7562326869806094, 'eval_runtime': 20.1595, 'eval_samples_per_second': 35.814, 'eval_steps_per_second': 0.149, 'epoch': 24.0}\n","  8% 264/3300 [33:02<3:22:19,  4.00s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:35:35,947 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-02 19:35:35,966 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:35:36,099 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:35:36,104 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:35:36,340 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-231] due to args.save_total_limit\n","  8% 275/3300 [34:04<3:30:40,  4.18s/it][INFO|trainer.py:3213] 2023-10-02 19:36:38,009 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:36:38,009 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:36:38,009 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.884084165096283, 'eval_accuracy': 0.7797783933518005, 'eval_runtime': 19.9679, 'eval_samples_per_second': 36.158, 'eval_steps_per_second': 0.15, 'epoch': 25.0}\n","  8% 275/3300 [34:24<3:30:40,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:36:57,985 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-02 19:36:57,991 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:36:58,104 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:36:58,109 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:36:58,359 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-242] due to args.save_total_limit\n","  9% 286/3300 [35:26<3:20:34,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 19:38:00,285 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:38:00,286 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:38:00,286 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8735901117324829, 'eval_accuracy': 0.7894736842105263, 'eval_runtime': 19.9503, 'eval_samples_per_second': 36.19, 'eval_steps_per_second': 0.15, 'epoch': 26.0}\n","  9% 286/3300 [35:46<3:20:34,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:38:20,242 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-02 19:38:20,248 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:38:20,362 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:38:20,366 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:38:20,589 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [36:48<3:26:54,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 19:39:22,222 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:39:22,222 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:39:22,222 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8154903650283813, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.97, 'eval_samples_per_second': 36.154, 'eval_steps_per_second': 0.15, 'epoch': 27.0}\n","  9% 297/3300 [37:08<3:26:54,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:39:42,197 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-02 19:39:42,203 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:39:42,314 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:39:42,318 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:39:42,541 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.252, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [38:10<3:27:56,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 19:40:44,117 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:40:44,117 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:40:44,117 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8733290433883667, 'eval_accuracy': 0.7590027700831025, 'eval_runtime': 20.1284, 'eval_samples_per_second': 35.87, 'eval_steps_per_second': 0.149, 'epoch': 28.0}\n","  9% 308/3300 [38:30<3:27:56,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:41:04,253 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-02 19:41:04,259 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:41:04,374 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:41:04,378 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:41:04,619 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [39:32<3:20:29,  4.04s/it][INFO|trainer.py:3213] 2023-10-02 19:42:06,267 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:42:06,267 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:42:06,268 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.755901038646698, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 19.93, 'eval_samples_per_second': 36.227, 'eval_steps_per_second': 0.151, 'epoch': 29.0}\n"," 10% 319/3300 [39:52<3:20:29,  4.04s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:42:26,206 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-02 19:42:26,212 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:42:26,326 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:42:26,330 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:42:26,556 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-297] due to args.save_total_limit\n"," 10% 330/3300 [40:54<3:23:16,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 19:43:28,137 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:43:28,137 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:43:28,137 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9395654797554016, 'eval_accuracy': 0.778393351800554, 'eval_runtime': 20.1038, 'eval_samples_per_second': 35.914, 'eval_steps_per_second': 0.149, 'epoch': 30.0}\n"," 10% 330/3300 [41:14<3:23:16,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.71s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:43:48,247 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-330\n","[INFO|configuration_utils.py:460] 2023-10-02 19:43:48,252 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-330/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:43:48,368 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-330/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:43:48,372 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-330/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:43:48,597 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-308] due to args.save_total_limit\n"," 10% 341/3300 [42:17<3:22:22,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 19:44:50,441 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:44:50,441 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:44:50,441 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8150877952575684, 'eval_accuracy': 0.7977839335180056, 'eval_runtime': 20.06, 'eval_samples_per_second': 35.992, 'eval_steps_per_second': 0.15, 'epoch': 31.0}\n"," 10% 341/3300 [42:37<3:22:22,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:45:10,508 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-341\n","[INFO|configuration_utils.py:460] 2023-10-02 19:45:10,514 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-341/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:45:10,631 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-341/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:45:10,635 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-341/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:45:10,883 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-319] due to args.save_total_limit\n"," 11% 352/3300 [43:39<3:21:13,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 19:46:12,758 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:46:12,759 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:46:12,759 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0173698663711548, 'eval_accuracy': 0.7631578947368421, 'eval_runtime': 19.9942, 'eval_samples_per_second': 36.11, 'eval_steps_per_second': 0.15, 'epoch': 32.0}\n"," 11% 352/3300 [43:59<3:21:13,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:46:32,758 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-352\n","[INFO|configuration_utils.py:460] 2023-10-02 19:46:32,764 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-352/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:46:32,882 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-352/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:46:32,887 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-352/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:46:33,121 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-330] due to args.save_total_limit\n"," 11% 363/3300 [45:02<3:27:48,  4.25s/it][INFO|trainer.py:3213] 2023-10-02 19:47:35,782 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:47:35,782 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:47:35,782 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0954678058624268, 'eval_accuracy': 0.7493074792243767, 'eval_runtime': 19.9459, 'eval_samples_per_second': 36.198, 'eval_steps_per_second': 0.15, 'epoch': 33.0}\n"," 11% 363/3300 [45:22<3:27:48,  4.25s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:47:55,735 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-363\n","[INFO|configuration_utils.py:460] 2023-10-02 19:47:55,742 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-363/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:47:55,854 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-363/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:47:55,860 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-363/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:47:56,090 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-341] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 19:47:56,111 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 19:47:56,111 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/checkpoint-253 (score: 0.751430094242096).\n","{'train_runtime': 2722.7578, 'train_samples_per_second': 308.07, 'train_steps_per_second': 1.212, 'train_loss': 0.38775664978447694, 'epoch': 33.0}\n"," 11% 363/3300 [45:22<6:07:09,  7.50s/it]\n","[INFO|trainer.py:2939] 2023-10-02 19:47:56,184 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/\n","[INFO|configuration_utils.py:460] 2023-10-02 19:47:56,189 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:47:56,302 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:47:56,306 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       33.0\n","  train_loss               =     0.3878\n","  train_runtime            = 0:45:22.75\n","  train_samples_per_second =     308.07\n","  train_steps_per_second   =      1.212\n","[INFO|trainer.py:3213] 2023-10-02 19:47:56,340 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:47:56,340 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:47:56,340 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.81s/it]\n","***** eval metrics *****\n","  epoch                   =       33.0\n","  eval_accuracy           =     0.7576\n","  eval_loss               =     0.7514\n","  eval_runtime            = 0:00:20.25\n","  eval_samples_per_second =     35.642\n","  eval_steps_per_second   =      0.148\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▂▁▅▆▆▅▆▆▆▅▄▆▇▇▇▇█▅▇▇▇▆▇▇███▇███▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ██▃▂▂▃▂▂▂▃▅▂▂▂▂▂▁▃▃▂▂▂▁▂▁▁▁▁▁▁▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▄▃▅▄▆▄▅▄▄▂▂▁▂▂▁▃▃▁▁▁▂▃▄▂▂▂▃▁▃▃▂▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▅▆▄▅▃▅▄▅▅▇▇█▇▇█▆▆███▇▆▅▇▇▇▅█▆▆▇▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▅▅▄▅▂▅▄▅▅▇▇█▇▇█▇▅███▇▇▅▇▇▇▅█▅▇▇▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.75762\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.75143\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.2567\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.642\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.148\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 33.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 363\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.252\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 9.333444229543526e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.38776\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2722.7578\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 308.07\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.212\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlegendary-lake-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/1q7dc0mo\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_190226-1q7dc0mo/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_8/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 8 \\\n","    --seed 8 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJ6zuVm0Fa-J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696278787853,"user_tz":-120,"elapsed":2678452,"user":{"displayName":"Andreas Louw","userId":"11183032721846533402"}},"outputId":"1c3a3279-2846-453f-c2f1-605e4bf0c5d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-02 19:48:29.930477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_194834-osrgdbl1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlunar-surf-9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/osrgdbl1\u001b[0m\n","10/02/2023 19:48:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 19:48:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=9,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/runs/Oct02_19-48-35_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=9,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 54565.85it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 297185.59it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 19:48:39,397 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 19:48:39,399 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 19:48:39,401 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 19:48:39,526 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 19:48:39,526 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 19:48:39,772 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 19:48:39,772 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 19:48:39,774 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 19:48:39,774 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 19:48:41,720 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 19:48:41,720 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 19:48:41,721 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 19:48:41,721 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 19:48:41,721 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 19:48:41,721 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 19:48:41,721 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 19:48:41,722 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 19:48:41,722 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:05<3:36:35,  3.95s/it][INFO|trainer.py:3213] 2023-10-02 19:49:47,725 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:49:47,726 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:49:47,726 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 5.261803150177002, 'eval_accuracy': 0.27008310249307477, 'eval_runtime': 20.3877, 'eval_samples_per_second': 35.413, 'eval_steps_per_second': 0.147, 'epoch': 1.0}\n","  0% 11/3300 [01:26<3:36:35,  3.95s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:50:08,120 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 19:50:08,127 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:50:08,246 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:50:08,251 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:27<3:43:58,  4.10s/it][INFO|trainer.py:3213] 2023-10-02 19:51:09,629 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:51:09,629 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:51:09,629 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.492619037628174, 'eval_accuracy': 0.4667590027700831, 'eval_runtime': 19.974, 'eval_samples_per_second': 36.147, 'eval_steps_per_second': 0.15, 'epoch': 2.0}\n","  1% 22/3300 [02:47<3:43:58,  4.10s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:51:29,610 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 19:51:29,616 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:51:29,734 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:51:29,739 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:49<3:37:39,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 19:52:31,311 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:52:31,311 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:52:31,311 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.355520248413086, 'eval_accuracy': 0.6398891966759003, 'eval_runtime': 20.082, 'eval_samples_per_second': 35.953, 'eval_steps_per_second': 0.149, 'epoch': 3.0}\n","  1% 33/3300 [04:09<3:37:39,  4.00s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:52:51,402 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 19:52:51,408 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:52:51,519 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:52:51,524 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:11<3:48:32,  4.21s/it][INFO|trainer.py:3213] 2023-10-02 19:53:53,565 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:53:53,566 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:53:53,566 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2471256256103516, 'eval_accuracy': 0.6232686980609419, 'eval_runtime': 19.891, 'eval_samples_per_second': 36.298, 'eval_steps_per_second': 0.151, 'epoch': 4.0}\n","  1% 44/3300 [05:31<3:48:32,  4.21s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:54:13,463 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 19:54:13,469 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:54:13,577 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:54:13,581 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:54:13,795 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:33<3:30:47,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 19:55:15,115 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:55:15,116 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:55:15,116 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.4762402772903442, 'eval_accuracy': 0.6412742382271468, 'eval_runtime': 20.0165, 'eval_samples_per_second': 36.07, 'eval_steps_per_second': 0.15, 'epoch': 5.0}\n","  2% 55/3300 [06:53<3:30:47,  3.90s/it]\n","100% 3/3 [00:05<00:00,  2.60s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:55:35,137 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 19:55:35,144 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:55:35,256 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:55:35,261 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:55:35,520 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-22] due to args.save_total_limit\n","  2% 66/3300 [07:56<3:45:20,  4.18s/it][INFO|trainer.py:3213] 2023-10-02 19:56:37,950 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:56:37,950 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:56:37,950 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.024101972579956, 'eval_accuracy': 0.6994459833795014, 'eval_runtime': 20.0777, 'eval_samples_per_second': 35.96, 'eval_steps_per_second': 0.149, 'epoch': 6.0}\n","  2% 66/3300 [08:16<3:45:20,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:56:58,036 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-66\n","[INFO|configuration_utils.py:460] 2023-10-02 19:56:58,042 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-66/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:56:58,156 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-66/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:56:58,161 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-66/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:56:58,387 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-33] due to args.save_total_limit\n","  2% 77/3300 [09:18<3:40:45,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 19:58:00,557 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:58:00,557 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:58:00,558 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.2290953397750854, 'eval_accuracy': 0.6689750692520776, 'eval_runtime': 20.1342, 'eval_samples_per_second': 35.859, 'eval_steps_per_second': 0.149, 'epoch': 7.0}\n","  2% 77/3300 [09:38<3:40:45,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:58:20,698 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-77\n","[INFO|configuration_utils.py:460] 2023-10-02 19:58:20,704 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-77/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:58:20,819 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-77/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:58:20,823 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-77/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:58:21,047 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-44] due to args.save_total_limit\n","  3% 88/3300 [10:41<3:34:13,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 19:59:22,974 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 19:59:22,975 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 19:59:22,975 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3071436882019043, 'eval_accuracy': 0.6703601108033241, 'eval_runtime': 20.0166, 'eval_samples_per_second': 36.07, 'eval_steps_per_second': 0.15, 'epoch': 8.0}\n","  3% 88/3300 [11:01<3:34:13,  4.00s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 19:59:42,998 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-88\n","[INFO|configuration_utils.py:460] 2023-10-02 19:59:43,004 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-88/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 19:59:43,116 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-88/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 19:59:43,120 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-88/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 19:59:43,356 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-55] due to args.save_total_limit\n","  3% 99/3300 [12:03<3:28:16,  3.90s/it][INFO|trainer.py:3213] 2023-10-02 20:00:45,015 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:00:45,015 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:00:45,016 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.5648274421691895, 'eval_accuracy': 0.6440443213296398, 'eval_runtime': 20.1578, 'eval_samples_per_second': 35.817, 'eval_steps_per_second': 0.149, 'epoch': 9.0}\n","  3% 99/3300 [12:23<3:28:16,  3.90s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:01:05,180 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-99\n","[INFO|configuration_utils.py:460] 2023-10-02 20:01:05,186 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-99/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:01:05,305 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-99/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:01:05,310 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-99/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:01:05,536 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-77] due to args.save_total_limit\n","{'loss': 0.7007, 'learning_rate': 0.0009696969696969698, 'epoch': 9.09}\n","  3% 110/3300 [13:25<3:41:41,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 20:02:07,111 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:02:07,111 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:02:07,111 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 4.5132155418396, 'eval_accuracy': 0.45013850415512463, 'eval_runtime': 20.1189, 'eval_samples_per_second': 35.887, 'eval_steps_per_second': 0.149, 'epoch': 10.0}\n","  3% 110/3300 [13:45<3:41:41,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:02:27,238 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-110\n","[INFO|configuration_utils.py:460] 2023-10-02 20:02:27,244 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-110/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:02:27,361 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-110/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:02:27,366 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-110/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:02:27,597 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-88] due to args.save_total_limit\n","  4% 121/3300 [14:48<3:33:40,  4.03s/it][INFO|trainer.py:3213] 2023-10-02 20:03:29,934 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:03:29,934 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:03:29,934 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9609712958335876, 'eval_accuracy': 0.7202216066481995, 'eval_runtime': 19.9809, 'eval_samples_per_second': 36.135, 'eval_steps_per_second': 0.15, 'epoch': 11.0}\n","  4% 121/3300 [15:08<3:33:40,  4.03s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:03:49,921 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-121\n","[INFO|configuration_utils.py:460] 2023-10-02 20:03:49,927 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-121/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:03:50,054 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-121/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:03:50,058 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-121/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:03:50,312 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-66] due to args.save_total_limit\n","  4% 132/3300 [16:09<3:29:34,  3.97s/it][INFO|trainer.py:3213] 2023-10-02 20:04:51,601 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:04:51,601 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:04:51,601 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.9685724973678589, 'eval_accuracy': 0.590027700831025, 'eval_runtime': 20.0411, 'eval_samples_per_second': 36.026, 'eval_steps_per_second': 0.15, 'epoch': 12.0}\n","  4% 132/3300 [16:29<3:29:34,  3.97s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:05:11,648 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-132\n","[INFO|configuration_utils.py:460] 2023-10-02 20:05:11,655 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-132/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:05:11,774 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-132/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:05:11,778 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-132/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:05:12,000 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-99] due to args.save_total_limit\n","  4% 143/3300 [17:32<3:32:56,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 20:06:13,916 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:06:13,916 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:06:13,916 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1011725664138794, 'eval_accuracy': 0.7022160664819944, 'eval_runtime': 20.0257, 'eval_samples_per_second': 36.054, 'eval_steps_per_second': 0.15, 'epoch': 13.0}\n","  4% 143/3300 [17:52<3:32:56,  4.05s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:06:33,949 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-143\n","[INFO|configuration_utils.py:460] 2023-10-02 20:06:33,955 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-143/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:06:34,068 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-143/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:06:34,073 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-143/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:06:34,294 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-110] due to args.save_total_limit\n","  5% 154/3300 [18:54<3:35:40,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 20:07:36,013 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:07:36,013 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:07:36,013 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1294604539871216, 'eval_accuracy': 0.7299168975069252, 'eval_runtime': 19.9737, 'eval_samples_per_second': 36.148, 'eval_steps_per_second': 0.15, 'epoch': 14.0}\n","  5% 154/3300 [19:14<3:35:40,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:07:55,993 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-154\n","[INFO|configuration_utils.py:460] 2023-10-02 20:07:55,999 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-154/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:07:56,111 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-154/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:07:56,115 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-154/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:07:56,351 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-132] due to args.save_total_limit\n","  5% 165/3300 [20:16<3:29:02,  4.00s/it][INFO|trainer.py:3213] 2023-10-02 20:08:57,918 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:08:57,918 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:08:57,918 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0240116119384766, 'eval_accuracy': 0.7229916897506925, 'eval_runtime': 19.9841, 'eval_samples_per_second': 36.129, 'eval_steps_per_second': 0.15, 'epoch': 15.0}\n","  5% 165/3300 [20:36<3:29:02,  4.00s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:09:17,910 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-165\n","[INFO|configuration_utils.py:460] 2023-10-02 20:09:17,916 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-165/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:09:18,032 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-165/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:09:18,036 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-165/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:09:18,259 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-143] due to args.save_total_limit\n","  5% 176/3300 [21:38<3:36:44,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 20:10:19,987 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:10:19,987 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:10:19,987 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.403226613998413, 'eval_accuracy': 0.6925207756232687, 'eval_runtime': 20.1361, 'eval_samples_per_second': 35.856, 'eval_steps_per_second': 0.149, 'epoch': 16.0}\n","  5% 176/3300 [21:58<3:36:44,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:10:40,128 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-176\n","[INFO|configuration_utils.py:460] 2023-10-02 20:10:40,135 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-176/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:10:40,263 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-176/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:10:40,268 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-176/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:10:40,513 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-154] due to args.save_total_limit\n","  6% 187/3300 [23:00<3:26:02,  3.97s/it][INFO|trainer.py:3213] 2023-10-02 20:11:42,406 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:11:42,406 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:11:42,407 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9161938428878784, 'eval_accuracy': 0.7465373961218836, 'eval_runtime': 20.0196, 'eval_samples_per_second': 36.065, 'eval_steps_per_second': 0.15, 'epoch': 17.0}\n","  6% 187/3300 [23:20<3:26:02,  3.97s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:12:02,432 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-187\n","[INFO|configuration_utils.py:460] 2023-10-02 20:12:02,438 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-187/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:12:02,555 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-187/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:12:02,559 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-187/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:12:02,803 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-121] due to args.save_total_limit\n","  6% 198/3300 [24:22<3:25:27,  3.97s/it][INFO|trainer.py:3213] 2023-10-02 20:13:04,147 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:13:04,147 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:13:04,147 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8775964379310608, 'eval_accuracy': 0.7659279778393352, 'eval_runtime': 19.8813, 'eval_samples_per_second': 36.316, 'eval_steps_per_second': 0.151, 'epoch': 18.0}\n","  6% 198/3300 [24:42<3:25:27,  3.97s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:13:24,037 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-198\n","[INFO|configuration_utils.py:460] 2023-10-02 20:13:24,043 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-198/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:13:24,160 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-198/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:13:24,164 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-198/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:13:24,385 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-165] due to args.save_total_limit\n","{'loss': 0.3121, 'learning_rate': 0.0009393939393939394, 'epoch': 18.18}\n","  6% 209/3300 [25:44<3:34:10,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 20:14:25,783 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:14:25,783 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:14:25,784 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0254727602005005, 'eval_accuracy': 0.7493074792243767, 'eval_runtime': 19.9728, 'eval_samples_per_second': 36.149, 'eval_steps_per_second': 0.15, 'epoch': 19.0}\n","  6% 209/3300 [26:04<3:34:10,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:14:45,764 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-209\n","[INFO|configuration_utils.py:460] 2023-10-02 20:14:45,770 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-209/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:14:45,881 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-209/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:14:45,885 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-209/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:14:46,103 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-176] due to args.save_total_limit\n","  7% 220/3300 [27:06<3:27:50,  4.05s/it][INFO|trainer.py:3213] 2023-10-02 20:15:48,010 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:15:48,010 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:15:48,010 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.1695119142532349, 'eval_accuracy': 0.7063711911357341, 'eval_runtime': 20.1564, 'eval_samples_per_second': 35.82, 'eval_steps_per_second': 0.149, 'epoch': 20.0}\n","  7% 220/3300 [27:26<3:27:50,  4.05s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:16:08,173 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-220\n","[INFO|configuration_utils.py:460] 2023-10-02 20:16:08,179 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-220/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:16:08,299 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-220/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:16:08,317 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-220/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:16:08,541 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-187] due to args.save_total_limit\n","  7% 231/3300 [28:28<3:28:05,  4.07s/it][INFO|trainer.py:3213] 2023-10-02 20:17:10,446 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:17:10,446 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:17:10,446 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.128255009651184, 'eval_accuracy': 0.7409972299168975, 'eval_runtime': 20.005, 'eval_samples_per_second': 36.091, 'eval_steps_per_second': 0.15, 'epoch': 21.0}\n","  7% 231/3300 [28:48<3:28:05,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:17:30,456 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-231\n","[INFO|configuration_utils.py:460] 2023-10-02 20:17:30,462 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-231/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:17:30,576 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-231/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:17:30,580 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-231/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:17:30,800 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-209] due to args.save_total_limit\n","  7% 242/3300 [29:51<3:23:35,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 20:18:32,845 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:18:32,845 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:18:32,845 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.8211570978164673, 'eval_accuracy': 0.7908587257617729, 'eval_runtime': 20.0918, 'eval_samples_per_second': 35.935, 'eval_steps_per_second': 0.149, 'epoch': 22.0}\n","  7% 242/3300 [30:11<3:23:35,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:18:52,946 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-242\n","[INFO|configuration_utils.py:460] 2023-10-02 20:18:52,952 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-242/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:18:53,064 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-242/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:18:53,068 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-242/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:18:53,299 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-198] due to args.save_total_limit\n","  8% 253/3300 [31:13<3:22:25,  3.99s/it][INFO|trainer.py:3213] 2023-10-02 20:19:54,740 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:19:54,741 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:19:54,741 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9264184236526489, 'eval_accuracy': 0.7313019390581718, 'eval_runtime': 20.1409, 'eval_samples_per_second': 35.848, 'eval_steps_per_second': 0.149, 'epoch': 23.0}\n","  8% 253/3300 [31:33<3:22:25,  3.99s/it]\n","100% 3/3 [00:05<00:00,  2.67s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:20:14,888 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-253\n","[INFO|configuration_utils.py:460] 2023-10-02 20:20:14,894 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-253/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:20:15,025 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-253/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:20:15,030 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-253/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:20:15,257 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-220] due to args.save_total_limit\n","  8% 264/3300 [32:34<3:31:12,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 20:21:16,699 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:21:16,699 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:21:16,699 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9557559490203857, 'eval_accuracy': 0.7576177285318559, 'eval_runtime': 19.9522, 'eval_samples_per_second': 36.186, 'eval_steps_per_second': 0.15, 'epoch': 24.0}\n","  8% 264/3300 [32:54<3:31:12,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:21:36,657 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-264\n","[INFO|configuration_utils.py:460] 2023-10-02 20:21:36,663 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-264/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:21:36,777 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-264/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:21:36,782 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-264/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:21:37,005 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-231] due to args.save_total_limit\n","  8% 275/3300 [33:56<3:34:39,  4.26s/it][INFO|trainer.py:3213] 2023-10-02 20:22:38,700 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:22:38,701 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:22:38,701 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9024868011474609, 'eval_accuracy': 0.778393351800554, 'eval_runtime': 20.2307, 'eval_samples_per_second': 35.688, 'eval_steps_per_second': 0.148, 'epoch': 25.0}\n","  8% 275/3300 [34:17<3:34:39,  4.26s/it]\n","100% 3/3 [00:05<00:00,  2.74s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:22:58,939 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-275\n","[INFO|configuration_utils.py:460] 2023-10-02 20:22:58,944 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-275/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:22:59,058 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-275/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:22:59,062 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-275/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:22:59,282 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-253] due to args.save_total_limit\n","  9% 286/3300 [35:19<3:27:30,  4.13s/it][INFO|trainer.py:3213] 2023-10-02 20:24:01,182 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:24:01,182 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:24:01,182 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0227110385894775, 'eval_accuracy': 0.7285318559556787, 'eval_runtime': 19.9677, 'eval_samples_per_second': 36.158, 'eval_steps_per_second': 0.15, 'epoch': 26.0}\n","  9% 286/3300 [35:39<3:27:30,  4.13s/it]\n","100% 3/3 [00:05<00:00,  2.62s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:24:21,155 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-286\n","[INFO|configuration_utils.py:460] 2023-10-02 20:24:21,161 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-286/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:24:21,271 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-286/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:24:21,275 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-286/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:24:21,492 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-264] due to args.save_total_limit\n","  9% 297/3300 [36:41<3:29:13,  4.18s/it][INFO|trainer.py:3213] 2023-10-02 20:25:23,387 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:25:23,388 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:25:23,388 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.927912175655365, 'eval_accuracy': 0.7409972299168975, 'eval_runtime': 19.9993, 'eval_samples_per_second': 36.101, 'eval_steps_per_second': 0.15, 'epoch': 27.0}\n","  9% 297/3300 [37:01<3:29:13,  4.18s/it]\n","100% 3/3 [00:05<00:00,  2.63s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:25:43,393 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-297\n","[INFO|configuration_utils.py:460] 2023-10-02 20:25:43,399 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-297/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:25:43,513 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-297/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:25:43,517 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-297/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:25:43,753 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-275] due to args.save_total_limit\n","{'loss': 0.2454, 'learning_rate': 0.0009090909090909091, 'epoch': 27.27}\n","  9% 308/3300 [38:03<3:24:44,  4.11s/it][INFO|trainer.py:3213] 2023-10-02 20:26:45,518 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:26:45,518 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:26:45,518 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.972435712814331, 'eval_accuracy': 0.7368421052631579, 'eval_runtime': 20.0334, 'eval_samples_per_second': 36.04, 'eval_steps_per_second': 0.15, 'epoch': 28.0}\n","  9% 308/3300 [38:23<3:24:44,  4.11s/it]\n","100% 3/3 [00:05<00:00,  2.64s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:27:05,557 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-308\n","[INFO|configuration_utils.py:460] 2023-10-02 20:27:05,564 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-308/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:27:05,677 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-308/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:27:05,681 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-308/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:27:05,919 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-286] due to args.save_total_limit\n"," 10% 319/3300 [39:25<3:24:55,  4.12s/it][INFO|trainer.py:3213] 2023-10-02 20:28:07,704 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:28:07,704 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:28:07,704 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.0999892950057983, 'eval_accuracy': 0.7479224376731302, 'eval_runtime': 20.1145, 'eval_samples_per_second': 35.895, 'eval_steps_per_second': 0.149, 'epoch': 29.0}\n"," 10% 319/3300 [39:46<3:24:55,  4.12s/it]\n","100% 3/3 [00:05<00:00,  2.69s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:28:27,827 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-319\n","[INFO|configuration_utils.py:460] 2023-10-02 20:28:27,833 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-319/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:28:27,950 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-319/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:28:27,954 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-319/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:28:28,185 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-297] due to args.save_total_limit\n"," 10% 330/3300 [40:48<3:26:30,  4.17s/it][INFO|trainer.py:3213] 2023-10-02 20:29:30,106 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:29:30,106 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:29:30,106 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9986996650695801, 'eval_accuracy': 0.775623268698061, 'eval_runtime': 20.2021, 'eval_samples_per_second': 35.739, 'eval_steps_per_second': 0.148, 'epoch': 30.0}\n"," 10% 330/3300 [41:08<3:26:30,  4.17s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:29:50,324 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-330\n","[INFO|configuration_utils.py:460] 2023-10-02 20:29:50,333 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-330/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:29:50,453 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-330/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:29:50,458 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-330/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:29:50,690 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-308] due to args.save_total_limit\n"," 10% 341/3300 [42:10<3:14:11,  3.94s/it][INFO|trainer.py:3213] 2023-10-02 20:30:52,288 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:30:52,289 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:30:52,289 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.3402713537216187, 'eval_accuracy': 0.7534626038781164, 'eval_runtime': 20.0466, 'eval_samples_per_second': 36.016, 'eval_steps_per_second': 0.15, 'epoch': 31.0}\n"," 10% 341/3300 [42:30<3:14:11,  3.94s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:31:12,343 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-341\n","[INFO|configuration_utils.py:460] 2023-10-02 20:31:12,350 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-341/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:31:12,469 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-341/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:31:12,475 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-341/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:31:12,729 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-319] due to args.save_total_limit\n"," 11% 352/3300 [43:32<3:24:20,  4.16s/it][INFO|trainer.py:3213] 2023-10-02 20:32:14,562 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:32:14,563 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:32:14,563 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.9461730718612671, 'eval_accuracy': 0.7686980609418282, 'eval_runtime': 20.0615, 'eval_samples_per_second': 35.989, 'eval_steps_per_second': 0.15, 'epoch': 32.0}\n"," 11% 352/3300 [43:52<3:24:20,  4.16s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:32:34,631 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-352\n","[INFO|configuration_utils.py:460] 2023-10-02 20:32:34,636 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-352/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:32:34,750 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-352/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:32:34,755 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-352/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:32:34,981 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-330] due to args.save_total_limit\n","[INFO|trainer.py:2017] 2023-10-02 20:32:35,019 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2196] 2023-10-02 20:32:35,019 >> Loading best model from drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/checkpoint-242 (score: 0.8211570978164673).\n","{'train_runtime': 2633.3623, 'train_samples_per_second': 318.528, 'train_steps_per_second': 1.253, 'train_loss': 0.3886662152680484, 'epoch': 32.0}\n"," 11% 352/3300 [43:53<6:07:34,  7.48s/it]\n","[INFO|trainer.py:2939] 2023-10-02 20:32:35,089 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/\n","[INFO|configuration_utils.py:460] 2023-10-02 20:32:35,095 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:32:35,218 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:32:35,221 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/preprocessor_config.json\n","***** train metrics *****\n","  epoch                    =       32.0\n","  train_loss               =     0.3887\n","  train_runtime            = 0:43:53.36\n","  train_samples_per_second =    318.528\n","  train_steps_per_second   =      1.253\n","[INFO|trainer.py:3213] 2023-10-02 20:32:35,238 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:32:35,238 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:32:35,238 >>   Batch size = 256\n","100% 3/3 [00:05<00:00,  1.78s/it]\n","***** eval metrics *****\n","  epoch                   =       32.0\n","  eval_accuracy           =     0.7909\n","  eval_loss               =     0.8212\n","  eval_runtime            = 0:00:20.06\n","  eval_samples_per_second =     35.989\n","  eval_steps_per_second   =       0.15\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▆▆▆▇▆▆▆▃▇▅▇▇▇▇▇█▇▇▇█▇██▇▇▇▇█▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▂▂▂▁▂▂▂▇▁▃▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▂▄▁▃▄▄▃▅▄▂▃▃▂▂▅▃▁▂▅▃▄▅▂▆▂▃▃▄▅▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▇▅█▆▅▄▆▄▅▇▆▆▇▇▄▆█▇▄▆▅▄▇▃▇▆▆▅▄▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▆▅█▆▅▅▆▅▅▆▆▆▆▆▅▆█▆▅▆▅▅▆▃▆▆▆▅▃▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.79086\n","\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.82116\n","\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 20.0615\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 35.989\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.15\n","\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 32.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 352\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.00091\n","\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2454\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 9.050612586224026e+17\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.38867\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2633.3623\n","\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 318.528\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.253\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlunar-surf-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/osrgdbl1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231002_194834-osrgdbl1/logs\u001b[0m\n"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_9/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 9 \\\n","    --seed 9 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLPgVSaRFd--","outputId":"9cdf2a01-88aa-470c-fd96-93e73ab74cae","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-02 20:33:10.101761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu15048366\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.11\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231002_203314-claumfsa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfluent-wildflower-10\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/u15048366/cnnFabi/runs/claumfsa\u001b[0m\n","10/02/2023 20:33:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","10/02/2023 20:33:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=10,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=epoch,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=False,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=True,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/runs/Oct02_20-33-15_d4e9efa07485,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=loss,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=300.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=256,\n","per_device_train_batch_size=256,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=3,\n","seed=10,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n","\n","  warnings.warn(\n","Resolving data files: 100% 2801/2801 [00:00<00:00, 342934.37it/s]\n","Resolving data files: 100% 723/723 [00:00<00:00, 219347.69it/s]\n","\n","\n","\n","None\n","\n","\n","\n","[INFO|configuration_utils.py:715] 2023-10-02 20:33:19,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/config.json\n","[INFO|configuration_utils.py:775] 2023-10-02 20:33:19,405 >> Model config ResNetConfig {\n","  \"_name_or_path\": \"microsoft/resnet-18\",\n","  \"architectures\": [\n","    \"ResNetForImageClassification\"\n","  ],\n","  \"depths\": [\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"downsample_in_first_stage\": false,\n","  \"embedding_size\": 64,\n","  \"finetuning_task\": \"image-classification\",\n","  \"hidden_act\": \"relu\",\n","  \"hidden_sizes\": [\n","    64,\n","    128,\n","    256,\n","    512\n","  ],\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"18\",\n","    \"11\": \"19\",\n","    \"12\": \"2\",\n","    \"13\": \"20\",\n","    \"14\": \"21\",\n","    \"15\": \"22\",\n","    \"16\": \"23\",\n","    \"17\": \"24\",\n","    \"18\": \"25\",\n","    \"19\": \"26\",\n","    \"2\": \"10\",\n","    \"20\": \"27\",\n","    \"21\": \"28\",\n","    \"22\": \"29\",\n","    \"23\": \"3\",\n","    \"24\": \"30\",\n","    \"25\": \"31\",\n","    \"26\": \"32\",\n","    \"27\": \"33\",\n","    \"28\": \"34\",\n","    \"29\": \"35\",\n","    \"3\": \"11\",\n","    \"30\": \"36\",\n","    \"31\": \"37\",\n","    \"32\": \"38\",\n","    \"33\": \"39\",\n","    \"34\": \"4\",\n","    \"35\": \"40\",\n","    \"36\": \"41\",\n","    \"37\": \"42\",\n","    \"38\": \"43\",\n","    \"39\": \"44\",\n","    \"4\": \"12\",\n","    \"40\": \"45\",\n","    \"41\": \"5\",\n","    \"42\": \"6\",\n","    \"43\": \"7\",\n","    \"44\": \"8\",\n","    \"45\": \"9\",\n","    \"5\": \"13\",\n","    \"6\": \"14\",\n","    \"7\": \"15\",\n","    \"8\": \"16\",\n","    \"9\": \"17\"\n","  },\n","  \"label2id\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\",\n","    \"10\": \"2\",\n","    \"11\": \"3\",\n","    \"12\": \"4\",\n","    \"13\": \"5\",\n","    \"14\": \"6\",\n","    \"15\": \"7\",\n","    \"16\": \"8\",\n","    \"17\": \"9\",\n","    \"18\": \"10\",\n","    \"19\": \"11\",\n","    \"2\": \"12\",\n","    \"20\": \"13\",\n","    \"21\": \"14\",\n","    \"22\": \"15\",\n","    \"23\": \"16\",\n","    \"24\": \"17\",\n","    \"25\": \"18\",\n","    \"26\": \"19\",\n","    \"27\": \"20\",\n","    \"28\": \"21\",\n","    \"29\": \"22\",\n","    \"3\": \"23\",\n","    \"30\": \"24\",\n","    \"31\": \"25\",\n","    \"32\": \"26\",\n","    \"33\": \"27\",\n","    \"34\": \"28\",\n","    \"35\": \"29\",\n","    \"36\": \"30\",\n","    \"37\": \"31\",\n","    \"38\": \"32\",\n","    \"39\": \"33\",\n","    \"4\": \"34\",\n","    \"40\": \"35\",\n","    \"41\": \"36\",\n","    \"42\": \"37\",\n","    \"43\": \"38\",\n","    \"44\": \"39\",\n","    \"45\": \"40\",\n","    \"5\": \"41\",\n","    \"6\": \"42\",\n","    \"7\": \"43\",\n","    \"8\": \"44\",\n","    \"9\": \"45\"\n","  },\n","  \"layer_type\": \"basic\",\n","  \"model_type\": \"resnet\",\n","  \"num_channels\": 3,\n","  \"out_features\": [\n","    \"stage4\"\n","  ],\n","  \"out_indices\": [\n","    4\n","  ],\n","  \"stage_names\": [\n","    \"stem\",\n","    \"stage1\",\n","    \"stage2\",\n","    \"stage3\",\n","    \"stage4\"\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.34.0.dev0\"\n","}\n","\n","[INFO|modeling_utils.py:2989] 2023-10-02 20:33:19,407 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/model.safetensors\n","[INFO|modeling_utils.py:3771] 2023-10-02 20:33:19,533 >> All model checkpoint weights were used when initializing ResNetForImageClassification.\n","\n","[WARNING|modeling_utils.py:3792] 2023-10-02 20:33:19,534 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n","- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([46]) in the model instantiated\n","- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([46, 512]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|image_processing_utils.py:369] 2023-10-02 20:33:19,796 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--resnet-18/snapshots/f45a6faa12e0381f5620c5c8d7e916bca90f2c44/preprocessor_config.json\n","[WARNING|image_processing_auto.py:357] 2023-10-02 20:33:19,796 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","[INFO|image_processing_utils.py:732] 2023-10-02 20:33:19,798 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n","[INFO|image_processing_utils.py:419] 2023-10-02 20:33:19,798 >> Image processor ConvNextImageProcessor {\n","  \"crop_pct\": 0.875,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.485,\n","    0.456,\n","    0.406\n","  ],\n","  \"image_processor_type\": \"ConvNextImageProcessor\",\n","  \"image_std\": [\n","    0.229,\n","    0.224,\n","    0.225\n","  ],\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"shortest_edge\": 224\n","  }\n","}\n","\n","[INFO|trainer.py:1760] 2023-10-02 20:33:21,761 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-02 20:33:21,762 >>   Num examples = 2,796\n","[INFO|trainer.py:1762] 2023-10-02 20:33:21,762 >>   Num Epochs = 300\n","[INFO|trainer.py:1763] 2023-10-02 20:33:21,762 >>   Instantaneous batch size per device = 256\n","[INFO|trainer.py:1766] 2023-10-02 20:33:21,762 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n","[INFO|trainer.py:1767] 2023-10-02 20:33:21,762 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-02 20:33:21,762 >>   Total optimization steps = 3,300\n","[INFO|trainer.py:1769] 2023-10-02 20:33:21,763 >>   Number of trainable parameters = 11,200,110\n","[INFO|integration_utils.py:722] 2023-10-02 20:33:21,763 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","  0% 11/3300 [01:07<3:44:00,  4.09s/it][INFO|trainer.py:3213] 2023-10-02 20:34:29,021 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:34:29,022 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:34:29,022 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 4.307317733764648, 'eval_accuracy': 0.3975069252077562, 'eval_runtime': 20.2033, 'eval_samples_per_second': 35.737, 'eval_steps_per_second': 0.148, 'epoch': 1.0}\n","  0% 11/3300 [01:27<3:44:00,  4.09s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:34:49,230 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-11\n","[INFO|configuration_utils.py:460] 2023-10-02 20:34:49,235 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-11/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:34:49,348 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-11/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:34:49,352 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-11/preprocessor_config.json\n","  1% 22/3300 [02:29<3:39:30,  4.02s/it][INFO|trainer.py:3213] 2023-10-02 20:35:50,817 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:35:50,817 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:35:50,817 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 2.55678391456604, 'eval_accuracy': 0.502770083102493, 'eval_runtime': 20.1153, 'eval_samples_per_second': 35.893, 'eval_steps_per_second': 0.149, 'epoch': 2.0}\n","  1% 22/3300 [02:49<3:39:30,  4.02s/it]\n","100% 3/3 [00:05<00:00,  2.66s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:36:10,941 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-22\n","[INFO|configuration_utils.py:460] 2023-10-02 20:36:10,947 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-22/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:36:11,057 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-22/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:36:11,061 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-22/preprocessor_config.json\n","  1% 33/3300 [03:50<3:41:30,  4.07s/it][INFO|trainer.py:3213] 2023-10-02 20:37:12,722 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:37:12,722 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:37:12,722 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3998664617538452, 'eval_accuracy': 0.6371191135734072, 'eval_runtime': 20.0089, 'eval_samples_per_second': 36.084, 'eval_steps_per_second': 0.15, 'epoch': 3.0}\n","  1% 33/3300 [04:10<3:41:30,  4.07s/it]\n","100% 3/3 [00:05<00:00,  2.68s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:37:32,737 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-33\n","[INFO|configuration_utils.py:460] 2023-10-02 20:37:32,743 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-33/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:37:32,868 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-33/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:37:32,873 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-33/preprocessor_config.json\n","  1% 44/3300 [05:12<3:40:10,  4.06s/it][INFO|trainer.py:3213] 2023-10-02 20:38:34,634 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:38:34,634 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:38:34,634 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.64589262008667, 'eval_accuracy': 0.6329639889196675, 'eval_runtime': 19.9873, 'eval_samples_per_second': 36.123, 'eval_steps_per_second': 0.15, 'epoch': 4.0}\n","  1% 44/3300 [05:32<3:40:10,  4.06s/it]\n","100% 3/3 [00:05<00:00,  2.61s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:38:54,628 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-44\n","[INFO|configuration_utils.py:460] 2023-10-02 20:38:54,648 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-44/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:38:54,761 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-44/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:38:54,766 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-44/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:38:54,989 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-11] due to args.save_total_limit\n","  2% 55/3300 [06:34<3:37:00,  4.01s/it][INFO|trainer.py:3213] 2023-10-02 20:39:56,408 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-02 20:39:56,408 >>   Num examples = 722\n","[INFO|trainer.py:3218] 2023-10-02 20:39:56,408 >>   Batch size = 256\n","\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 1.3848164081573486, 'eval_accuracy': 0.6454293628808865, 'eval_runtime': 19.917, 'eval_samples_per_second': 36.25, 'eval_steps_per_second': 0.151, 'epoch': 5.0}\n","  2% 55/3300 [06:54<3:37:00,  4.01s/it]\n","100% 3/3 [00:05<00:00,  2.65s/it]\u001b[A\n","                                 \u001b[A[INFO|trainer.py:2939] 2023-10-02 20:40:16,331 >> Saving model checkpoint to drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-55\n","[INFO|configuration_utils.py:460] 2023-10-02 20:40:16,337 >> Configuration saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-55/config.json\n","[INFO|modeling_utils.py:2114] 2023-10-02 20:40:16,453 >> Model weights saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-55/pytorch_model.bin\n","[INFO|image_processing_utils.py:253] 2023-10-02 20:40:16,457 >> Image processor saved in drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-55/preprocessor_config.json\n","[INFO|trainer.py:3026] 2023-10-02 20:40:16,709 >> Deleting older checkpoint [drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/checkpoint-22] due to args.save_total_limit\n","  2% 63/3300 [07:45<5:48:23,  6.46s/it]"]}],"source":["!python cnn.py \\\n","    --model_name microsoft/resnet-18 \\\n","    --model_type resnet \\\n","    --train_dir data/fabi-data/train \\\n","    --validation_dir data/fabi-data/test \\\n","    --output_dir drive/MyDrive/hons-research/outputs/cnn/fabi/fabiPandD_outputs_10/ \\\n","    --remove_unused_columns False \\\n","    --do_train \\\n","    --do_eval \\\n","    --push_to_hub False \\\n","    --optim adamw_torch \\\n","    --learning_rate 0.001 \\\n","    --num_train_epochs 300 \\\n","    --per_device_train_batch_size 256 \\\n","    --per_device_eval_batch_size 256 \\\n","    --logging_strategy steps \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy epoch \\\n","    --save_strategy epoch \\\n","    --load_best_model_at_end True \\\n","    --save_total_limit 3 \\\n","    --data_seed 10 \\\n","    --seed 10 \\\n","    --report_to wandb \\\n","    --ignore_mismatched_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dqz8U8f5-7h"},"outputs":[],"source":["# End run\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"17Xhtplfpdqdz1TwipHI60aqky1CvSd1x","timestamp":1693214691473}],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}